I0525 10:27:46.397211      19 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-325827021
I0525 10:27:46.397589      19 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0525 10:27:46.397893      19 e2e.go:129] Starting e2e run "43eb5237-1d2a-4277-b2ce-d003dbdc078f" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621938464 - Will randomize all specs
Will run 311 of 5667 specs

May 25 10:27:46.415: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:27:46.418: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 25 10:27:46.462: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 25 10:27:46.518: INFO: 1 / 1 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 25 10:27:46.518: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
May 25 10:27:46.518: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 25 10:27:46.530: INFO: e2e test version: v1.20.5
May 25 10:27:46.535: INFO: kube-apiserver version: v1.20.5+k3s1
May 25 10:27:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:27:46.546: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:27:46.546: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
May 25 10:27:46.599: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-7e11dc92-ed80-4227-8552-ec1a6d1c8292
STEP: Creating configMap with name cm-test-opt-upd-302f6626-019d-447d-b92c-1f98512b7f11
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7e11dc92-ed80-4227-8552-ec1a6d1c8292
STEP: Updating configmap cm-test-opt-upd-302f6626-019d-447d-b92c-1f98512b7f11
STEP: Creating configMap with name cm-test-opt-create-d655dea2-4cb2-4601-a94f-f8ede0879c63
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:27:50.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5219" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:27:50.874: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-34e2ef83-4a99-4105-8977-d4de26859174
STEP: Creating a pod to test consume secrets
May 25 10:27:51.008: INFO: Waiting up to 5m0s for pod "pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714" in namespace "secrets-6445" to be "Succeeded or Failed"
May 25 10:27:51.014: INFO: Pod "pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714": Phase="Pending", Reason="", readiness=false. Elapsed: 6.176256ms
May 25 10:27:53.027: INFO: Pod "pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01917958s
STEP: Saw pod success
May 25 10:27:53.027: INFO: Pod "pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714" satisfied condition "Succeeded or Failed"
May 25 10:27:53.033: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714 container secret-volume-test: <nil>
STEP: delete the pod
May 25 10:27:53.076: INFO: Waiting for pod pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714 to disappear
May 25 10:27:53.085: INFO: Pod pod-secrets-06503ed9-2821-4fa8-b558-7dfba564e714 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:27:53.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6445" for this suite.
STEP: Destroying namespace "secret-namespace-4064" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":18,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:27:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:28:04.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7263" for this suite.

• [SLOW TEST:11.319 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":3,"skipped":45,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:28:04.444: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 10:28:04.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1" in namespace "projected-3175" to be "Succeeded or Failed"
May 25 10:28:04.531: INFO: Pod "downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699494ms
May 25 10:28:06.547: INFO: Pod "downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022838621s
STEP: Saw pod success
May 25 10:28:06.547: INFO: Pod "downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1" satisfied condition "Succeeded or Failed"
May 25 10:28:06.553: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1 container client-container: <nil>
STEP: delete the pod
May 25 10:28:06.579: INFO: Waiting for pod downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1 to disappear
May 25 10:28:06.592: INFO: Pod downwardapi-volume-6c53bf04-069a-4098-b084-620889190cb1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:28:06.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3175" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":49,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:28:06.610: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
May 25 10:28:06.676: INFO: Waiting up to 5m0s for pod "var-expansion-d60196be-583c-46be-a6dd-36891a35af02" in namespace "var-expansion-6985" to be "Succeeded or Failed"
May 25 10:28:06.685: INFO: Pod "var-expansion-d60196be-583c-46be-a6dd-36891a35af02": Phase="Pending", Reason="", readiness=false. Elapsed: 8.182957ms
May 25 10:28:08.701: INFO: Pod "var-expansion-d60196be-583c-46be-a6dd-36891a35af02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024723296s
STEP: Saw pod success
May 25 10:28:08.701: INFO: Pod "var-expansion-d60196be-583c-46be-a6dd-36891a35af02" satisfied condition "Succeeded or Failed"
May 25 10:28:08.706: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod var-expansion-d60196be-583c-46be-a6dd-36891a35af02 container dapi-container: <nil>
STEP: delete the pod
May 25 10:28:08.741: INFO: Waiting for pod var-expansion-d60196be-583c-46be-a6dd-36891a35af02 to disappear
May 25 10:28:08.748: INFO: Pod var-expansion-d60196be-583c-46be-a6dd-36891a35af02 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:28:08.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6985" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":49,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:28:08.774: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 10:28:08.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f" in namespace "downward-api-1118" to be "Succeeded or Failed"
May 25 10:28:08.862: INFO: Pod "downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.666376ms
May 25 10:28:10.870: INFO: Pod "downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014259498s
STEP: Saw pod success
May 25 10:28:10.870: INFO: Pod "downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f" satisfied condition "Succeeded or Failed"
May 25 10:28:10.877: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f container client-container: <nil>
STEP: delete the pod
May 25 10:28:10.906: INFO: Waiting for pod downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f to disappear
May 25 10:28:10.918: INFO: Pod downwardapi-volume-edcdb9a7-33c3-48a8-94cd-8d994169664f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:28:10.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1118" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":75,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:28:10.943: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 25 10:28:11.007: INFO: Waiting up to 5m0s for pod "pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0" in namespace "emptydir-630" to be "Succeeded or Failed"
May 25 10:28:11.017: INFO: Pod "pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440987ms
May 25 10:28:13.028: INFO: Pod "pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021660706s
STEP: Saw pod success
May 25 10:28:13.029: INFO: Pod "pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0" satisfied condition "Succeeded or Failed"
May 25 10:28:13.035: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0 container test-container: <nil>
STEP: delete the pod
May 25 10:28:13.065: INFO: Waiting for pod pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0 to disappear
May 25 10:28:13.072: INFO: Pod pod-86ac4343-b41a-4940-9a3b-8c0352b73cf0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:28:13.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-630" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":105,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:28:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:28:13.150: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:28:15.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3849" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:28:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-941747d2-00d9-4ce2-a2ad-e3edf7c2df49
STEP: Creating secret with name s-test-opt-upd-6a126f93-7edc-4417-bca1-31fa377ec638
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-941747d2-00d9-4ce2-a2ad-e3edf7c2df49
STEP: Updating secret s-test-opt-upd-6a126f93-7edc-4417-bca1-31fa377ec638
STEP: Creating secret with name s-test-opt-create-2389f7c9-caef-4620-9d9f-7315af98dfcd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:29:48.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8016" for this suite.

• [SLOW TEST:93.245 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:29:48.599: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:29:52.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6499" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":10,"skipped":182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:29:52.713: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4774.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4774.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4774.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4774.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 10:29:56.927: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.934: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.940: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.946: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.970: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.976: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.982: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.988: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4774.svc.cluster.local from pod dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422: the server could not find the requested resource (get pods dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422)
May 25 10:29:56.999: INFO: Lookups using dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4774.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4774.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4774.svc.cluster.local jessie_udp@dns-test-service-2.dns-4774.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4774.svc.cluster.local]

May 25 10:30:02.085: INFO: DNS probes using dns-4774/dns-test-dc81c3ae-f8a1-4e2a-8abf-a26f9b881422 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:30:02.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4774" for this suite.

• [SLOW TEST:9.479 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":11,"skipped":225,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:30:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7106
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7106 to expose endpoints map[]
May 25 10:30:02.391: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 25 10:30:03.410: INFO: successfully validated that service multi-endpoint-test in namespace services-7106 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7106
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7106 to expose endpoints map[pod1:[100]]
May 25 10:30:05.457: INFO: successfully validated that service multi-endpoint-test in namespace services-7106 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7106
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7106 to expose endpoints map[pod1:[100] pod2:[101]]
May 25 10:30:07.501: INFO: successfully validated that service multi-endpoint-test in namespace services-7106 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7106
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7106 to expose endpoints map[pod2:[101]]
May 25 10:30:07.552: INFO: successfully validated that service multi-endpoint-test in namespace services-7106 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7106
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7106 to expose endpoints map[]
May 25 10:30:07.601: INFO: successfully validated that service multi-endpoint-test in namespace services-7106 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:30:07.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7106" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.469 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":12,"skipped":229,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:30:07.661: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:30:07.772: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 25 10:30:12.783: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 25 10:30:12.783: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 25 10:30:12.833: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7880  4da6e837-a894-4bd5-aaf6-e56b8027bab2 88216 1 2021-05-25 10:30:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-05-25 10:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aaf6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 25 10:30:12.850: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:30:12.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7880" for this suite.

• [SLOW TEST:5.225 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":13,"skipped":236,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:30:12.887: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:30:15.121: INFO: Waiting up to 5m0s for pod "client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293" in namespace "pods-3412" to be "Succeeded or Failed"
May 25 10:30:15.128: INFO: Pod "client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293": Phase="Pending", Reason="", readiness=false. Elapsed: 7.029398ms
May 25 10:30:17.142: INFO: Pod "client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02141209s
STEP: Saw pod success
May 25 10:30:17.142: INFO: Pod "client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293" satisfied condition "Succeeded or Failed"
May 25 10:30:17.152: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293 container env3cont: <nil>
STEP: delete the pod
May 25 10:30:17.194: INFO: Waiting for pod client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293 to disappear
May 25 10:30:17.203: INFO: Pod client-envvars-d5272dbe-9fc2-4351-a40d-118fceb01293 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:30:17.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3412" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":239,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:30:17.225: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:30:34.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4781" for this suite.

• [SLOW TEST:17.197 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":15,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:30:34.426: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-64229a5d-c805-4fc1-a952-84a2d8d58d29
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:30:36.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5750" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":266,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:30:36.586: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 25 10:30:36.637: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 10:31:36.668: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:31:36.674: INFO: Starting informer...
STEP: Starting pods...
May 25 10:31:36.912: INFO: Pod1 is running on gke-cluster-1-default-pool-8f0bb8bb-p6wx. Tainting Node
May 25 10:31:39.167: INFO: Pod2 is running on gke-cluster-1-default-pool-8f0bb8bb-p6wx. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 25 10:31:46.195: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 25 10:32:06.202: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:06.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2788" for this suite.

• [SLOW TEST:89.904 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":17,"skipped":267,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:06.493: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
May 25 10:32:06.568: INFO: Major version: 1
STEP: Confirm minor version
May 25 10:32:06.568: INFO: cleanMinorVersion: 20
May 25 10:32:06.569: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:06.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9373" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":18,"skipped":275,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:06.594: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 25 10:32:06.647: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:32:10.306: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2498" for this suite.

• [SLOW TEST:17.795 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":19,"skipped":282,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:24.389: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-5344d68f-b3d2-4aab-919f-9118aa661e06
STEP: Creating a pod to test consume configMaps
May 25 10:32:24.473: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a" in namespace "projected-9727" to be "Succeeded or Failed"
May 25 10:32:24.481: INFO: Pod "pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801337ms
May 25 10:32:26.497: INFO: Pod "pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024123371s
STEP: Saw pod success
May 25 10:32:26.497: INFO: Pod "pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a" satisfied condition "Succeeded or Failed"
May 25 10:32:26.504: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a container agnhost-container: <nil>
STEP: delete the pod
May 25 10:32:26.549: INFO: Waiting for pod pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a to disappear
May 25 10:32:26.559: INFO: Pod pod-projected-configmaps-6dec0c39-a23c-4d59-826a-9762fbe7816a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:26.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9727" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":283,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:26.576: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-95f12b87-7e72-48f8-bf96-0d785fd432ae
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:26.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4744" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":21,"skipped":293,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:26.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8051" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":301,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:26.793: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-7926023b-83f0-430b-ae68-a2eac411d562 in namespace container-probe-7863
May 25 10:32:28.874: INFO: Started pod liveness-7926023b-83f0-430b-ae68-a2eac411d562 in namespace container-probe-7863
STEP: checking the pod's current state and verifying that restartCount is present
May 25 10:32:28.880: INFO: Initial restart count of pod liveness-7926023b-83f0-430b-ae68-a2eac411d562 is 0
May 25 10:32:45.007: INFO: Restart count of pod container-probe-7863/liveness-7926023b-83f0-430b-ae68-a2eac411d562 is now 1 (16.126164162s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:45.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7863" for this suite.

• [SLOW TEST:18.277 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":315,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:45.074: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:32:45.154: INFO: Creating deployment "webserver-deployment"
May 25 10:32:45.165: INFO: Waiting for observed generation 1
May 25 10:32:47.188: INFO: Waiting for all required pods to come up
May 25 10:32:47.198: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 25 10:32:49.263: INFO: Waiting for deployment "webserver-deployment" to complete
May 25 10:32:49.276: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 25 10:32:49.295: INFO: Updating deployment webserver-deployment
May 25 10:32:49.295: INFO: Waiting for observed generation 2
May 25 10:32:51.347: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 25 10:32:51.362: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 25 10:32:51.383: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 25 10:32:51.418: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 25 10:32:51.418: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 25 10:32:51.441: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 25 10:32:51.472: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 25 10:32:51.472: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 25 10:32:51.505: INFO: Updating deployment webserver-deployment
May 25 10:32:51.505: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 25 10:32:51.534: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 25 10:32:53.754: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 25 10:32:54.183: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2268  1414154f-12bc-4a41-a674-f709b5a94832 88790 3 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {k3s Update apps/v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060d1088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-25 10:32:51 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-25 10:32:51 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 25 10:32:54.264: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2268  ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 88788 3 2021-05-25 10:32:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1414154f-12bc-4a41-a674-f709b5a94832 0xc0060d15f7 0xc0060d15f8}] []  [{k3s Update apps/v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1414154f-12bc-4a41-a674-f709b5a94832\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060d1698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 10:32:54.266: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 25 10:32:54.267: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2268  52d7a0cc-a937-4353-a15c-cf6bd3b3672d 88780 3 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1414154f-12bc-4a41-a674-f709b5a94832 0xc0060d14f7 0xc0060d14f8}] []  [{k3s Update apps/v1 2021-05-25 10:32:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1414154f-12bc-4a41-a674-f709b5a94832\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060d1588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 25 10:32:54.472: INFO: Pod "webserver-deployment-dd94f59b7-klb7z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-klb7z webserver-deployment-dd94f59b7- deployment-2268  f4ae1b1e-b127-48a6-a2b4-228ad31fc737 88630 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b90b7 0xc0060b90b8}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.247,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://de742b32ab9b690d6b6c2a8d1584df6aa7188710a737b9c14358f334b3981741,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.476: INFO: Pod "webserver-deployment-dd94f59b7-qv6kx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qv6kx webserver-deployment-dd94f59b7- deployment-2268  cacc238a-ff6f-47c1-9a81-72a58c7f90cb 88635 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b9250 0xc0060b9251}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.1.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:10.64.1.148,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://727b6a4dc29e19d8d9897f8a23b81ebfd44cafc2bff11791fd1e7dc274a40b1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.496: INFO: Pod "webserver-deployment-dd94f59b7-qjq2z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qjq2z webserver-deployment-dd94f59b7- deployment-2268  7a4cdcb8-a9f1-4867-b43b-ebd8967f267f 88639 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b93e0 0xc0060b93e1}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:10.64.2.111,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5b95f00ecd959b1590956529ce503753a9a37f96ae54649593ccb5b26f2a69fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.498: INFO: Pod "webserver-deployment-dd94f59b7-9mctq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9mctq webserver-deployment-dd94f59b7- deployment-2268  f212a108-142b-4bd2-b01b-75aa559f19f3 88642 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b9570 0xc0060b9571}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:10.64.2.108,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://8788c79142131dc7bf03a0401e1489b2404774b7b875ed79e3b7c66dad19af15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.506: INFO: Pod "webserver-deployment-dd94f59b7-vghkq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vghkq webserver-deployment-dd94f59b7- deployment-2268  c8abca0e-14e3-45ed-8dee-f428c3fcb0d7 88647 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b9730 0xc0060b9731}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:10.64.2.109,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6c49efa39fcb1cb202c8b2c312cc35b4dbdb410ab3c5df40edf58d3033c0c682,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.506: INFO: Pod "webserver-deployment-dd94f59b7-2xdkn" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2xdkn webserver-deployment-dd94f59b7- deployment-2268  6694e202-fcbe-4823-b7b5-b2a2d657648b 88661 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b98e0 0xc0060b98e1}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.250,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://709dc3b35b400e28cf9167f94fdd33543647790b59606e7042f0e624ba8c7c30,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.518: INFO: Pod "webserver-deployment-dd94f59b7-prtg2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-prtg2 webserver-deployment-dd94f59b7- deployment-2268  815abbeb-4b06-4475-a9cf-9ba1f9d11413 88668 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b9ad0 0xc0060b9ad1}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.251,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ab0d53ed3e104e37aba1098c85a3c2aa5200f3ca2cd5d98354ce99b4b31780e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.519: INFO: Pod "webserver-deployment-dd94f59b7-54s4m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-54s4m webserver-deployment-dd94f59b7- deployment-2268  82023834-beb6-45ab-81fe-ad82c7eda24e 88671 0 2021-05-25 10:32:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060b9ca0 0xc0060b9ca1}] []  [{k3s Update v1 2021-05-25 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:10.64.2.110,StartTime:2021-05-25 10:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e0bfc6d87c4a8f02de510067a7bae5d1faeb1264dee2c84637a6869f8f3d6dca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.519: INFO: Pod "webserver-deployment-795d758f88-97vxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-97vxj webserver-deployment-795d758f88- deployment-2268  fbcbfce8-7532-4c94-975a-62886431a1e6 88717 0 2021-05-25 10:32:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060b9e90 0xc0060b9e91}] []  [{k3s Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:,StartTime:2021-05-25 10:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.534: INFO: Pod "webserver-deployment-795d758f88-bkrn5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bkrn5 webserver-deployment-795d758f88- deployment-2268  f33f5466-59c7-4cee-b661-19b5c9a4050f 88721 0 2021-05-25 10:32:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ec080 0xc0060ec081}] []  [{k3s Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:,StartTime:2021-05-25 10:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.541: INFO: Pod "webserver-deployment-795d758f88-4c9bb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4c9bb webserver-deployment-795d758f88- deployment-2268  9937e2cd-5b77-4c42-8896-ba8f54e2b104 88722 0 2021-05-25 10:32:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ec210 0xc0060ec211}] []  [{k3s Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 10:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.542: INFO: Pod "webserver-deployment-795d758f88-2x2wd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2x2wd webserver-deployment-795d758f88- deployment-2268  5976067b-b433-42be-9efa-af5d584e80a3 88723 0 2021-05-25 10:32:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ec3a0 0xc0060ec3a1}] []  [{k3s Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:,StartTime:2021-05-25 10:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.553: INFO: Pod "webserver-deployment-795d758f88-n5nst" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n5nst webserver-deployment-795d758f88- deployment-2268  0c29ab51-f8b0-43e6-90cb-61d4f3abff8e 88724 0 2021-05-25 10:32:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ec530 0xc0060ec531}] []  [{k3s Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 10:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.558: INFO: Pod "webserver-deployment-795d758f88-8h5cc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8h5cc webserver-deployment-795d758f88- deployment-2268  5351175d-ac7e-42f6-974b-90f4e0e4ee15 88773 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ec6c0 0xc0060ec6c1}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.571: INFO: Pod "webserver-deployment-795d758f88-gbdrk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gbdrk webserver-deployment-795d758f88- deployment-2268  079de0da-4df3-45ff-b8fa-83398a7fe8b2 88777 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ec867 0xc0060ec868}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.572: INFO: Pod "webserver-deployment-dd94f59b7-vhrlw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vhrlw webserver-deployment-dd94f59b7- deployment-2268  2c646aa9-d8a6-42d0-8cb5-738132585228 88791 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060ec9a7 0xc0060ec9a8}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.572: INFO: Pod "webserver-deployment-dd94f59b7-gqtzf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gqtzf webserver-deployment-dd94f59b7- deployment-2268  7818aae9-588b-49c4-b277-03b44baf21f5 88795 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060ecb47 0xc0060ecb48}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.573: INFO: Pod "webserver-deployment-795d758f88-2rjh2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2rjh2 webserver-deployment-795d758f88- deployment-2268  d525f2b1-0bcd-4a02-8d26-e93f76e0ce64 88799 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ecd97 0xc0060ecd98}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.573: INFO: Pod "webserver-deployment-dd94f59b7-8msmq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8msmq webserver-deployment-dd94f59b7- deployment-2268  47172d28-9a41-4950-abb0-1aa37add67e4 88803 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060ecfe0 0xc0060ecfe1}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.574: INFO: Pod "webserver-deployment-795d758f88-7p4xn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7p4xn webserver-deployment-795d758f88- deployment-2268  7485beb9-fdd5-4360-9705-abaaca10a739 88807 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ed1e7 0xc0060ed1e8}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.580: INFO: Pod "webserver-deployment-dd94f59b7-7hzxk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7hzxk webserver-deployment-dd94f59b7- deployment-2268  8df535a0-374e-4e8c-939f-b3cfd185b140 88815 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060ed420 0xc0060ed421}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.580: INFO: Pod "webserver-deployment-dd94f59b7-k6bmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-k6bmh webserver-deployment-dd94f59b7- deployment-2268  aa60ecd3-7258-44fd-8be6-583b1f46c923 88825 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060ed627 0xc0060ed628}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.582: INFO: Pod "webserver-deployment-795d758f88-tnltc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tnltc webserver-deployment-795d758f88- deployment-2268  ae0520c7-8ae2-4624-93e6-2a6f875c3ef2 88828 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ed837 0xc0060ed838}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.582: INFO: Pod "webserver-deployment-dd94f59b7-gmdhs" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gmdhs webserver-deployment-dd94f59b7- deployment-2268  a2a1dd38-3202-4690-8ad4-68ea64d61244 88832 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060eda00 0xc0060eda01}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.582: INFO: Pod "webserver-deployment-dd94f59b7-4vr5r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4vr5r webserver-deployment-dd94f59b7- deployment-2268  b1441d76-005e-430e-bd43-f904d91ccb2c 88833 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060edb77 0xc0060edb78}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodScheduled\"}":{"f:lastTransitionTime":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.583: INFO: Pod "webserver-deployment-dd94f59b7-qzlmr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qzlmr webserver-deployment-dd94f59b7- deployment-2268  948f9740-92d7-4830-a3bc-1620cae7e8a5 88836 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060edcf7 0xc0060edcf8}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.590: INFO: Pod "webserver-deployment-795d758f88-2gv9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2gv9v webserver-deployment-795d758f88- deployment-2268  61e98323-44ad-4c7c-b752-4d71e3557cd3 88838 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc0060ede10 0xc0060ede11}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodScheduled\"}":{"f:lastTransitionTime":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.592: INFO: Pod "webserver-deployment-dd94f59b7-k6bbr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-k6bbr webserver-deployment-dd94f59b7- deployment-2268  e43dcafd-bd09-485f-8224-faadfd565564 88840 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc0060edfa0 0xc0060edfa1}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.593: INFO: Pod "webserver-deployment-dd94f59b7-j777w" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-j777w webserver-deployment-dd94f59b7- deployment-2268  e27975f1-5a92-4418-96b7-7a3e0117a860 88843 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc00610a100 0xc00610a101}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.594: INFO: Pod "webserver-deployment-dd94f59b7-tsc7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tsc7z webserver-deployment-dd94f59b7- deployment-2268  e93de4de-9827-49b3-9558-a68a6a48ac99 88844 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc00610a240 0xc00610a241}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.599: INFO: Pod "webserver-deployment-795d758f88-fs2sd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fs2sd webserver-deployment-795d758f88- deployment-2268  5bcfc044-4683-4bb5-9c25-3fb880ca5b29 88847 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc00610a360 0xc00610a361}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodScheduled\"}":{"f:lastTransitionTime":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-vl79,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.15,PodIP:,StartTime:2021-05-25 10:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.599: INFO: Pod "webserver-deployment-dd94f59b7-gwjwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gwjwj webserver-deployment-dd94f59b7- deployment-2268  9ea77947-0466-4928-b571-2b47aa5f5867 88849 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 52d7a0cc-a937-4353-a15c-cf6bd3b3672d 0xc00610a500 0xc00610a501}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d7a0cc-a937-4353-a15c-cf6bd3b3672d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 10:32:54.599: INFO: Pod "webserver-deployment-795d758f88-mhhsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mhhsv webserver-deployment-795d758f88- deployment-2268  06b824e4-1e64-44c7-a3db-5cd6ffa29e39 88853 0 2021-05-25 10:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff 0xc00610a630 0xc00610a631}] []  [{k3s Update v1 2021-05-25 10:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddd52b2b-bff0-4f4e-8e68-42d4b57a4fff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qzz4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qzz4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qzz4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:32:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:54.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2268" for this suite.

• [SLOW TEST:9.741 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":24,"skipped":316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:54.816: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 25 10:32:56.191: INFO: Waiting up to 5m0s for pod "pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118" in namespace "emptydir-4772" to be "Succeeded or Failed"
May 25 10:32:56.222: INFO: Pod "pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118": Phase="Pending", Reason="", readiness=false. Elapsed: 31.197985ms
May 25 10:32:58.239: INFO: Pod "pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047979797s
STEP: Saw pod success
May 25 10:32:58.239: INFO: Pod "pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118" satisfied condition "Succeeded or Failed"
May 25 10:32:58.246: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118 container test-container: <nil>
STEP: delete the pod
May 25 10:32:58.343: INFO: Waiting for pod pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118 to disappear
May 25 10:32:58.361: INFO: Pod pod-ed296ea1-9a1c-467d-8ba8-bc6b6c5db118 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:32:58.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4772" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:32:58.399: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-182ae027-0f8f-4e0e-a93f-e63e9059d1be
STEP: Creating a pod to test consume secrets
May 25 10:32:58.531: INFO: Waiting up to 5m0s for pod "pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf" in namespace "secrets-277" to be "Succeeded or Failed"
May 25 10:32:58.540: INFO: Pod "pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.020303ms
May 25 10:33:00.563: INFO: Pod "pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032179517s
STEP: Saw pod success
May 25 10:33:00.563: INFO: Pod "pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf" satisfied condition "Succeeded or Failed"
May 25 10:33:00.571: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf container secret-volume-test: <nil>
STEP: delete the pod
May 25 10:33:00.615: INFO: Waiting for pod pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf to disappear
May 25 10:33:00.629: INFO: Pod pod-secrets-4c1361bc-e6ab-4907-9311-5acd1c0fe5cf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:33:00.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-277" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:33:00.664: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2134.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2134.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2134.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2134.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 10:33:06.838: INFO: DNS probes using dns-test-226b7706-c410-4c01-96f2-28bd8b0fb325 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2134.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2134.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2134.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2134.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 10:33:10.972: INFO: File wheezy_udp@dns-test-service-3.dns-2134.svc.cluster.local from pod  dns-2134/dns-test-fd32e1c2-9118-40ee-b5d2-21b5920aa812 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 10:33:10.979: INFO: Lookups using dns-2134/dns-test-fd32e1c2-9118-40ee-b5d2-21b5920aa812 failed for: [wheezy_udp@dns-test-service-3.dns-2134.svc.cluster.local]

May 25 10:33:15.996: INFO: DNS probes using dns-test-fd32e1c2-9118-40ee-b5d2-21b5920aa812 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2134.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2134.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2134.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2134.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 10:33:20.199: INFO: DNS probes using dns-test-d44a57c3-d437-420e-a69c-36d783a26bf5 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:33:20.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2134" for this suite.

• [SLOW TEST:19.694 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":27,"skipped":436,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:33:20.360: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 10:33:20.619: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 10:34:20.677: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 25 10:34:20.735: INFO: Created pod: pod0-sched-preemption-low-priority
May 25 10:34:20.769: INFO: Created pod: pod1-sched-preemption-medium-priority
May 25 10:34:20.821: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:34:28.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3656" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:69.411 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":28,"skipped":437,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:34:29.771: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:34:29.872: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c195579f-3160-4b67-9724-74da75278cb7", Controller:(*bool)(0xc0060d150a), BlockOwnerDeletion:(*bool)(0xc0060d150b)}}
May 25 10:34:29.887: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"255d5525-9296-4aed-8fef-78eb7087fe83", Controller:(*bool)(0xc00609f226), BlockOwnerDeletion:(*bool)(0xc00609f227)}}
May 25 10:34:29.903: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"10bca005-5dd5-497e-90a0-0ce386455985", Controller:(*bool)(0xc0060d16e6), BlockOwnerDeletion:(*bool)(0xc0060d16e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:34:34.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9333" for this suite.

• [SLOW TEST:5.172 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":29,"skipped":447,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:34:34.945: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 10:34:35.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb" in namespace "downward-api-3200" to be "Succeeded or Failed"
May 25 10:34:35.088: INFO: Pod "downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.733173ms
May 25 10:34:37.104: INFO: Pod "downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02736551s
STEP: Saw pod success
May 25 10:34:37.104: INFO: Pod "downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb" satisfied condition "Succeeded or Failed"
May 25 10:34:37.110: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb container client-container: <nil>
STEP: delete the pod
May 25 10:34:37.148: INFO: Waiting for pod downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb to disappear
May 25 10:34:37.157: INFO: Pod downwardapi-volume-523325f0-4c58-441b-a869-76c3fb5167bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:34:37.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3200" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:34:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1616
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1616
I0525 10:34:37.371884      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1616, replica count: 2
I0525 10:34:40.422405      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 10:34:40.422: INFO: Creating new exec pod
May 25 10:34:43.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-1616 exec execpodzrt59 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 25 10:34:44.031: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 10:34:44.031: INFO: stdout: ""
May 25 10:34:44.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-1616 exec execpodzrt59 -- /bin/sh -x -c nc -zv -t -w 2 10.68.0.145 80'
May 25 10:34:44.252: INFO: stderr: "+ nc -zv -t -w 2 10.68.0.145 80\nConnection to 10.68.0.145 80 port [tcp/http] succeeded!\n"
May 25 10:34:44.253: INFO: stdout: ""
May 25 10:34:44.253: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:34:44.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1616" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.130 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":31,"skipped":479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:34:44.313: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
May 25 10:34:46.926: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9121 pod-service-account-fe008962-b317-4f7e-89a9-e7b44cdeb4e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 25 10:34:47.145: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9121 pod-service-account-fe008962-b317-4f7e-89a9-e7b44cdeb4e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 25 10:34:47.357: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9121 pod-service-account-fe008962-b317-4f7e-89a9-e7b44cdeb4e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:34:47.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9121" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":32,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:34:47.591: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 25 10:34:47.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 create -f -'
May 25 10:34:47.998: INFO: stderr: ""
May 25 10:34:47.998: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 10:34:47.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 10:34:48.103: INFO: stderr: ""
May 25 10:34:48.103: INFO: stdout: "update-demo-nautilus-h4f57 update-demo-nautilus-ft8xz "
May 25 10:34:48.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods update-demo-nautilus-h4f57 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 10:34:48.220: INFO: stderr: ""
May 25 10:34:48.220: INFO: stdout: ""
May 25 10:34:48.220: INFO: update-demo-nautilus-h4f57 is created but not running
May 25 10:34:53.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 10:34:53.326: INFO: stderr: ""
May 25 10:34:53.326: INFO: stdout: "update-demo-nautilus-ft8xz update-demo-nautilus-h4f57 "
May 25 10:34:53.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods update-demo-nautilus-ft8xz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 10:34:53.424: INFO: stderr: ""
May 25 10:34:53.424: INFO: stdout: "true"
May 25 10:34:53.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods update-demo-nautilus-ft8xz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 10:34:53.523: INFO: stderr: ""
May 25 10:34:53.523: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 10:34:53.523: INFO: validating pod update-demo-nautilus-ft8xz
May 25 10:34:53.542: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 10:34:53.542: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 10:34:53.542: INFO: update-demo-nautilus-ft8xz is verified up and running
May 25 10:34:53.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods update-demo-nautilus-h4f57 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 10:34:53.653: INFO: stderr: ""
May 25 10:34:53.653: INFO: stdout: "true"
May 25 10:34:53.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods update-demo-nautilus-h4f57 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 10:34:53.756: INFO: stderr: ""
May 25 10:34:53.756: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 10:34:53.756: INFO: validating pod update-demo-nautilus-h4f57
May 25 10:34:53.766: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 10:34:53.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 10:34:53.766: INFO: update-demo-nautilus-h4f57 is verified up and running
STEP: using delete to clean up resources
May 25 10:34:53.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 delete --grace-period=0 --force -f -'
May 25 10:34:53.883: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 10:34:53.883: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 25 10:34:53.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get rc,svc -l name=update-demo --no-headers'
May 25 10:34:54.159: INFO: stderr: "No resources found in kubectl-3252 namespace.\n"
May 25 10:34:54.159: INFO: stdout: ""
May 25 10:34:54.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3252 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 25 10:34:54.275: INFO: stderr: ""
May 25 10:34:54.275: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:34:54.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3252" for this suite.

• [SLOW TEST:6.704 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":33,"skipped":535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:34:54.296: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-772
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 25 10:34:54.424: INFO: Found 0 stateful pods, waiting for 3
May 25 10:35:04.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 10:35:04.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 10:35:04.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 25 10:35:04.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-772 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 10:35:04.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 10:35:04.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 10:35:04.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 25 10:35:14.744: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 25 10:35:24.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-772 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 10:35:25.025: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 10:35:25.025: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 10:35:25.025: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 10:35:35.069: INFO: Waiting for StatefulSet statefulset-772/ss2 to complete update
May 25 10:35:35.069: INFO: Waiting for Pod statefulset-772/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 25 10:35:35.069: INFO: Waiting for Pod statefulset-772/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 25 10:35:45.088: INFO: Waiting for StatefulSet statefulset-772/ss2 to complete update
May 25 10:35:45.089: INFO: Waiting for Pod statefulset-772/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 25 10:35:55.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-772 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 10:35:55.332: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 10:35:55.332: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 10:35:55.332: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 10:36:05.399: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 25 10:36:15.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-772 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 10:36:15.659: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 10:36:15.659: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 10:36:15.659: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 10:36:25.706: INFO: Waiting for StatefulSet statefulset-772/ss2 to complete update
May 25 10:36:25.706: INFO: Waiting for Pod statefulset-772/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 25 10:36:25.706: INFO: Waiting for Pod statefulset-772/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 25 10:36:35.730: INFO: Waiting for StatefulSet statefulset-772/ss2 to complete update
May 25 10:36:35.730: INFO: Waiting for Pod statefulset-772/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 25 10:36:45.728: INFO: Waiting for StatefulSet statefulset-772/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 25 10:36:55.729: INFO: Deleting all statefulset in ns statefulset-772
May 25 10:36:55.735: INFO: Scaling statefulset ss2 to 0
May 25 10:37:15.787: INFO: Waiting for statefulset status.replicas updated to 0
May 25 10:37:15.794: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:37:15.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-772" for this suite.

• [SLOW TEST:141.548 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":34,"skipped":589,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:37:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 10:37:15.913: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd" in namespace "downward-api-555" to be "Succeeded or Failed"
May 25 10:37:15.920: INFO: Pod "downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.908203ms
May 25 10:37:17.936: INFO: Pod "downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02233235s
STEP: Saw pod success
May 25 10:37:17.936: INFO: Pod "downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd" satisfied condition "Succeeded or Failed"
May 25 10:37:17.941: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd container client-container: <nil>
STEP: delete the pod
May 25 10:37:18.001: INFO: Waiting for pod downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd to disappear
May 25 10:37:18.011: INFO: Pod downwardapi-volume-8cae691d-4fa3-4173-9e4e-bcc627a44cfd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:37:18.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-555" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:37:18.032: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 25 10:37:18.088: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 25 10:37:31.746: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:37:35.322: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:37:49.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5645" for this suite.

• [SLOW TEST:31.628 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":36,"skipped":618,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:37:49.660: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:37:49.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3798" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":37,"skipped":639,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:37:49.751: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 25 10:37:49.824: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:37:53.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-877" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":38,"skipped":658,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:37:53.290: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 10:37:53.381: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 10:38:53.427: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:38:53.436: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 25 10:38:55.559: INFO: found a healthy node: gke-cluster-1-default-pool-8f0bb8bb-p6wx
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:39:05.976: INFO: pods created so far: [1 1 1]
May 25 10:39:05.977: INFO: length of pods created so far: 3
May 25 10:39:18.002: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:39:25.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3850" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:39:25.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5967" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:91.951 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":39,"skipped":662,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:39:25.242: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-95e832ce-53a4-4d96-adf8-10fe9081bde5
STEP: Creating a pod to test consume configMaps
May 25 10:39:25.340: INFO: Waiting up to 5m0s for pod "pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52" in namespace "configmap-9292" to be "Succeeded or Failed"
May 25 10:39:25.347: INFO: Pod "pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52": Phase="Pending", Reason="", readiness=false. Elapsed: 7.512704ms
May 25 10:39:27.382: INFO: Pod "pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042428478s
STEP: Saw pod success
May 25 10:39:27.382: INFO: Pod "pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52" satisfied condition "Succeeded or Failed"
May 25 10:39:27.393: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52 container agnhost-container: <nil>
STEP: delete the pod
May 25 10:39:27.459: INFO: Waiting for pod pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52 to disappear
May 25 10:39:27.478: INFO: Pod pod-configmaps-c77c84b0-181a-4cac-b06c-92df2907bc52 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:39:27.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9292" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":664,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:39:27.511: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:39:27.587: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Creating first CR 
May 25 10:39:28.231: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T10:39:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T10:39:28Z]] name:name1 resourceVersion:90404 uid:882f6714-819c-40a8-a547-6faf9ea1502d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 25 10:39:38.260: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T10:39:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T10:39:38Z]] name:name2 resourceVersion:90475 uid:9d3fad27-b34a-4a3d-8aa0-78be9b5a7425] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 25 10:39:48.349: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T10:39:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T10:39:48Z]] name:name1 resourceVersion:90479 uid:882f6714-819c-40a8-a547-6faf9ea1502d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 25 10:39:58.370: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T10:39:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T10:39:58Z]] name:name2 resourceVersion:90481 uid:9d3fad27-b34a-4a3d-8aa0-78be9b5a7425] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 25 10:40:08.405: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T10:39:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T10:39:48Z]] name:name1 resourceVersion:90483 uid:882f6714-819c-40a8-a547-6faf9ea1502d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 25 10:40:18.438: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T10:39:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T10:39:58Z]] name:name2 resourceVersion:90485 uid:9d3fad27-b34a-4a3d-8aa0-78be9b5a7425] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:40:28.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9477" for this suite.

• [SLOW TEST:61.220 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":41,"skipped":675,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:40:28.733: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 25 10:40:28.803: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:40:48.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4969" for this suite.

• [SLOW TEST:19.591 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":42,"skipped":676,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:40:48.325: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:40:50.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-768" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":687,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:40:50.521: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 25 10:40:50.585: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:41:01.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6125" for this suite.

• [SLOW TEST:11.513 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":689,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:41:02.035: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 25 10:41:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
May 25 10:41:02.537: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 25 10:41:06.793: INFO: Waited 2.016840578s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:41:07.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5653" for this suite.

• [SLOW TEST:5.664 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":45,"skipped":697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:41:07.701: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 25 10:41:07.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90663 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:41:07.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90663 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 25 10:41:17.858: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90681 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:41:17.859: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90681 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 25 10:41:27.890: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90683 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:41:27.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90683 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 25 10:41:37.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90685 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:41:37.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2065  48d02e03-2790-4b0c-8c1e-a84b3105d717 90685 0 2021-05-25 10:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 25 10:41:47.947: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2065  61135a64-d741-4e1c-9474-0dcfe1be85d8 90687 0 2021-05-25 10:41:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:41:47.947: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2065  61135a64-d741-4e1c-9474-0dcfe1be85d8 90687 0 2021-05-25 10:41:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 25 10:41:57.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2065  61135a64-d741-4e1c-9474-0dcfe1be85d8 90689 0 2021-05-25 10:41:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:41:57.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2065  61135a64-d741-4e1c-9474-0dcfe1be85d8 90689 0 2021-05-25 10:41:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:42:07.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2065" for this suite.

• [SLOW TEST:60.303 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":46,"skipped":765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:42:08.004: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 10:42:08.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 10:42:10.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536128, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536128, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536128, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536128, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 10:42:13.503: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:42:13.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5201" for this suite.
STEP: Destroying namespace "webhook-5201-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.807 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":47,"skipped":790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:42:13.817: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 10:42:14.238: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 10:42:17.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
May 25 10:42:27.377: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:42:27.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7381" for this suite.
STEP: Destroying namespace "webhook-7381-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.990 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":48,"skipped":817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:42:27.807: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 25 10:42:27.945: INFO: Waiting up to 5m0s for pod "pod-6d7455fe-2d66-4925-8532-02879a9e4fbb" in namespace "emptydir-2839" to be "Succeeded or Failed"
May 25 10:42:27.952: INFO: Pod "pod-6d7455fe-2d66-4925-8532-02879a9e4fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.952363ms
May 25 10:42:29.969: INFO: Pod "pod-6d7455fe-2d66-4925-8532-02879a9e4fbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02358664s
STEP: Saw pod success
May 25 10:42:29.969: INFO: Pod "pod-6d7455fe-2d66-4925-8532-02879a9e4fbb" satisfied condition "Succeeded or Failed"
May 25 10:42:29.974: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-6d7455fe-2d66-4925-8532-02879a9e4fbb container test-container: <nil>
STEP: delete the pod
May 25 10:42:30.031: INFO: Waiting for pod pod-6d7455fe-2d66-4925-8532-02879a9e4fbb to disappear
May 25 10:42:30.038: INFO: Pod pod-6d7455fe-2d66-4925-8532-02879a9e4fbb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:42:30.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2839" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":874,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:42:30.058: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:42:30.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-480" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":50,"skipped":878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:42:30.151: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:42:30.214: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-26a8c179-fe3f-4901-adeb-4da17fabd93e" in namespace "security-context-test-5928" to be "Succeeded or Failed"
May 25 10:42:30.221: INFO: Pod "alpine-nnp-false-26a8c179-fe3f-4901-adeb-4da17fabd93e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.996974ms
May 25 10:42:32.228: INFO: Pod "alpine-nnp-false-26a8c179-fe3f-4901-adeb-4da17fabd93e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014171115s
May 25 10:42:32.228: INFO: Pod "alpine-nnp-false-26a8c179-fe3f-4901-adeb-4da17fabd93e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:42:32.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5928" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":937,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:42:32.262: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 25 10:42:32.834: INFO: Pod name wrapped-volume-race-ade03eff-fdd5-48e6-b501-dfea13549af4: Found 0 pods out of 5
May 25 10:42:37.859: INFO: Pod name wrapped-volume-race-ade03eff-fdd5-48e6-b501-dfea13549af4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ade03eff-fdd5-48e6-b501-dfea13549af4 in namespace emptydir-wrapper-1412, will wait for the garbage collector to delete the pods
May 25 10:42:49.972: INFO: Deleting ReplicationController wrapped-volume-race-ade03eff-fdd5-48e6-b501-dfea13549af4 took: 13.815999ms
May 25 10:42:50.672: INFO: Terminating ReplicationController wrapped-volume-race-ade03eff-fdd5-48e6-b501-dfea13549af4 pods took: 700.114092ms
STEP: Creating RC which spawns configmap-volume pods
May 25 10:43:02.338: INFO: Pod name wrapped-volume-race-8c0c0eaa-ef77-4bd4-b544-51469b86c502: Found 0 pods out of 5
May 25 10:43:07.354: INFO: Pod name wrapped-volume-race-8c0c0eaa-ef77-4bd4-b544-51469b86c502: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8c0c0eaa-ef77-4bd4-b544-51469b86c502 in namespace emptydir-wrapper-1412, will wait for the garbage collector to delete the pods
May 25 10:43:17.489: INFO: Deleting ReplicationController wrapped-volume-race-8c0c0eaa-ef77-4bd4-b544-51469b86c502 took: 12.92118ms
May 25 10:43:18.789: INFO: Terminating ReplicationController wrapped-volume-race-8c0c0eaa-ef77-4bd4-b544-51469b86c502 pods took: 1.300195831s
STEP: Creating RC which spawns configmap-volume pods
May 25 10:43:32.236: INFO: Pod name wrapped-volume-race-ec2bc9ab-8ac6-4261-9573-400220eef1c8: Found 0 pods out of 5
May 25 10:43:37.255: INFO: Pod name wrapped-volume-race-ec2bc9ab-8ac6-4261-9573-400220eef1c8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ec2bc9ab-8ac6-4261-9573-400220eef1c8 in namespace emptydir-wrapper-1412, will wait for the garbage collector to delete the pods
May 25 10:43:49.439: INFO: Deleting ReplicationController wrapped-volume-race-ec2bc9ab-8ac6-4261-9573-400220eef1c8 took: 31.137126ms
May 25 10:43:50.142: INFO: Terminating ReplicationController wrapped-volume-race-ec2bc9ab-8ac6-4261-9573-400220eef1c8 pods took: 702.614206ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:44:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1412" for this suite.

• [SLOW TEST:90.564 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":52,"skipped":940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:44:02.827: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 25 10:44:05.469: INFO: Successfully updated pod "labelsupdate90e5b139-7906-4314-89a6-29ae5f9ef793"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:44:09.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4293" for this suite.

• [SLOW TEST:6.744 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":971,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:44:09.573: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
May 25 10:44:11.698: INFO: Pod pod-hostip-e1f8fdd1-3ab1-4e60-8a99-d3afbc5b1dfc has hostIP: 10.156.0.14
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:44:11.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-921" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":976,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:44:11.718: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 25 10:44:11.778: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 10:45:11.815: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:45:11.822: INFO: Starting informer...
STEP: Starting pod...
May 25 10:45:12.054: INFO: Pod is running on gke-cluster-1-default-pool-8f0bb8bb-p6wx. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 25 10:45:12.319: INFO: Pod wasn't evicted. Proceeding
May 25 10:45:12.319: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 25 10:46:27.604: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:46:27.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3539" for this suite.

• [SLOW TEST:135.930 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":55,"skipped":990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:46:27.650: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
May 25 10:46:27.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1543 create -f -'
May 25 10:46:28.407: INFO: stderr: ""
May 25 10:46:28.407: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 25 10:46:28.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1543 diff -f -'
May 25 10:46:28.874: INFO: rc: 1
May 25 10:46:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1543 delete -f -'
May 25 10:46:29.086: INFO: stderr: ""
May 25 10:46:29.086: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:46:29.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1543" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":56,"skipped":1034,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:46:29.104: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 25 10:46:29.172: INFO: Waiting up to 5m0s for pod "pod-d8d002b1-04ae-48c7-a874-cff9990babb5" in namespace "emptydir-8085" to be "Succeeded or Failed"
May 25 10:46:29.178: INFO: Pod "pod-d8d002b1-04ae-48c7-a874-cff9990babb5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.997791ms
May 25 10:46:31.190: INFO: Pod "pod-d8d002b1-04ae-48c7-a874-cff9990babb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017936823s
STEP: Saw pod success
May 25 10:46:31.190: INFO: Pod "pod-d8d002b1-04ae-48c7-a874-cff9990babb5" satisfied condition "Succeeded or Failed"
May 25 10:46:31.196: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-d8d002b1-04ae-48c7-a874-cff9990babb5 container test-container: <nil>
STEP: delete the pod
May 25 10:46:31.257: INFO: Waiting for pod pod-d8d002b1-04ae-48c7-a874-cff9990babb5 to disappear
May 25 10:46:31.265: INFO: Pod pod-d8d002b1-04ae-48c7-a874-cff9990babb5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:46:31.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8085" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:46:31.287: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 25 10:46:31.435: INFO: Number of nodes with available pods: 0
May 25 10:46:31.435: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:32.482: INFO: Number of nodes with available pods: 0
May 25 10:46:32.482: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:33.452: INFO: Number of nodes with available pods: 2
May 25 10:46:33.452: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:46:34.455: INFO: Number of nodes with available pods: 3
May 25 10:46:34.455: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 25 10:46:34.508: INFO: Number of nodes with available pods: 2
May 25 10:46:34.508: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:35.527: INFO: Number of nodes with available pods: 2
May 25 10:46:35.527: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:36.531: INFO: Number of nodes with available pods: 2
May 25 10:46:36.531: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:37.537: INFO: Number of nodes with available pods: 2
May 25 10:46:37.538: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:38.540: INFO: Number of nodes with available pods: 2
May 25 10:46:38.540: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:39.525: INFO: Number of nodes with available pods: 2
May 25 10:46:39.525: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:40.526: INFO: Number of nodes with available pods: 2
May 25 10:46:40.526: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:41.526: INFO: Number of nodes with available pods: 2
May 25 10:46:41.527: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:42.536: INFO: Number of nodes with available pods: 2
May 25 10:46:42.536: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 10:46:43.530: INFO: Number of nodes with available pods: 3
May 25 10:46:43.530: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3695, will wait for the garbage collector to delete the pods
May 25 10:46:43.605: INFO: Deleting DaemonSet.extensions daemon-set took: 11.421056ms
May 25 10:46:44.305: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.377569ms
May 25 10:46:52.415: INFO: Number of nodes with available pods: 0
May 25 10:46:52.415: INFO: Number of running nodes: 0, number of available pods: 0
May 25 10:46:52.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"91978"},"items":null}

May 25 10:46:52.429: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"91978"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:46:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3695" for this suite.

• [SLOW TEST:21.190 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":58,"skipped":1061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:46:52.478: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-6a2c7bd6-6058-457a-aed8-0228b986e516
STEP: Creating a pod to test consume secrets
May 25 10:46:52.563: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432" in namespace "projected-3055" to be "Succeeded or Failed"
May 25 10:46:52.570: INFO: Pod "pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432": Phase="Pending", Reason="", readiness=false. Elapsed: 7.178912ms
May 25 10:46:54.576: INFO: Pod "pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013309254s
STEP: Saw pod success
May 25 10:46:54.576: INFO: Pod "pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432" satisfied condition "Succeeded or Failed"
May 25 10:46:54.583: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 10:46:54.625: INFO: Waiting for pod pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432 to disappear
May 25 10:46:54.641: INFO: Pod pod-projected-secrets-8e0443af-eacf-4d97-8055-c43005297432 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:46:54.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3055" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:46:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4358
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-4358
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4358
May 25 10:46:54.824: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 25 10:47:04.834: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 25 10:47:04.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 10:47:05.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 10:47:05.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 10:47:05.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 10:47:05.156: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 25 10:47:15.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 10:47:15.165: INFO: Waiting for statefulset status.replicas updated to 0
May 25 10:47:15.204: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:15.204: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:15.204: INFO: 
May 25 10:47:15.204: INFO: StatefulSet ss has not reached scale 3, at 1
May 25 10:47:16.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983531434s
May 25 10:47:17.244: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956182746s
May 25 10:47:18.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.943571447s
May 25 10:47:19.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.928647155s
May 25 10:47:20.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.918534402s
May 25 10:47:21.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.906333351s
May 25 10:47:22.307: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.89467647s
May 25 10:47:23.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.880486285s
May 25 10:47:24.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 867.633073ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4358
May 25 10:47:25.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 10:47:25.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 10:47:25.566: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 10:47:25.566: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 10:47:25.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 10:47:25.867: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 25 10:47:25.867: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 10:47:25.867: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 10:47:25.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 10:47:26.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 25 10:47:26.071: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 10:47:26.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 10:47:26.081: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 10:47:26.081: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 10:47:26.081: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 25 10:47:26.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 10:47:26.301: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 10:47:26.301: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 10:47:26.301: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 10:47:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 10:47:26.521: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 10:47:26.521: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 10:47:26.521: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 10:47:26.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-4358 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 10:47:26.728: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 10:47:26.728: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 10:47:26.728: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 10:47:26.728: INFO: Waiting for statefulset status.replicas updated to 0
May 25 10:47:26.736: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 25 10:47:36.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 10:47:36.763: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 25 10:47:36.763: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 25 10:47:36.796: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:36.796: INFO: ss-2  gke-cluster-1-default-pool-8f0bb8bb-wpkp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:36.796: INFO: ss-1  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:36.796: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:36.796: INFO: 
May 25 10:47:36.796: INFO: StatefulSet ss has not reached scale 0, at 3
May 25 10:47:37.808: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:37.808: INFO: ss-2  gke-cluster-1-default-pool-8f0bb8bb-wpkp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:37.808: INFO: ss-1  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:37.808: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:37.808: INFO: 
May 25 10:47:37.808: INFO: StatefulSet ss has not reached scale 0, at 3
May 25 10:47:38.822: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:38.822: INFO: ss-1  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:38.823: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:38.823: INFO: 
May 25 10:47:38.823: INFO: StatefulSet ss has not reached scale 0, at 2
May 25 10:47:39.835: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:39.835: INFO: ss-1  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:39.835: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:39.835: INFO: 
May 25 10:47:39.835: INFO: StatefulSet ss has not reached scale 0, at 2
May 25 10:47:40.851: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:40.851: INFO: ss-1  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:40.851: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:40.851: INFO: 
May 25 10:47:40.851: INFO: StatefulSet ss has not reached scale 0, at 2
May 25 10:47:41.862: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May 25 10:47:41.862: INFO: ss-1  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:15 +0000 UTC  }]
May 25 10:47:41.862: INFO: ss-0  gke-cluster-1-default-pool-8f0bb8bb-p6wx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:47:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 10:46:54 +0000 UTC  }]
May 25 10:47:41.862: INFO: 
May 25 10:47:41.862: INFO: StatefulSet ss has not reached scale 0, at 2
May 25 10:47:42.876: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.919273313s
May 25 10:47:43.885: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.904968806s
May 25 10:47:44.896: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.896638725s
May 25 10:47:45.908: INFO: Verifying statefulset ss doesn't scale past 0 for another 885.31428ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4358
May 25 10:47:46.926: INFO: Scaling statefulset ss to 0
May 25 10:47:46.948: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 25 10:47:46.955: INFO: Deleting all statefulset in ns statefulset-4358
May 25 10:47:46.961: INFO: Scaling statefulset ss to 0
May 25 10:47:47.013: INFO: Waiting for statefulset status.replicas updated to 0
May 25 10:47:47.022: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:47:47.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4358" for this suite.

• [SLOW TEST:52.419 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":60,"skipped":1117,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:47:47.081: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
May 25 10:47:47.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 create -f -'
May 25 10:47:47.446: INFO: stderr: ""
May 25 10:47:47.446: INFO: stdout: "pod/pause created\n"
May 25 10:47:47.446: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 25 10:47:47.447: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2802" to be "running and ready"
May 25 10:47:47.454: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.972127ms
May 25 10:47:49.467: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.02062017s
May 25 10:47:49.467: INFO: Pod "pause" satisfied condition "running and ready"
May 25 10:47:49.467: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
May 25 10:47:49.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 label pods pause testing-label=testing-label-value'
May 25 10:47:49.586: INFO: stderr: ""
May 25 10:47:49.586: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 25 10:47:49.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 get pod pause -L testing-label'
May 25 10:47:49.708: INFO: stderr: ""
May 25 10:47:49.708: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 25 10:47:49.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 label pods pause testing-label-'
May 25 10:47:49.824: INFO: stderr: ""
May 25 10:47:49.824: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 25 10:47:49.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 get pod pause -L testing-label'
May 25 10:47:49.922: INFO: stderr: ""
May 25 10:47:49.922: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
May 25 10:47:49.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 delete --grace-period=0 --force -f -'
May 25 10:47:50.064: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 10:47:50.064: INFO: stdout: "pod \"pause\" force deleted\n"
May 25 10:47:50.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 get rc,svc -l name=pause --no-headers'
May 25 10:47:50.249: INFO: stderr: "No resources found in kubectl-2802 namespace.\n"
May 25 10:47:50.249: INFO: stdout: ""
May 25 10:47:50.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-2802 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 25 10:47:50.379: INFO: stderr: ""
May 25 10:47:50.379: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:47:50.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2802" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":61,"skipped":1118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:47:50.397: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
May 25 10:47:50.484: INFO: Waiting up to 5m0s for pod "var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a" in namespace "var-expansion-6621" to be "Succeeded or Failed"
May 25 10:47:50.492: INFO: Pod "var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.531042ms
May 25 10:47:52.507: INFO: Pod "var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023252699s
STEP: Saw pod success
May 25 10:47:52.507: INFO: Pod "var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a" satisfied condition "Succeeded or Failed"
May 25 10:47:52.514: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a container dapi-container: <nil>
STEP: delete the pod
May 25 10:47:52.548: INFO: Waiting for pod var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a to disappear
May 25 10:47:52.556: INFO: Pod var-expansion-560f7e92-a097-4b4e-956e-7d6ca6c4353a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:47:52.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6621" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":62,"skipped":1145,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:47:52.577: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 25 10:47:55.250: INFO: Successfully updated pod "labelsupdate82cbba3f-5f46-41c6-917b-9221ceb4a7b7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:47:59.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1721" for this suite.

• [SLOW TEST:6.779 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:47:59.360: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 25 10:47:59.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8253  67934e5c-c23c-4060-9e90-a31179349db0 92344 0 2021-05-25 10:47:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-25 10:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 10:47:59.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8253  67934e5c-c23c-4060-9e90-a31179349db0 92345 0 2021-05-25 10:47:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-25 10:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:47:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8253" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":64,"skipped":1202,"failed":0}
SSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:47:59.616: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 10:47:59.855: INFO: starting watch
STEP: patching
STEP: updating
May 25 10:47:59.874: INFO: waiting for watch events with expected annotations
May 25 10:47:59.874: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:47:59.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7560" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":65,"skipped":1206,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:47:59.940: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-drkbi in namespace proxy-4523
I0525 10:48:00.088368      19 runners.go:190] Created replication controller with name: proxy-service-drkbi, namespace: proxy-4523, replica count: 1
I0525 10:48:01.139339      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0525 10:48:02.139648      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:03.139989      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:04.140204      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:05.140457      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:06.140665      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:07.140896      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:08.141292      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 10:48:09.141597      19 runners.go:190] proxy-service-drkbi Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 10:48:09.167: INFO: setup took 9.179965227s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 25 10:48:09.245: INFO: (0) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 77.618402ms)
May 25 10:48:09.245: INFO: (0) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 77.069224ms)
May 25 10:48:09.246: INFO: (0) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 78.189638ms)
May 25 10:48:09.249: INFO: (0) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 80.932508ms)
May 25 10:48:09.252: INFO: (0) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 83.793268ms)
May 25 10:48:09.252: INFO: (0) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 84.230075ms)
May 25 10:48:09.255: INFO: (0) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 87.944542ms)
May 25 10:48:09.255: INFO: (0) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 86.988951ms)
May 25 10:48:09.255: INFO: (0) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 87.19286ms)
May 25 10:48:09.255: INFO: (0) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 87.577013ms)
May 25 10:48:09.256: INFO: (0) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 88.039899ms)
May 25 10:48:09.260: INFO: (0) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 93.101052ms)
May 25 10:48:09.261: INFO: (0) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 92.98587ms)
May 25 10:48:09.261: INFO: (0) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 93.211334ms)
May 25 10:48:09.261: INFO: (0) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 93.261641ms)
May 25 10:48:09.263: INFO: (0) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 95.335375ms)
May 25 10:48:09.283: INFO: (1) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 19.019854ms)
May 25 10:48:09.283: INFO: (1) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 19.454776ms)
May 25 10:48:09.291: INFO: (1) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 26.770426ms)
May 25 10:48:09.294: INFO: (1) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 31.302566ms)
May 25 10:48:09.297: INFO: (1) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 33.266996ms)
May 25 10:48:09.298: INFO: (1) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 33.674305ms)
May 25 10:48:09.298: INFO: (1) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 33.883044ms)
May 25 10:48:09.298: INFO: (1) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 33.742528ms)
May 25 10:48:09.298: INFO: (1) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 34.427569ms)
May 25 10:48:09.299: INFO: (1) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 34.268056ms)
May 25 10:48:09.299: INFO: (1) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 34.813341ms)
May 25 10:48:09.299: INFO: (1) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 34.733965ms)
May 25 10:48:09.299: INFO: (1) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 35.149673ms)
May 25 10:48:09.299: INFO: (1) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 35.101997ms)
May 25 10:48:09.300: INFO: (1) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 35.758195ms)
May 25 10:48:09.300: INFO: (1) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 35.994658ms)
May 25 10:48:09.325: INFO: (2) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 24.23271ms)
May 25 10:48:09.325: INFO: (2) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 24.32718ms)
May 25 10:48:09.334: INFO: (2) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 33.195748ms)
May 25 10:48:09.334: INFO: (2) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 33.532346ms)
May 25 10:48:09.334: INFO: (2) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 32.829139ms)
May 25 10:48:09.339: INFO: (2) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 37.300675ms)
May 25 10:48:09.339: INFO: (2) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 38.607705ms)
May 25 10:48:09.339: INFO: (2) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 37.854461ms)
May 25 10:48:09.339: INFO: (2) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 38.728323ms)
May 25 10:48:09.339: INFO: (2) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 38.591672ms)
May 25 10:48:09.339: INFO: (2) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 37.735667ms)
May 25 10:48:09.340: INFO: (2) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 38.495935ms)
May 25 10:48:09.342: INFO: (2) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 42.132165ms)
May 25 10:48:09.342: INFO: (2) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 40.863658ms)
May 25 10:48:09.345: INFO: (2) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 44.121339ms)
May 25 10:48:09.345: INFO: (2) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 44.548917ms)
May 25 10:48:09.372: INFO: (3) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 26.504434ms)
May 25 10:48:09.384: INFO: (3) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 38.168102ms)
May 25 10:48:09.395: INFO: (3) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 48.766787ms)
May 25 10:48:09.395: INFO: (3) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 47.972379ms)
May 25 10:48:09.395: INFO: (3) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 47.935214ms)
May 25 10:48:09.403: INFO: (3) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 56.592698ms)
May 25 10:48:09.418: INFO: (3) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 71.62304ms)
May 25 10:48:09.418: INFO: (3) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 71.722882ms)
May 25 10:48:09.418: INFO: (3) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 71.092737ms)
May 25 10:48:09.428: INFO: (3) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 81.900786ms)
May 25 10:48:09.428: INFO: (3) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 81.461743ms)
May 25 10:48:09.429: INFO: (3) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 83.202244ms)
May 25 10:48:09.429: INFO: (3) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 82.017194ms)
May 25 10:48:09.429: INFO: (3) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 82.706824ms)
May 25 10:48:09.429: INFO: (3) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 82.646711ms)
May 25 10:48:09.429: INFO: (3) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 82.734732ms)
May 25 10:48:09.496: INFO: (4) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 65.659984ms)
May 25 10:48:09.496: INFO: (4) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 65.755696ms)
May 25 10:48:09.496: INFO: (4) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 66.223641ms)
May 25 10:48:09.501: INFO: (4) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 70.859705ms)
May 25 10:48:09.503: INFO: (4) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 73.428897ms)
May 25 10:48:09.510: INFO: (4) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 80.09513ms)
May 25 10:48:09.510: INFO: (4) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 79.877709ms)
May 25 10:48:09.515: INFO: (4) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 85.649972ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 90.074283ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 89.943814ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 90.809793ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 90.57977ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 90.783246ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 90.767692ms)
May 25 10:48:09.520: INFO: (4) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 90.957906ms)
May 25 10:48:09.538: INFO: (4) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 108.54996ms)
May 25 10:48:09.564: INFO: (5) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 24.697636ms)
May 25 10:48:09.567: INFO: (5) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 27.881091ms)
May 25 10:48:09.570: INFO: (5) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 30.670116ms)
May 25 10:48:09.574: INFO: (5) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 34.579693ms)
May 25 10:48:09.578: INFO: (5) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 38.653521ms)
May 25 10:48:09.579: INFO: (5) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 38.692179ms)
May 25 10:48:09.596: INFO: (5) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 56.119941ms)
May 25 10:48:09.596: INFO: (5) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 56.977731ms)
May 25 10:48:09.596: INFO: (5) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 56.464064ms)
May 25 10:48:09.596: INFO: (5) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 56.370782ms)
May 25 10:48:09.597: INFO: (5) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 57.098205ms)
May 25 10:48:09.597: INFO: (5) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 56.884109ms)
May 25 10:48:09.598: INFO: (5) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 57.983861ms)
May 25 10:48:09.602: INFO: (5) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 62.618539ms)
May 25 10:48:09.602: INFO: (5) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 62.293608ms)
May 25 10:48:09.603: INFO: (5) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 62.615497ms)
May 25 10:48:09.799: INFO: (6) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 195.312015ms)
May 25 10:48:09.811: INFO: (6) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 206.9496ms)
May 25 10:48:09.811: INFO: (6) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 207.018165ms)
May 25 10:48:09.812: INFO: (6) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 207.845153ms)
May 25 10:48:09.817: INFO: (6) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 213.010354ms)
May 25 10:48:09.818: INFO: (6) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 214.197567ms)
May 25 10:48:09.826: INFO: (6) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 221.798201ms)
May 25 10:48:09.860: INFO: (6) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 256.275955ms)
May 25 10:48:09.864: INFO: (6) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 260.074697ms)
May 25 10:48:09.873: INFO: (6) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 270.062969ms)
May 25 10:48:09.874: INFO: (6) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 269.753025ms)
May 25 10:48:09.884: INFO: (6) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 279.911048ms)
May 25 10:48:09.888: INFO: (6) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 284.843558ms)
May 25 10:48:09.889: INFO: (6) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 285.589737ms)
May 25 10:48:09.891: INFO: (6) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 287.243919ms)
May 25 10:48:09.897: INFO: (6) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 293.226486ms)
May 25 10:48:10.137: INFO: (7) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 238.933896ms)
May 25 10:48:10.159: INFO: (7) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 260.867078ms)
May 25 10:48:10.159: INFO: (7) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 260.533537ms)
May 25 10:48:10.159: INFO: (7) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 260.794661ms)
May 25 10:48:10.159: INFO: (7) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 260.988212ms)
May 25 10:48:10.168: INFO: (7) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 269.772664ms)
May 25 10:48:10.172: INFO: (7) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 272.914687ms)
May 25 10:48:10.172: INFO: (7) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 273.118651ms)
May 25 10:48:10.172: INFO: (7) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 274.173418ms)
May 25 10:48:10.173: INFO: (7) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 274.53721ms)
May 25 10:48:10.173: INFO: (7) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 274.347522ms)
May 25 10:48:10.181: INFO: (7) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 282.742302ms)
May 25 10:48:10.181: INFO: (7) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 283.171327ms)
May 25 10:48:10.181: INFO: (7) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 282.604645ms)
May 25 10:48:10.181: INFO: (7) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 282.477074ms)
May 25 10:48:10.182: INFO: (7) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 283.087266ms)
May 25 10:48:10.209: INFO: (8) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 27.121396ms)
May 25 10:48:10.216: INFO: (8) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 34.22783ms)
May 25 10:48:10.229: INFO: (8) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 47.171819ms)
May 25 10:48:10.229: INFO: (8) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 46.877519ms)
May 25 10:48:10.233: INFO: (8) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 50.937215ms)
May 25 10:48:10.235: INFO: (8) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 53.163426ms)
May 25 10:48:10.247: INFO: (8) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 64.395808ms)
May 25 10:48:10.247: INFO: (8) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 64.37249ms)
May 25 10:48:10.251: INFO: (8) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 69.09462ms)
May 25 10:48:10.251: INFO: (8) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 68.839152ms)
May 25 10:48:10.253: INFO: (8) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 71.045537ms)
May 25 10:48:10.253: INFO: (8) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 70.854154ms)
May 25 10:48:10.254: INFO: (8) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 70.922992ms)
May 25 10:48:10.254: INFO: (8) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 71.549183ms)
May 25 10:48:10.254: INFO: (8) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 71.936219ms)
May 25 10:48:10.257: INFO: (8) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 75.083447ms)
May 25 10:48:10.271: INFO: (9) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 13.246303ms)
May 25 10:48:10.280: INFO: (9) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 22.549524ms)
May 25 10:48:10.286: INFO: (9) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 28.116163ms)
May 25 10:48:10.288: INFO: (9) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 30.612536ms)
May 25 10:48:10.288: INFO: (9) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 30.586343ms)
May 25 10:48:10.297: INFO: (9) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 38.234984ms)
May 25 10:48:10.297: INFO: (9) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 38.767838ms)
May 25 10:48:10.298: INFO: (9) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 39.735974ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 42.722982ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 42.571234ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 43.418542ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 42.881046ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 43.426059ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 43.131629ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 43.386623ms)
May 25 10:48:10.301: INFO: (9) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 43.293423ms)
May 25 10:48:10.326: INFO: (10) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 23.651044ms)
May 25 10:48:10.334: INFO: (10) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 31.744464ms)
May 25 10:48:10.334: INFO: (10) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 31.072062ms)
May 25 10:48:10.334: INFO: (10) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 30.872939ms)
May 25 10:48:10.334: INFO: (10) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 31.414975ms)
May 25 10:48:10.334: INFO: (10) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 31.961612ms)
May 25 10:48:10.338: INFO: (10) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 35.742107ms)
May 25 10:48:10.338: INFO: (10) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 36.337774ms)
May 25 10:48:10.339: INFO: (10) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 36.397071ms)
May 25 10:48:10.339: INFO: (10) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 36.061336ms)
May 25 10:48:10.339: INFO: (10) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 36.223072ms)
May 25 10:48:10.339: INFO: (10) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 36.326504ms)
May 25 10:48:10.340: INFO: (10) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 37.675882ms)
May 25 10:48:10.340: INFO: (10) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 38.0588ms)
May 25 10:48:10.341: INFO: (10) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 37.449165ms)
May 25 10:48:10.341: INFO: (10) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 37.567877ms)
May 25 10:48:10.375: INFO: (11) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 33.978591ms)
May 25 10:48:10.376: INFO: (11) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 34.675764ms)
May 25 10:48:10.376: INFO: (11) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 34.744267ms)
May 25 10:48:10.376: INFO: (11) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 35.063037ms)
May 25 10:48:10.376: INFO: (11) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 34.83905ms)
May 25 10:48:10.376: INFO: (11) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 34.855936ms)
May 25 10:48:10.376: INFO: (11) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 34.768658ms)
May 25 10:48:10.384: INFO: (11) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 42.874197ms)
May 25 10:48:10.384: INFO: (11) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 42.798103ms)
May 25 10:48:10.385: INFO: (11) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 43.168932ms)
May 25 10:48:10.385: INFO: (11) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 43.57231ms)
May 25 10:48:10.385: INFO: (11) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 43.842474ms)
May 25 10:48:10.385: INFO: (11) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 43.753766ms)
May 25 10:48:10.385: INFO: (11) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 43.94857ms)
May 25 10:48:10.386: INFO: (11) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 43.946835ms)
May 25 10:48:10.386: INFO: (11) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 44.13679ms)
May 25 10:48:10.421: INFO: (12) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 35.039382ms)
May 25 10:48:10.421: INFO: (12) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 34.905123ms)
May 25 10:48:10.423: INFO: (12) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 35.966315ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 42.390052ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 42.91739ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 42.28256ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 42.193484ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 42.302273ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 42.314671ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 42.717039ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 42.766384ms)
May 25 10:48:10.429: INFO: (12) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 42.661717ms)
May 25 10:48:10.430: INFO: (12) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 43.925087ms)
May 25 10:48:10.430: INFO: (12) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 43.99917ms)
May 25 10:48:10.430: INFO: (12) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 44.060701ms)
May 25 10:48:10.430: INFO: (12) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 44.172485ms)
May 25 10:48:10.472: INFO: (13) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 41.222716ms)
May 25 10:48:10.472: INFO: (13) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 41.508939ms)
May 25 10:48:10.476: INFO: (13) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 44.500144ms)
May 25 10:48:10.476: INFO: (13) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 44.755559ms)
May 25 10:48:10.476: INFO: (13) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 44.227247ms)
May 25 10:48:10.476: INFO: (13) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 44.448606ms)
May 25 10:48:10.477: INFO: (13) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 45.798213ms)
May 25 10:48:10.477: INFO: (13) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 45.5549ms)
May 25 10:48:10.477: INFO: (13) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 45.821831ms)
May 25 10:48:10.478: INFO: (13) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 46.085868ms)
May 25 10:48:10.478: INFO: (13) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 46.812908ms)
May 25 10:48:10.478: INFO: (13) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 46.797936ms)
May 25 10:48:10.478: INFO: (13) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 46.563364ms)
May 25 10:48:10.479: INFO: (13) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 47.82051ms)
May 25 10:48:10.479: INFO: (13) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 48.891999ms)
May 25 10:48:10.479: INFO: (13) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 47.948999ms)
May 25 10:48:10.511: INFO: (14) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 30.54608ms)
May 25 10:48:10.511: INFO: (14) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 30.609728ms)
May 25 10:48:10.511: INFO: (14) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 30.819375ms)
May 25 10:48:10.512: INFO: (14) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 31.810517ms)
May 25 10:48:10.512: INFO: (14) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 31.66059ms)
May 25 10:48:10.512: INFO: (14) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 31.780687ms)
May 25 10:48:10.512: INFO: (14) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 31.719162ms)
May 25 10:48:10.517: INFO: (14) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 36.737505ms)
May 25 10:48:10.517: INFO: (14) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 36.340352ms)
May 25 10:48:10.517: INFO: (14) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 36.181256ms)
May 25 10:48:10.517: INFO: (14) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 36.5095ms)
May 25 10:48:10.518: INFO: (14) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 37.596734ms)
May 25 10:48:10.518: INFO: (14) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 37.711262ms)
May 25 10:48:10.518: INFO: (14) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 38.025744ms)
May 25 10:48:10.518: INFO: (14) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 37.785119ms)
May 25 10:48:10.519: INFO: (14) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 38.144243ms)
May 25 10:48:10.540: INFO: (15) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 20.994681ms)
May 25 10:48:10.542: INFO: (15) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 22.471642ms)
May 25 10:48:10.556: INFO: (15) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 36.071145ms)
May 25 10:48:10.560: INFO: (15) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 40.479452ms)
May 25 10:48:10.560: INFO: (15) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 39.887419ms)
May 25 10:48:10.560: INFO: (15) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 40.714541ms)
May 25 10:48:10.561: INFO: (15) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 40.70017ms)
May 25 10:48:10.561: INFO: (15) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 41.700356ms)
May 25 10:48:10.561: INFO: (15) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 41.448657ms)
May 25 10:48:10.562: INFO: (15) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 41.610451ms)
May 25 10:48:10.562: INFO: (15) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 41.426763ms)
May 25 10:48:10.562: INFO: (15) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 41.905795ms)
May 25 10:48:10.562: INFO: (15) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 41.637331ms)
May 25 10:48:10.562: INFO: (15) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 42.388436ms)
May 25 10:48:10.563: INFO: (15) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 42.218161ms)
May 25 10:48:10.563: INFO: (15) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 42.524605ms)
May 25 10:48:10.604: INFO: (16) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 40.506536ms)
May 25 10:48:10.610: INFO: (16) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 45.975927ms)
May 25 10:48:10.610: INFO: (16) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 46.565221ms)
May 25 10:48:10.611: INFO: (16) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 46.128015ms)
May 25 10:48:10.611: INFO: (16) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 46.716245ms)
May 25 10:48:10.611: INFO: (16) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 46.996198ms)
May 25 10:48:10.611: INFO: (16) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 47.869767ms)
May 25 10:48:10.615: INFO: (16) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 51.109255ms)
May 25 10:48:10.615: INFO: (16) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 51.190687ms)
May 25 10:48:10.615: INFO: (16) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 51.614925ms)
May 25 10:48:10.619: INFO: (16) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 55.395485ms)
May 25 10:48:10.619: INFO: (16) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 55.569702ms)
May 25 10:48:10.620: INFO: (16) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 55.043575ms)
May 25 10:48:10.624: INFO: (16) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 59.66997ms)
May 25 10:48:10.624: INFO: (16) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 60.475317ms)
May 25 10:48:10.624: INFO: (16) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 60.074331ms)
May 25 10:48:10.643: INFO: (17) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 17.98007ms)
May 25 10:48:10.643: INFO: (17) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 17.850454ms)
May 25 10:48:10.647: INFO: (17) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 22.1169ms)
May 25 10:48:10.651: INFO: (17) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 25.731592ms)
May 25 10:48:10.658: INFO: (17) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 32.247734ms)
May 25 10:48:10.658: INFO: (17) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 32.402916ms)
May 25 10:48:10.658: INFO: (17) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 32.314197ms)
May 25 10:48:10.659: INFO: (17) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 33.639714ms)
May 25 10:48:10.660: INFO: (17) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 34.784501ms)
May 25 10:48:10.660: INFO: (17) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 34.907731ms)
May 25 10:48:10.662: INFO: (17) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 36.899651ms)
May 25 10:48:10.663: INFO: (17) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 38.190885ms)
May 25 10:48:10.664: INFO: (17) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 38.082415ms)
May 25 10:48:10.664: INFO: (17) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 38.426378ms)
May 25 10:48:10.665: INFO: (17) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 38.998757ms)
May 25 10:48:10.665: INFO: (17) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 39.013379ms)
May 25 10:48:10.688: INFO: (18) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 23.038244ms)
May 25 10:48:10.690: INFO: (18) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 24.811517ms)
May 25 10:48:10.699: INFO: (18) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 34.093073ms)
May 25 10:48:10.699: INFO: (18) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 33.999426ms)
May 25 10:48:10.700: INFO: (18) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 34.363062ms)
May 25 10:48:10.701: INFO: (18) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 35.373642ms)
May 25 10:48:10.704: INFO: (18) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 39.502206ms)
May 25 10:48:10.705: INFO: (18) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 39.707112ms)
May 25 10:48:10.708: INFO: (18) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 42.288359ms)
May 25 10:48:10.708: INFO: (18) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 43.192018ms)
May 25 10:48:10.708: INFO: (18) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 43.027624ms)
May 25 10:48:10.708: INFO: (18) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 43.185322ms)
May 25 10:48:10.709: INFO: (18) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 43.335676ms)
May 25 10:48:10.709: INFO: (18) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 43.210899ms)
May 25 10:48:10.709: INFO: (18) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 43.396272ms)
May 25 10:48:10.709: INFO: (18) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 43.294032ms)
May 25 10:48:10.733: INFO: (19) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:162/proxy/: bar (200; 23.266235ms)
May 25 10:48:10.735: INFO: (19) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th/proxy/rewriteme">test</a> (200; 25.490285ms)
May 25 10:48:10.736: INFO: (19) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:462/proxy/: tls qux (200; 25.8825ms)
May 25 10:48:10.739: INFO: (19) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname1/proxy/: foo (200; 29.483027ms)
May 25 10:48:10.739: INFO: (19) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:1080/proxy/rewriteme">... (200; 29.238742ms)
May 25 10:48:10.742: INFO: (19) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname1/proxy/: foo (200; 32.618209ms)
May 25 10:48:10.744: INFO: (19) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:160/proxy/: foo (200; 34.286562ms)
May 25 10:48:10.744: INFO: (19) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:1080/proxy/rewriteme">test<... (200; 34.331771ms)
May 25 10:48:10.744: INFO: (19) /api/v1/namespaces/proxy-4523/pods/http:proxy-service-drkbi-c54th:162/proxy/: bar (200; 34.045683ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:460/proxy/: tls baz (200; 37.657053ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/pods/proxy-service-drkbi-c54th:160/proxy/: foo (200; 37.807666ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/services/proxy-service-drkbi:portname2/proxy/: bar (200; 37.884462ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname2/proxy/: tls qux (200; 38.168092ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/: <a href="/api/v1/namespaces/proxy-4523/pods/https:proxy-service-drkbi-c54th:443/proxy/tlsrewritem... (200; 37.868061ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/services/http:proxy-service-drkbi:portname2/proxy/: bar (200; 38.2774ms)
May 25 10:48:10.748: INFO: (19) /api/v1/namespaces/proxy-4523/services/https:proxy-service-drkbi:tlsportname1/proxy/: tls baz (200; 38.188256ms)
STEP: deleting ReplicationController proxy-service-drkbi in namespace proxy-4523, will wait for the garbage collector to delete the pods
May 25 10:48:10.822: INFO: Deleting ReplicationController proxy-service-drkbi took: 12.665875ms
May 25 10:48:10.922: INFO: Terminating ReplicationController proxy-service-drkbi pods took: 100.454381ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:48:22.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4523" for this suite.

• [SLOW TEST:22.116 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":66,"skipped":1212,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:48:22.055: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:48:22.118: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:48:23.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4407" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":67,"skipped":1216,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:48:23.320: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 10:48:23.403: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 10:49:23.451: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:23.459: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:49:23.558: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 25 10:49:23.566: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:23.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8161" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:23.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-314" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.439 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":68,"skipped":1217,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:23.759: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0525 10:49:29.884244      19 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0525 10:49:29.884389      19 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0525 10:49:29.884417      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 10:49:29.884: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:29.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9355" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":69,"skipped":1225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:29.909: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-a5f59b17-6b92-4c9d-b456-5f361f63826f
STEP: Creating a pod to test consume configMaps
May 25 10:49:29.989: INFO: Waiting up to 5m0s for pod "pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944" in namespace "configmap-2378" to be "Succeeded or Failed"
May 25 10:49:29.997: INFO: Pod "pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944": Phase="Pending", Reason="", readiness=false. Elapsed: 7.033524ms
May 25 10:49:32.015: INFO: Pod "pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02505347s
STEP: Saw pod success
May 25 10:49:32.015: INFO: Pod "pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944" satisfied condition "Succeeded or Failed"
May 25 10:49:32.021: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944 container agnhost-container: <nil>
STEP: delete the pod
May 25 10:49:32.069: INFO: Waiting for pod pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944 to disappear
May 25 10:49:32.075: INFO: Pod pod-configmaps-599975a0-5d46-44f4-9d1e-e9ed6554a944 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:32.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2378" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1255,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:32.101: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:49:32.159: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 25 10:49:34.278: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:34.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6030" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":71,"skipped":1260,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:34.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9584" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":72,"skipped":1266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:34.508: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-899a844e-e7f0-4fed-9d27-60a9950ac5f6
STEP: Creating a pod to test consume secrets
May 25 10:49:34.584: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744" in namespace "projected-3849" to be "Succeeded or Failed"
May 25 10:49:34.591: INFO: Pod "pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744": Phase="Pending", Reason="", readiness=false. Elapsed: 7.143987ms
May 25 10:49:36.605: INFO: Pod "pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021649272s
STEP: Saw pod success
May 25 10:49:36.605: INFO: Pod "pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744" satisfied condition "Succeeded or Failed"
May 25 10:49:36.611: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 10:49:36.659: INFO: Waiting for pod pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744 to disappear
May 25 10:49:36.669: INFO: Pod pod-projected-secrets-6e373a9a-0d3b-4892-8706-f9248640d744 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3849" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:36.689: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:49:36.745: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 25 10:49:36.759: INFO: Pod name sample-pod: Found 0 pods out of 1
May 25 10:49:41.777: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 25 10:49:41.777: INFO: Creating deployment "test-rolling-update-deployment"
May 25 10:49:41.790: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 25 10:49:41.813: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 25 10:49:43.868: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 25 10:49:43.880: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 25 10:49:43.911: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4470  9a49b079-ad07-4d78-8051-90d8f75763e0 92890 1 2021-05-25 10:49:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-25 10:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {k3s Update apps/v1 2021-05-25 10:49:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d798d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-25 10:49:41 +0000 UTC,LastTransitionTime:2021-05-25 10:49:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-05-25 10:49:43 +0000 UTC,LastTransitionTime:2021-05-25 10:49:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 25 10:49:43.931: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-4470  12663027-5169-4888-b33b-4aebf72bc5cc 92881 1 2021-05-25 10:49:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9a49b079-ad07-4d78-8051-90d8f75763e0 0xc002d79c67 0xc002d79c68}] []  [{k3s Update apps/v1 2021-05-25 10:49:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a49b079-ad07-4d78-8051-90d8f75763e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d79cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 25 10:49:43.931: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 25 10:49:43.931: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4470  e4d32fd1-6c26-45c3-b8e1-b785284474b3 92889 2 2021-05-25 10:49:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9a49b079-ad07-4d78-8051-90d8f75763e0 0xc002d79d57 0xc002d79d58}] []  [{e2e.test Update apps/v1 2021-05-25 10:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {k3s Update apps/v1 2021-05-25 10:49:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a49b079-ad07-4d78-8051-90d8f75763e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d79df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 10:49:43.940: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-hnrlb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-hnrlb test-rolling-update-deployment-6b6bf9df46- deployment-4470  9e2ab9b5-3ad3-46f8-b78c-a19461f840c4 92879 0 2021-05-25 10:49:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 12663027-5169-4888-b33b-4aebf72bc5cc 0xc002e84227 0xc002e84228}] []  [{k3s Update v1 2021-05-25 10:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12663027-5169-4888-b33b-4aebf72bc5cc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 10:49:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wwg5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wwg5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wwg5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:49:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:49:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:49:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 10:49:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.71,StartTime:2021-05-25 10:49:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 10:49:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://a7f21df3a7d2d3d7bd50ff429d242ac344d6396ac6582a5e7ef16100aef48f49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:49:43.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4470" for this suite.

• [SLOW TEST:7.290 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":74,"skipped":1339,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:49:43.979: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:50:05.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7240" for this suite.

• [SLOW TEST:21.782 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:50:05.764: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-3667f443-279e-4f10-90b8-de9ca0580909
STEP: Creating a pod to test consume configMaps
May 25 10:50:05.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3" in namespace "projected-3078" to be "Succeeded or Failed"
May 25 10:50:05.852: INFO: Pod "pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079261ms
May 25 10:50:07.873: INFO: Pod "pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02793182s
STEP: Saw pod success
May 25 10:50:07.873: INFO: Pod "pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3" satisfied condition "Succeeded or Failed"
May 25 10:50:07.880: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3 container agnhost-container: <nil>
STEP: delete the pod
May 25 10:50:07.914: INFO: Waiting for pod pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3 to disappear
May 25 10:50:07.932: INFO: Pod pod-projected-configmaps-455f8eaa-ba12-4761-9cd1-2989a74a00a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:50:07.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3078" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1365,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:50:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-d18b4aa7-6f97-4049-bd7d-21f5cd05fc54
STEP: Creating a pod to test consume configMaps
May 25 10:50:08.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f" in namespace "projected-659" to be "Succeeded or Failed"
May 25 10:50:08.062: INFO: Pod "pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073256ms
May 25 10:50:10.078: INFO: Pod "pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026200686s
STEP: Saw pod success
May 25 10:50:10.078: INFO: Pod "pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f" satisfied condition "Succeeded or Failed"
May 25 10:50:10.086: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 25 10:50:10.124: INFO: Waiting for pod pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f to disappear
May 25 10:50:10.134: INFO: Pod pod-projected-configmaps-04c31ad7-89e0-41a8-85a2-f9b09ef6322f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:50:10.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-659" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:50:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:50:12.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1732" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:50:12.316: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0525 10:50:52.479448      19 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0525 10:50:52.479478      19 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0525 10:50:52.479486      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 10:50:52.479: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 25 10:50:52.479: INFO: Deleting pod "simpletest.rc-hpff9" in namespace "gc-5932"
May 25 10:50:52.497: INFO: Deleting pod "simpletest.rc-tg9kt" in namespace "gc-5932"
May 25 10:50:52.515: INFO: Deleting pod "simpletest.rc-wjtbq" in namespace "gc-5932"
May 25 10:50:52.533: INFO: Deleting pod "simpletest.rc-x8bwl" in namespace "gc-5932"
May 25 10:50:52.556: INFO: Deleting pod "simpletest.rc-pc9b2" in namespace "gc-5932"
May 25 10:50:52.573: INFO: Deleting pod "simpletest.rc-724f5" in namespace "gc-5932"
May 25 10:50:52.594: INFO: Deleting pod "simpletest.rc-dmkzt" in namespace "gc-5932"
May 25 10:50:52.613: INFO: Deleting pod "simpletest.rc-s7k8m" in namespace "gc-5932"
May 25 10:50:52.633: INFO: Deleting pod "simpletest.rc-jmsh5" in namespace "gc-5932"
May 25 10:50:52.650: INFO: Deleting pod "simpletest.rc-4skrj" in namespace "gc-5932"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:50:52.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5932" for this suite.

• [SLOW TEST:40.370 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":79,"skipped":1465,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:50:52.691: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 25 10:50:55.340: INFO: Successfully updated pod "annotationupdate8712c294-fd75-4dfb-b54f-9d1dbbd76b1e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:50:57.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7585" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1478,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:50:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-m2p6
STEP: Creating a pod to test atomic-volume-subpath
May 25 10:50:57.502: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-m2p6" in namespace "subpath-402" to be "Succeeded or Failed"
May 25 10:50:57.510: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.738328ms
May 25 10:50:59.524: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 2.021494655s
May 25 10:51:01.540: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 4.038368533s
May 25 10:51:03.558: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 6.056204261s
May 25 10:51:05.571: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 8.069347656s
May 25 10:51:07.611: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 10.10914813s
May 25 10:51:09.628: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 12.12602199s
May 25 10:51:11.644: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 14.141863536s
May 25 10:51:13.659: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 16.156460453s
May 25 10:51:15.669: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 18.166708992s
May 25 10:51:17.701: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Running", Reason="", readiness=true. Elapsed: 20.199247961s
May 25 10:51:19.719: INFO: Pod "pod-subpath-test-downwardapi-m2p6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.21662751s
STEP: Saw pod success
May 25 10:51:19.719: INFO: Pod "pod-subpath-test-downwardapi-m2p6" satisfied condition "Succeeded or Failed"
May 25 10:51:19.725: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-subpath-test-downwardapi-m2p6 container test-container-subpath-downwardapi-m2p6: <nil>
STEP: delete the pod
May 25 10:51:19.766: INFO: Waiting for pod pod-subpath-test-downwardapi-m2p6 to disappear
May 25 10:51:19.772: INFO: Pod pod-subpath-test-downwardapi-m2p6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-m2p6
May 25 10:51:19.773: INFO: Deleting pod "pod-subpath-test-downwardapi-m2p6" in namespace "subpath-402"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:51:19.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-402" for this suite.

• [SLOW TEST:22.384 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":81,"skipped":1483,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:51:19.798: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-630
May 25 10:51:21.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-630 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 25 10:51:22.128: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 25 10:51:22.128: INFO: stdout: "iptables"
May 25 10:51:22.128: INFO: proxyMode: iptables
May 25 10:51:22.152: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 25 10:51:22.160: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-630
STEP: creating replication controller affinity-clusterip-timeout in namespace services-630
I0525 10:51:22.268543      19 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-630, replica count: 3
I0525 10:51:25.319289      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 10:51:25.335: INFO: Creating new exec pod
May 25 10:51:28.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-630 exec execpod-affinityskblq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 25 10:51:28.897: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 25 10:51:28.897: INFO: stdout: ""
May 25 10:51:28.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-630 exec execpod-affinityskblq -- /bin/sh -x -c nc -zv -t -w 2 10.68.11.128 80'
May 25 10:51:29.152: INFO: stderr: "+ nc -zv -t -w 2 10.68.11.128 80\nConnection to 10.68.11.128 80 port [tcp/http] succeeded!\n"
May 25 10:51:29.152: INFO: stdout: ""
May 25 10:51:29.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-630 exec execpod-affinityskblq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.11.128:80/ ; done'
May 25 10:51:29.486: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n"
May 25 10:51:29.486: INFO: stdout: "\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q\naffinity-clusterip-timeout-gd29q"
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Received response from host: affinity-clusterip-timeout-gd29q
May 25 10:51:29.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-630 exec execpod-affinityskblq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.68.11.128:80/'
May 25 10:51:29.692: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n"
May 25 10:51:29.692: INFO: stdout: "affinity-clusterip-timeout-gd29q"
May 25 10:51:49.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-630 exec execpod-affinityskblq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.68.11.128:80/'
May 25 10:51:49.931: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.68.11.128:80/\n"
May 25 10:51:49.931: INFO: stdout: "affinity-clusterip-timeout-dsjjj"
May 25 10:51:49.931: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-630, will wait for the garbage collector to delete the pods
May 25 10:51:50.027: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 10.990288ms
May 25 10:51:50.728: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 701.31611ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:02.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-630" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:42.623 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":82,"skipped":1484,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:02.433: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:02.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5214" for this suite.
STEP: Destroying namespace "nspatchtest-8c2650e0-a9a7-47d5-80b9-6753c7b9e5f6-3875" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":83,"skipped":1504,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:02.571: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9304 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9304;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9304 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9304;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9304.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9304.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9304.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9304.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9304.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9304.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9304.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9304.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9304.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 191.6.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.6.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.6.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.6.191_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9304 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9304;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9304 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9304;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9304.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9304.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9304.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9304.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9304.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9304.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9304.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9304.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9304.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9304.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 191.6.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.6.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.6.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.6.191_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 10:52:06.765: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.773: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.780: INFO: Unable to read wheezy_udp@dns-test-service.dns-9304 from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9304 from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.802: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.823: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.831: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.878: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.884: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.892: INFO: Unable to read jessie_udp@dns-test-service.dns-9304 from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-9304 from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.907: INFO: Unable to read jessie_udp@dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.924: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.931: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9304.svc from pod dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781: the server could not find the requested resource (get pods dns-test-3e29f1bc-637c-4814-923c-acd4d076a781)
May 25 10:52:06.975: INFO: Lookups using dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9304 wheezy_tcp@dns-test-service.dns-9304 wheezy_udp@dns-test-service.dns-9304.svc wheezy_tcp@dns-test-service.dns-9304.svc wheezy_udp@_http._tcp.dns-test-service.dns-9304.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9304.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9304 jessie_tcp@dns-test-service.dns-9304 jessie_udp@dns-test-service.dns-9304.svc jessie_tcp@dns-test-service.dns-9304.svc jessie_udp@_http._tcp.dns-test-service.dns-9304.svc jessie_tcp@_http._tcp.dns-test-service.dns-9304.svc]

May 25 10:52:12.197: INFO: DNS probes using dns-9304/dns-test-3e29f1bc-637c-4814-923c-acd4d076a781 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:12.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9304" for this suite.

• [SLOW TEST:9.812 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":84,"skipped":1518,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:12.390: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 25 10:52:18.524: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:18.524: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:18.657: INFO: Exec stderr: ""
May 25 10:52:18.657: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:18.657: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:18.769: INFO: Exec stderr: ""
May 25 10:52:18.769: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:18.769: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:18.870: INFO: Exec stderr: ""
May 25 10:52:18.870: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:18.870: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:18.961: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 25 10:52:18.961: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:18.961: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:19.081: INFO: Exec stderr: ""
May 25 10:52:19.081: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:19.081: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:19.190: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 25 10:52:19.190: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:19.190: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:19.327: INFO: Exec stderr: ""
May 25 10:52:19.327: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:19.327: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:19.422: INFO: Exec stderr: ""
May 25 10:52:19.422: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:19.422: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:19.558: INFO: Exec stderr: ""
May 25 10:52:19.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3578 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:52:19.558: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:52:19.657: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:19.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3578" for this suite.

• [SLOW TEST:7.291 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1527,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:19.681: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:52:19.782: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 25 10:52:19.807: INFO: Number of nodes with available pods: 0
May 25 10:52:19.807: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 25 10:52:20.074: INFO: Number of nodes with available pods: 0
May 25 10:52:20.074: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:21.114: INFO: Number of nodes with available pods: 0
May 25 10:52:21.114: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:22.084: INFO: Number of nodes with available pods: 1
May 25 10:52:22.084: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 25 10:52:22.347: INFO: Number of nodes with available pods: 0
May 25 10:52:22.347: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 25 10:52:22.362: INFO: Number of nodes with available pods: 0
May 25 10:52:22.362: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:23.380: INFO: Number of nodes with available pods: 0
May 25 10:52:23.380: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:24.374: INFO: Number of nodes with available pods: 0
May 25 10:52:24.374: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:25.373: INFO: Number of nodes with available pods: 0
May 25 10:52:25.373: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:26.373: INFO: Number of nodes with available pods: 0
May 25 10:52:26.374: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:27.376: INFO: Number of nodes with available pods: 0
May 25 10:52:27.376: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 10:52:28.378: INFO: Number of nodes with available pods: 1
May 25 10:52:28.378: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6927, will wait for the garbage collector to delete the pods
May 25 10:52:28.475: INFO: Deleting DaemonSet.extensions daemon-set took: 23.002108ms
May 25 10:52:28.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.283389ms
May 25 10:52:41.536: INFO: Number of nodes with available pods: 0
May 25 10:52:41.536: INFO: Number of running nodes: 0, number of available pods: 0
May 25 10:52:41.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"93736"},"items":null}

May 25 10:52:41.561: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"93736"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:41.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6927" for this suite.

• [SLOW TEST:22.248 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":86,"skipped":1538,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:41.931: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c35e7c54-6772-4fb5-9fe0-1be38ffb7339
STEP: Creating a pod to test consume secrets
May 25 10:52:42.020: INFO: Waiting up to 5m0s for pod "pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f" in namespace "secrets-4848" to be "Succeeded or Failed"
May 25 10:52:42.030: INFO: Pod "pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.564573ms
May 25 10:52:44.047: INFO: Pod "pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026781966s
STEP: Saw pod success
May 25 10:52:44.047: INFO: Pod "pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f" satisfied condition "Succeeded or Failed"
May 25 10:52:44.053: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f container secret-volume-test: <nil>
STEP: delete the pod
May 25 10:52:44.107: INFO: Waiting for pod pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f to disappear
May 25 10:52:44.138: INFO: Pod pod-secrets-f72af1b0-b2cc-45eb-9a27-0f24bad5733f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:44.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4848" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1546,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:44.158: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 25 10:52:44.240: INFO: Waiting up to 5m0s for pod "pod-0d66b5bb-9a67-4ace-a91c-425df9fed669" in namespace "emptydir-3216" to be "Succeeded or Failed"
May 25 10:52:44.248: INFO: Pod "pod-0d66b5bb-9a67-4ace-a91c-425df9fed669": Phase="Pending", Reason="", readiness=false. Elapsed: 8.107722ms
May 25 10:52:46.263: INFO: Pod "pod-0d66b5bb-9a67-4ace-a91c-425df9fed669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022490139s
STEP: Saw pod success
May 25 10:52:46.263: INFO: Pod "pod-0d66b5bb-9a67-4ace-a91c-425df9fed669" satisfied condition "Succeeded or Failed"
May 25 10:52:46.269: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-0d66b5bb-9a67-4ace-a91c-425df9fed669 container test-container: <nil>
STEP: delete the pod
May 25 10:52:46.307: INFO: Waiting for pod pod-0d66b5bb-9a67-4ace-a91c-425df9fed669 to disappear
May 25 10:52:46.316: INFO: Pod pod-0d66b5bb-9a67-4ace-a91c-425df9fed669 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:46.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3216" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1548,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:46.335: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 25 10:52:46.563: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 25 10:52:46.597: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:46.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9993" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":89,"skipped":1565,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:46.662: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 10:52:46.722: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f" in namespace "projected-277" to be "Succeeded or Failed"
May 25 10:52:46.731: INFO: Pod "downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.578777ms
May 25 10:52:48.755: INFO: Pod "downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032479987s
STEP: Saw pod success
May 25 10:52:48.755: INFO: Pod "downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f" satisfied condition "Succeeded or Failed"
May 25 10:52:48.790: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f container client-container: <nil>
STEP: delete the pod
May 25 10:52:48.824: INFO: Waiting for pod downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f to disappear
May 25 10:52:48.831: INFO: Pod downwardapi-volume-b1982178-9779-4bba-a2c8-feef488ce87f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:52:48.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-277" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:52:48.852: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7777
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 10:52:48.909: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 10:52:48.970: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 10:52:50.984: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:52:52.981: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:52:54.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:52:56.985: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:52:58.984: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:00.988: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:02.984: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:04.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:06.998: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 10:53:07.023: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 25 10:53:09.040: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 25 10:53:09.054: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 25 10:53:11.099: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 25 10:53:11.099: INFO: Breadth first check of 10.64.0.88 on host 10.156.0.14...
May 25 10:53:11.106: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.0.89:9080/dial?request=hostname&protocol=http&host=10.64.0.88&port=8080&tries=1'] Namespace:pod-network-test-7777 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:53:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:53:11.208: INFO: Waiting for responses: map[]
May 25 10:53:11.208: INFO: reached 10.64.0.88 after 0/1 tries
May 25 10:53:11.208: INFO: Breadth first check of 10.64.2.134 on host 10.156.0.16...
May 25 10:53:11.216: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.0.89:9080/dial?request=hostname&protocol=http&host=10.64.2.134&port=8080&tries=1'] Namespace:pod-network-test-7777 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:53:11.216: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:53:11.327: INFO: Waiting for responses: map[]
May 25 10:53:11.327: INFO: reached 10.64.2.134 after 0/1 tries
May 25 10:53:11.327: INFO: Breadth first check of 10.64.1.165 on host 10.156.0.15...
May 25 10:53:11.338: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.0.89:9080/dial?request=hostname&protocol=http&host=10.64.1.165&port=8080&tries=1'] Namespace:pod-network-test-7777 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:53:11.338: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:53:11.447: INFO: Waiting for responses: map[]
May 25 10:53:11.447: INFO: reached 10.64.1.165 after 0/1 tries
May 25 10:53:11.447: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:11.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7777" for this suite.

• [SLOW TEST:22.616 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":91,"skipped":1635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:11.473: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:11.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7716" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":92,"skipped":1681,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:11.645: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5972
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 10:53:11.698: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 10:53:11.751: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 10:53:13.767: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:15.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:17.762: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:19.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:21.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:23.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:25.768: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:27.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:29.762: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 10:53:31.763: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 10:53:31.774: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 25 10:53:31.786: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 25 10:53:35.845: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 25 10:53:35.845: INFO: Going to poll 10.64.0.90 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 25 10:53:35.851: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.0.90:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5972 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:53:35.851: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:53:35.970: INFO: Found all 1 expected endpoints: [netserver-0]
May 25 10:53:35.970: INFO: Going to poll 10.64.2.135 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 25 10:53:35.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.2.135:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5972 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:53:35.977: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:53:36.092: INFO: Found all 1 expected endpoints: [netserver-1]
May 25 10:53:36.092: INFO: Going to poll 10.64.1.166 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 25 10:53:36.099: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.1.166:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5972 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 10:53:36.099: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 10:53:36.198: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:36.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5972" for this suite.

• [SLOW TEST:24.569 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1695,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:36.216: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 10:53:36.542: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 10:53:38.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536816, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536816, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536816, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757536816, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 10:53:41.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:41.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6870" for this suite.
STEP: Destroying namespace "webhook-6870-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.764 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":94,"skipped":1715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:41.980: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 10:53:44.134: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:44.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2420" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:44.213: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:53:44.326: INFO: Creating ReplicaSet my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e
May 25 10:53:44.367: INFO: Pod name my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e: Found 0 pods out of 1
May 25 10:53:49.396: INFO: Pod name my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e: Found 1 pods out of 1
May 25 10:53:49.396: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e" is running
May 25 10:53:49.408: INFO: Pod "my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e-wb2h2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:53:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:53:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:53:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:53:44 +0000 UTC Reason: Message:}])
May 25 10:53:49.409: INFO: Trying to dial the pod
May 25 10:53:54.432: INFO: Controller my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e: Got expected result from replica 1 [my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e-wb2h2]: "my-hostname-basic-25484eb6-5718-4515-b4c9-fc9a0074814e-wb2h2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:54.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2045" for this suite.

• [SLOW TEST:10.235 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":96,"skipped":1768,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:54.452: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:53:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8186" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:53:58.661: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-7d755503-6a8e-4e4e-a7be-bae83cbdff65
STEP: Creating a pod to test consume secrets
May 25 10:53:58.734: INFO: Waiting up to 5m0s for pod "pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9" in namespace "secrets-9069" to be "Succeeded or Failed"
May 25 10:53:58.741: INFO: Pod "pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.402753ms
May 25 10:54:00.755: INFO: Pod "pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020928133s
STEP: Saw pod success
May 25 10:54:00.755: INFO: Pod "pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9" satisfied condition "Succeeded or Failed"
May 25 10:54:00.761: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9 container secret-volume-test: <nil>
STEP: delete the pod
May 25 10:54:00.794: INFO: Waiting for pod pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9 to disappear
May 25 10:54:00.803: INFO: Pod pod-secrets-151ed8e1-6d13-49ba-aae2-ed1e37ed15f9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:54:00.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9069" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1829,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:54:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-0d5c997c-e913-4984-b024-989da2de5a28
STEP: Creating a pod to test consume configMaps
May 25 10:54:00.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109" in namespace "configmap-2248" to be "Succeeded or Failed"
May 25 10:54:00.906: INFO: Pod "pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109": Phase="Pending", Reason="", readiness=false. Elapsed: 10.495995ms
May 25 10:54:02.924: INFO: Pod "pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028237177s
May 25 10:54:04.961: INFO: Pod "pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065110505s
STEP: Saw pod success
May 25 10:54:04.961: INFO: Pod "pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109" satisfied condition "Succeeded or Failed"
May 25 10:54:04.967: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109 container agnhost-container: <nil>
STEP: delete the pod
May 25 10:54:05.124: INFO: Waiting for pod pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109 to disappear
May 25 10:54:05.151: INFO: Pod pod-configmaps-1cc6a0a6-59c9-4ea1-9330-a131d4221109 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:54:05.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2248" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:54:05.180: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 25 10:54:07.885: INFO: Successfully updated pod "pod-update-activedeadlineseconds-83f35275-4657-44aa-93d1-804e1de690b1"
May 25 10:54:07.885: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-83f35275-4657-44aa-93d1-804e1de690b1" in namespace "pods-1717" to be "terminated due to deadline exceeded"
May 25 10:54:07.892: INFO: Pod "pod-update-activedeadlineseconds-83f35275-4657-44aa-93d1-804e1de690b1": Phase="Running", Reason="", readiness=true. Elapsed: 7.150628ms
May 25 10:54:09.899: INFO: Pod "pod-update-activedeadlineseconds-83f35275-4657-44aa-93d1-804e1de690b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014512313s
May 25 10:54:11.913: INFO: Pod "pod-update-activedeadlineseconds-83f35275-4657-44aa-93d1-804e1de690b1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.028336165s
May 25 10:54:11.913: INFO: Pod "pod-update-activedeadlineseconds-83f35275-4657-44aa-93d1-804e1de690b1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:54:11.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1717" for this suite.

• [SLOW TEST:6.751 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1856,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:54:11.931: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
May 25 10:54:11.995: INFO: Waiting up to 5m0s for pod "var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61" in namespace "var-expansion-2959" to be "Succeeded or Failed"
May 25 10:54:12.003: INFO: Pod "var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61": Phase="Pending", Reason="", readiness=false. Elapsed: 8.128729ms
May 25 10:54:14.020: INFO: Pod "var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024820246s
May 25 10:54:16.033: INFO: Pod "var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038034538s
STEP: Saw pod success
May 25 10:54:16.033: INFO: Pod "var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61" satisfied condition "Succeeded or Failed"
May 25 10:54:16.039: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61 container dapi-container: <nil>
STEP: delete the pod
May 25 10:54:16.070: INFO: Waiting for pod var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61 to disappear
May 25 10:54:16.080: INFO: Pod var-expansion-b2016411-d0ec-4287-a89b-0d5ebce84b61 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:54:16.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2959" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1862,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:54:16.100: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 in namespace container-probe-2904
May 25 10:54:18.214: INFO: Started pod liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 in namespace container-probe-2904
STEP: checking the pod's current state and verifying that restartCount is present
May 25 10:54:18.220: INFO: Initial restart count of pod liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 is 0
May 25 10:54:34.370: INFO: Restart count of pod container-probe-2904/liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 is now 1 (16.149882331s elapsed)
May 25 10:54:54.787: INFO: Restart count of pod container-probe-2904/liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 is now 2 (36.566851932s elapsed)
May 25 10:55:14.952: INFO: Restart count of pod container-probe-2904/liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 is now 3 (56.731405928s elapsed)
May 25 10:55:35.107: INFO: Restart count of pod container-probe-2904/liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 is now 4 (1m16.886866531s elapsed)
May 25 10:56:43.674: INFO: Restart count of pod container-probe-2904/liveness-91c682db-326d-4ec0-9c16-68b944bc1f76 is now 5 (2m25.45351979s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:56:43.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2904" for this suite.

• [SLOW TEST:147.609 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1863,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:56:43.710: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:56:43.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2364" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":103,"skipped":1865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:56:43.857: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:57:12.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2534" for this suite.

• [SLOW TEST:28.167 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":104,"skipped":1933,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:57:12.024: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0525 10:57:22.241319      19 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0525 10:57:22.241469      19 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0525 10:57:22.241546      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 10:57:22.241: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 25 10:57:22.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhj57" in namespace "gc-2201"
May 25 10:57:22.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bzds" in namespace "gc-2201"
May 25 10:57:22.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwg8d" in namespace "gc-2201"
May 25 10:57:22.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-24c6g" in namespace "gc-2201"
May 25 10:57:22.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-djjkt" in namespace "gc-2201"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:57:22.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2201" for this suite.

• [SLOW TEST:10.366 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":105,"skipped":1942,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:57:22.392: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 10:57:23.062: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 10:57:26.205: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:57:26.213: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6345-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:57:27.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3751" for this suite.
STEP: Destroying namespace "webhook-3751-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.304 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":106,"skipped":1948,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:57:27.702: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 10:57:29.940: INFO: Deleting pod "var-expansion-8cb6d582-543d-43d7-99c2-e5ca70825096" in namespace "var-expansion-5165"
May 25 10:57:29.955: INFO: Wait up to 5m0s for pod "var-expansion-8cb6d582-543d-43d7-99c2-e5ca70825096" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:57:34.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5165" for this suite.

• [SLOW TEST:6.330 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":107,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:57:34.034: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 25 10:57:34.197: INFO: Waiting up to 5m0s for pod "pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f" in namespace "emptydir-4617" to be "Succeeded or Failed"
May 25 10:57:34.208: INFO: Pod "pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.502186ms
May 25 10:57:36.224: INFO: Pod "pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02654397s
STEP: Saw pod success
May 25 10:57:36.224: INFO: Pod "pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f" satisfied condition "Succeeded or Failed"
May 25 10:57:36.230: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f container test-container: <nil>
STEP: delete the pod
May 25 10:57:36.259: INFO: Waiting for pod pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f to disappear
May 25 10:57:36.272: INFO: Pod pod-514fce27-b2f7-4b25-b7b2-97fd8c6c781f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:57:36.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4617" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:57:36.295: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8918, will wait for the garbage collector to delete the pods
May 25 10:57:40.585: INFO: Deleting Job.batch foo took: 16.523021ms
May 25 10:57:40.686: INFO: Terminating Job.batch foo pods took: 100.278193ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:58:22.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8918" for this suite.

• [SLOW TEST:45.822 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":109,"skipped":2016,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:58:22.117: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-2daf2165-4b41-4a81-a4c2-db2b2dfc0e00
STEP: Creating a pod to test consume secrets
May 25 10:58:22.192: INFO: Waiting up to 5m0s for pod "pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4" in namespace "secrets-4374" to be "Succeeded or Failed"
May 25 10:58:22.198: INFO: Pod "pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.671453ms
May 25 10:58:24.214: INFO: Pod "pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021649973s
STEP: Saw pod success
May 25 10:58:24.214: INFO: Pod "pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4" satisfied condition "Succeeded or Failed"
May 25 10:58:24.220: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4 container secret-volume-test: <nil>
STEP: delete the pod
May 25 10:58:24.253: INFO: Waiting for pod pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4 to disappear
May 25 10:58:24.261: INFO: Pod pod-secrets-406d438e-bd8f-42ea-ab1e-bcc8c6ba05a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:58:24.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4374" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":2022,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:58:24.280: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 10:58:24.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4" in namespace "projected-3067" to be "Succeeded or Failed"
May 25 10:58:24.345: INFO: Pod "downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.315479ms
May 25 10:58:26.360: INFO: Pod "downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021435185s
STEP: Saw pod success
May 25 10:58:26.360: INFO: Pod "downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4" satisfied condition "Succeeded or Failed"
May 25 10:58:26.365: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4 container client-container: <nil>
STEP: delete the pod
May 25 10:58:26.396: INFO: Waiting for pod downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4 to disappear
May 25 10:58:26.404: INFO: Pod downwardapi-volume-65897d2f-5b5b-4952-821e-28d087026cc4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:58:26.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3067" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":2024,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:58:26.428: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:58:26.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5786" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":112,"skipped":2041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:58:26.564: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6070
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6070
STEP: creating replication controller externalsvc in namespace services-6070
I0525 10:58:26.759637      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6070, replica count: 2
I0525 10:58:29.810227      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 25 10:58:29.851: INFO: Creating new exec pod
May 25 10:58:31.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-6070 exec execpodnfbcj -- /bin/sh -x -c nslookup nodeport-service.services-6070.svc.cluster.local'
May 25 10:58:32.697: INFO: stderr: "+ nslookup nodeport-service.services-6070.svc.cluster.local\n"
May 25 10:58:32.697: INFO: stdout: "Server:\t\t10.68.0.111\nAddress:\t10.68.0.111#53\n\nnodeport-service.services-6070.svc.cluster.local\tcanonical name = externalsvc.services-6070.svc.cluster.local.\nName:\texternalsvc.services-6070.svc.cluster.local\nAddress: 10.68.12.40\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6070, will wait for the garbage collector to delete the pods
May 25 10:58:32.768: INFO: Deleting ReplicationController externalsvc took: 11.955762ms
May 25 10:58:33.468: INFO: Terminating ReplicationController externalsvc pods took: 700.254676ms
May 25 10:58:42.107: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:58:42.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6070" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.594 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":113,"skipped":2093,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:58:42.161: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7263.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7263.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7263.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7263.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7263.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7263.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 10:58:46.326: INFO: DNS probes using dns-7263/dns-test-df0dd488-6c7a-406b-8eff-0118b68dc8d1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:58:46.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7263" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":114,"skipped":2107,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:58:46.374: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 10:58:47.043: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 10:58:49.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537127, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537127, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537127, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537127, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 10:58:52.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
May 25 10:59:02.202: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9194" for this suite.
STEP: Destroying namespace "webhook-9194-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:28.289 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":115,"skipped":2108,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:14.661: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
May 25 10:59:14.736: INFO: Waiting up to 5m0s for pod "var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae" in namespace "var-expansion-733" to be "Succeeded or Failed"
May 25 10:59:14.745: INFO: Pod "var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.840791ms
May 25 10:59:16.761: INFO: Pod "var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024901853s
STEP: Saw pod success
May 25 10:59:16.761: INFO: Pod "var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae" satisfied condition "Succeeded or Failed"
May 25 10:59:16.766: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae container dapi-container: <nil>
STEP: delete the pod
May 25 10:59:16.796: INFO: Waiting for pod var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae to disappear
May 25 10:59:16.804: INFO: Pod var-expansion-d208b2e4-1d5b-4997-a1f8-e01ab06f80ae no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:16.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-733" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:16.822: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-72f00d9e-a43c-4b69-a8c7-d00811952415
STEP: Creating a pod to test consume configMaps
May 25 10:59:16.898: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e" in namespace "projected-8571" to be "Succeeded or Failed"
May 25 10:59:16.909: INFO: Pod "pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.395118ms
May 25 10:59:18.918: INFO: Pod "pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019988262s
STEP: Saw pod success
May 25 10:59:18.919: INFO: Pod "pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e" satisfied condition "Succeeded or Failed"
May 25 10:59:18.923: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e container agnhost-container: <nil>
STEP: delete the pod
May 25 10:59:18.951: INFO: Waiting for pod pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e to disappear
May 25 10:59:18.964: INFO: Pod pod-projected-configmaps-a78d24cf-8b2e-4de4-95ff-1ca4db03f26e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:18.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8571" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2161,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:18.982: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3500/configmap-test-2c40b45f-e3b2-445a-a103-37ae820ec7bb
STEP: Creating a pod to test consume configMaps
May 25 10:59:19.057: INFO: Waiting up to 5m0s for pod "pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb" in namespace "configmap-3500" to be "Succeeded or Failed"
May 25 10:59:19.083: INFO: Pod "pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082848ms
May 25 10:59:21.098: INFO: Pod "pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02380253s
STEP: Saw pod success
May 25 10:59:21.099: INFO: Pod "pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb" satisfied condition "Succeeded or Failed"
May 25 10:59:21.104: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb container env-test: <nil>
STEP: delete the pod
May 25 10:59:21.134: INFO: Waiting for pod pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb to disappear
May 25 10:59:21.144: INFO: Pod pod-configmaps-91a18b38-13d9-4be4-82f2-fc0fdc924dcb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:21.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3500" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:21.168: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-2b345cb1-1c77-4a9f-a038-05e85c977737
STEP: Creating a pod to test consume secrets
May 25 10:59:21.255: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d" in namespace "projected-6864" to be "Succeeded or Failed"
May 25 10:59:21.262: INFO: Pod "pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.872568ms
May 25 10:59:23.277: INFO: Pod "pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021620158s
STEP: Saw pod success
May 25 10:59:23.277: INFO: Pod "pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d" satisfied condition "Succeeded or Failed"
May 25 10:59:23.282: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 10:59:23.312: INFO: Waiting for pod pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d to disappear
May 25 10:59:23.326: INFO: Pod pod-projected-secrets-5cbd7f95-de3b-4516-b0bc-320aeb11b61d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:23.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6864" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2205,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:23.348: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-54a5afe3-02a3-48db-9910-5741de915929
STEP: Creating a pod to test consume configMaps
May 25 10:59:23.448: INFO: Waiting up to 5m0s for pod "pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39" in namespace "configmap-3096" to be "Succeeded or Failed"
May 25 10:59:23.459: INFO: Pod "pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.362573ms
May 25 10:59:25.466: INFO: Pod "pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017550711s
STEP: Saw pod success
May 25 10:59:25.466: INFO: Pod "pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39" satisfied condition "Succeeded or Failed"
May 25 10:59:25.477: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39 container configmap-volume-test: <nil>
STEP: delete the pod
May 25 10:59:25.509: INFO: Waiting for pod pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39 to disappear
May 25 10:59:25.524: INFO: Pod pod-configmaps-79b95bcf-9719-413e-b23e-5c301b47ea39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:25.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3096" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2225,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:25.544: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 25 10:59:25.621: INFO: Waiting up to 5m0s for pod "pod-198d579c-83b0-4ecc-8260-547fd33afe48" in namespace "emptydir-5265" to be "Succeeded or Failed"
May 25 10:59:25.630: INFO: Pod "pod-198d579c-83b0-4ecc-8260-547fd33afe48": Phase="Pending", Reason="", readiness=false. Elapsed: 9.08432ms
May 25 10:59:27.646: INFO: Pod "pod-198d579c-83b0-4ecc-8260-547fd33afe48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024934984s
STEP: Saw pod success
May 25 10:59:27.646: INFO: Pod "pod-198d579c-83b0-4ecc-8260-547fd33afe48" satisfied condition "Succeeded or Failed"
May 25 10:59:27.652: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-198d579c-83b0-4ecc-8260-547fd33afe48 container test-container: <nil>
STEP: delete the pod
May 25 10:59:27.683: INFO: Waiting for pod pod-198d579c-83b0-4ecc-8260-547fd33afe48 to disappear
May 25 10:59:27.693: INFO: Pod pod-198d579c-83b0-4ecc-8260-547fd33afe48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:27.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5265" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":2236,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:27.713: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501
May 25 10:59:27.787: INFO: Pod name my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501: Found 0 pods out of 1
May 25 10:59:32.811: INFO: Pod name my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501: Found 1 pods out of 1
May 25 10:59:32.811: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501" are running
May 25 10:59:32.830: INFO: Pod "my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501-x5xd7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:59:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:59:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:59:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 10:59:27 +0000 UTC Reason: Message:}])
May 25 10:59:32.831: INFO: Trying to dial the pod
May 25 10:59:37.871: INFO: Controller my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501: Got expected result from replica 1 [my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501-x5xd7]: "my-hostname-basic-9484d7ae-b698-49cf-917b-1ff4333ff501-x5xd7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:37.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7443" for this suite.

• [SLOW TEST:10.178 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":122,"skipped":2240,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 25 10:59:37.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8525 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 25 10:59:38.069: INFO: stderr: ""
May 25 10:59:38.069: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
May 25 10:59:38.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8525 delete pods e2e-test-httpd-pod'
May 25 10:59:51.993: INFO: stderr: ""
May 25 10:59:51.993: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:51.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8525" for this suite.

• [SLOW TEST:14.136 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":123,"skipped":2252,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-433ea399-3297-4e80-bdc9-d5755c7335b0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-433ea399-3297-4e80-bdc9-d5755c7335b0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 10:59:58.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9477" for this suite.

• [SLOW TEST:6.246 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2256,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 10:59:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:00:58.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4791" for this suite.

• [SLOW TEST:60.130 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:00:58.410: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0525 11:01:00.090482      19 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0525 11:01:00.090516      19 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0525 11:01:00.090523      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 11:01:00.090: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:00.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2105" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":126,"skipped":2331,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:00.119: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:01:00.175: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 25 11:01:03.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-742 --namespace=crd-publish-openapi-742 create -f -'
May 25 11:01:04.625: INFO: stderr: ""
May 25 11:01:04.625: INFO: stdout: "e2e-test-crd-publish-openapi-4562-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 25 11:01:04.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-742 --namespace=crd-publish-openapi-742 delete e2e-test-crd-publish-openapi-4562-crds test-cr'
May 25 11:01:04.742: INFO: stderr: ""
May 25 11:01:04.742: INFO: stdout: "e2e-test-crd-publish-openapi-4562-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 25 11:01:04.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-742 --namespace=crd-publish-openapi-742 apply -f -'
May 25 11:01:05.041: INFO: stderr: ""
May 25 11:01:05.041: INFO: stdout: "e2e-test-crd-publish-openapi-4562-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 25 11:01:05.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-742 --namespace=crd-publish-openapi-742 delete e2e-test-crd-publish-openapi-4562-crds test-cr'
May 25 11:01:05.164: INFO: stderr: ""
May 25 11:01:05.164: INFO: stdout: "e2e-test-crd-publish-openapi-4562-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 25 11:01:05.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-742 explain e2e-test-crd-publish-openapi-4562-crds'
May 25 11:01:05.466: INFO: stderr: ""
May 25 11:01:05.466: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4562-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:09.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-742" for this suite.

• [SLOW TEST:9.010 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":127,"skipped":2334,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:09.129: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 25 11:01:09.247: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4169  09cc5311-7879-4890-bc7c-1b4c6e8a9e57 95550 0 2021-05-25 11:01:09 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-25 11:01:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cln6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cln6k,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cln6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 11:01:09.254: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 25 11:01:11.267: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 25 11:01:11.267: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4169 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:01:11.267: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Verifying customized DNS server is configured on pod...
May 25 11:01:11.385: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4169 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:01:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:01:11.505: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:11.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4169" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":128,"skipped":2337,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:11.555: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 25 11:01:11.617: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 11:01:11.637: INFO: Waiting for terminating namespaces to be deleted...
May 25 11:01:11.645: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-p6wx before test
May 25 11:01:11.653: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:11.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:11.654: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:01:11.654: INFO: sonobuoy-e2e-job-7c943d3357464792 from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:11.654: INFO: 	Container e2e ready: true, restart count 0
May 25 11:01:11.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:11.654: INFO: test-webserver-0a545e31-a456-436b-9aec-0879e099f4b0 from container-probe-4791 started at 2021-05-25 10:59:58 +0000 UTC (1 container statuses recorded)
May 25 11:01:11.654: INFO: 	Container test-webserver ready: false, restart count 0
May 25 11:01:11.654: INFO: sonobuoy from sonobuoy started at 2021-05-25 10:27:41 +0000 UTC (1 container statuses recorded)
May 25 11:01:11.654: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 11:01:11.654: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-wpkp before test
May 25 11:01:11.664: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-8lgrl from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:11.664: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:11.664: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:01:11.664: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-vl79 before test
May 25 11:01:11.675: INFO: coredns-854c77959c-q5878 from kube-system started at 2021-05-19 12:01:03 +0000 UTC (1 container statuses recorded)
May 25 11:01:11.675: INFO: 	Container coredns ready: true, restart count 0
May 25 11:01:11.675: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-czfnh from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:11.675: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:11.675: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16824a44004cea85], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2245" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":129,"skipped":2351,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:12.770: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8835.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8835.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8835.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8835.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8835.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8835.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 11:01:17.112: INFO: DNS probes using dns-8835/dns-test-fbc03c7d-dd35-40a9-85ae-3581793a97c5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:17.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8835" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":130,"skipped":2362,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:17.274: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:01:17.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a" in namespace "projected-7114" to be "Succeeded or Failed"
May 25 11:01:17.348: INFO: Pod "downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623038ms
May 25 11:01:19.363: INFO: Pod "downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020928907s
STEP: Saw pod success
May 25 11:01:19.363: INFO: Pod "downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a" satisfied condition "Succeeded or Failed"
May 25 11:01:19.369: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a container client-container: <nil>
STEP: delete the pod
May 25 11:01:19.409: INFO: Waiting for pod downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a to disappear
May 25 11:01:19.422: INFO: Pod downwardapi-volume-d6bf75f5-bece-4330-bb07-370b0a428a2a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:19.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7114" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2372,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:19.440: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:21.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9123" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2372,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:21.565: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 25 11:01:21.619: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 11:01:21.635: INFO: Waiting for terminating namespaces to be deleted...
May 25 11:01:21.642: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-p6wx before test
May 25 11:01:21.654: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:21.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:21.654: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:01:21.654: INFO: sonobuoy-e2e-job-7c943d3357464792 from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:21.654: INFO: 	Container e2e ready: true, restart count 0
May 25 11:01:21.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:21.654: INFO: sonobuoy from sonobuoy started at 2021-05-25 10:27:41 +0000 UTC (1 container statuses recorded)
May 25 11:01:21.654: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 11:01:21.654: INFO: busybox-readonly-fsa687dbfd-a1ae-464d-bf97-bd3508828eed from kubelet-test-9123 started at 2021-05-25 11:01:19 +0000 UTC (1 container statuses recorded)
May 25 11:01:21.654: INFO: 	Container busybox-readonly-fsa687dbfd-a1ae-464d-bf97-bd3508828eed ready: true, restart count 0
May 25 11:01:21.654: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-wpkp before test
May 25 11:01:21.663: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-8lgrl from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:21.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:21.663: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:01:21.663: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-vl79 before test
May 25 11:01:21.670: INFO: coredns-854c77959c-q5878 from kube-system started at 2021-05-19 12:01:03 +0000 UTC (1 container statuses recorded)
May 25 11:01:21.670: INFO: 	Container coredns ready: true, restart count 0
May 25 11:01:21.670: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-czfnh from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:01:21.670: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:01:21.670: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-be48e3c1-711b-4bc1-bd75-11a5f13d16e7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-be48e3c1-711b-4bc1-bd75-11a5f13d16e7 off the node gke-cluster-1-default-pool-8f0bb8bb-p6wx
STEP: verifying the node doesn't have the label kubernetes.io/e2e-be48e3c1-711b-4bc1-bd75-11a5f13d16e7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:26.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-37" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":133,"skipped":2375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-hvkj
STEP: Creating a pod to test atomic-volume-subpath
May 25 11:01:26.423: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hvkj" in namespace "subpath-3666" to be "Succeeded or Failed"
May 25 11:01:26.434: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.77025ms
May 25 11:01:28.502: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 2.07913405s
May 25 11:01:30.519: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 4.095306925s
May 25 11:01:32.535: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 6.111284176s
May 25 11:01:34.550: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 8.126837915s
May 25 11:01:36.569: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 10.145957903s
May 25 11:01:38.583: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 12.159466664s
May 25 11:01:40.595: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 14.172059611s
May 25 11:01:42.613: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 16.189491943s
May 25 11:01:44.626: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 18.20290396s
May 25 11:01:46.649: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Running", Reason="", readiness=true. Elapsed: 20.225252041s
May 25 11:01:48.674: INFO: Pod "pod-subpath-test-configmap-hvkj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.250827526s
STEP: Saw pod success
May 25 11:01:48.674: INFO: Pod "pod-subpath-test-configmap-hvkj" satisfied condition "Succeeded or Failed"
May 25 11:01:48.680: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-subpath-test-configmap-hvkj container test-container-subpath-configmap-hvkj: <nil>
STEP: delete the pod
May 25 11:01:48.752: INFO: Waiting for pod pod-subpath-test-configmap-hvkj to disappear
May 25 11:01:48.789: INFO: Pod pod-subpath-test-configmap-hvkj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hvkj
May 25 11:01:48.790: INFO: Deleting pod "pod-subpath-test-configmap-hvkj" in namespace "subpath-3666"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:48.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3666" for this suite.

• [SLOW TEST:22.483 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":134,"skipped":2398,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:48.826: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
May 25 11:01:49.469: INFO: created pod pod-service-account-defaultsa
May 25 11:01:49.469: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 25 11:01:49.480: INFO: created pod pod-service-account-mountsa
May 25 11:01:49.480: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 25 11:01:49.489: INFO: created pod pod-service-account-nomountsa
May 25 11:01:49.489: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 25 11:01:49.501: INFO: created pod pod-service-account-defaultsa-mountspec
May 25 11:01:49.501: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 25 11:01:49.534: INFO: created pod pod-service-account-mountsa-mountspec
May 25 11:01:49.534: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 25 11:01:49.554: INFO: created pod pod-service-account-nomountsa-mountspec
May 25 11:01:49.554: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 25 11:01:49.564: INFO: created pod pod-service-account-defaultsa-nomountspec
May 25 11:01:49.564: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 25 11:01:49.585: INFO: created pod pod-service-account-mountsa-nomountspec
May 25 11:01:49.585: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 25 11:01:49.595: INFO: created pod pod-service-account-nomountsa-nomountspec
May 25 11:01:49.595: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:01:49.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7211" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":135,"skipped":2404,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:01:49.633: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6934
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6934
STEP: creating replication controller externalsvc in namespace services-6934
I0525 11:01:50.253618      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6934, replica count: 2
I0525 11:01:53.304260      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 25 11:01:53.353: INFO: Creating new exec pod
May 25 11:01:55.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-6934 exec execpodj8ncd -- /bin/sh -x -c nslookup clusterip-service.services-6934.svc.cluster.local'
May 25 11:01:55.815: INFO: stderr: "+ nslookup clusterip-service.services-6934.svc.cluster.local\n"
May 25 11:01:55.815: INFO: stdout: "Server:\t\t10.68.0.111\nAddress:\t10.68.0.111#53\n\nclusterip-service.services-6934.svc.cluster.local\tcanonical name = externalsvc.services-6934.svc.cluster.local.\nName:\texternalsvc.services-6934.svc.cluster.local\nAddress: 10.68.14.138\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6934, will wait for the garbage collector to delete the pods
May 25 11:01:55.910: INFO: Deleting ReplicationController externalsvc took: 19.098695ms
May 25 11:01:56.610: INFO: Terminating ReplicationController externalsvc pods took: 700.247458ms
May 25 11:02:12.199: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:02:12.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6934" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.608 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":136,"skipped":2423,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:02:12.244: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-abe6c7d0-123c-4b5a-a200-230d9b14cbc7 in namespace container-probe-8811
May 25 11:02:14.333: INFO: Started pod liveness-abe6c7d0-123c-4b5a-a200-230d9b14cbc7 in namespace container-probe-8811
STEP: checking the pod's current state and verifying that restartCount is present
May 25 11:02:14.340: INFO: Initial restart count of pod liveness-abe6c7d0-123c-4b5a-a200-230d9b14cbc7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:06:16.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8811" for this suite.

• [SLOW TEST:244.154 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2427,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:06:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:06:16.460: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 25 11:06:20.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 create -f -'
May 25 11:06:20.799: INFO: stderr: ""
May 25 11:06:20.799: INFO: stdout: "e2e-test-crd-publish-openapi-9635-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 25 11:06:20.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 delete e2e-test-crd-publish-openapi-9635-crds test-foo'
May 25 11:06:20.911: INFO: stderr: ""
May 25 11:06:20.911: INFO: stdout: "e2e-test-crd-publish-openapi-9635-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 25 11:06:20.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 apply -f -'
May 25 11:06:21.234: INFO: stderr: ""
May 25 11:06:21.234: INFO: stdout: "e2e-test-crd-publish-openapi-9635-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 25 11:06:21.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 delete e2e-test-crd-publish-openapi-9635-crds test-foo'
May 25 11:06:21.350: INFO: stderr: ""
May 25 11:06:21.350: INFO: stdout: "e2e-test-crd-publish-openapi-9635-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 25 11:06:21.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 create -f -'
May 25 11:06:21.631: INFO: rc: 1
May 25 11:06:21.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 apply -f -'
May 25 11:06:21.889: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 25 11:06:21.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 create -f -'
May 25 11:06:22.171: INFO: rc: 1
May 25 11:06:22.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 --namespace=crd-publish-openapi-384 apply -f -'
May 25 11:06:22.406: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 25 11:06:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 explain e2e-test-crd-publish-openapi-9635-crds'
May 25 11:06:22.672: INFO: stderr: ""
May 25 11:06:22.672: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 25 11:06:22.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 explain e2e-test-crd-publish-openapi-9635-crds.metadata'
May 25 11:06:22.947: INFO: stderr: ""
May 25 11:06:22.947: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 25 11:06:22.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 explain e2e-test-crd-publish-openapi-9635-crds.spec'
May 25 11:06:23.199: INFO: stderr: ""
May 25 11:06:23.199: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 25 11:06:23.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 explain e2e-test-crd-publish-openapi-9635-crds.spec.bars'
May 25 11:06:23.475: INFO: stderr: ""
May 25 11:06:23.475: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 25 11:06:23.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-384 explain e2e-test-crd-publish-openapi-9635-crds.spec.bars2'
May 25 11:06:23.749: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:06:26.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-384" for this suite.

• [SLOW TEST:9.645 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":138,"skipped":2440,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:06:26.045: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 11:06:28.164: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:06:28.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8045" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2446,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:06:28.222: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 25 11:06:28.305: INFO: Waiting up to 5m0s for pod "pod-e98a7980-d715-460f-b7b5-62ef66e7015d" in namespace "emptydir-6566" to be "Succeeded or Failed"
May 25 11:06:28.312: INFO: Pod "pod-e98a7980-d715-460f-b7b5-62ef66e7015d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.316096ms
May 25 11:06:30.328: INFO: Pod "pod-e98a7980-d715-460f-b7b5-62ef66e7015d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02260733s
May 25 11:06:32.341: INFO: Pod "pod-e98a7980-d715-460f-b7b5-62ef66e7015d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035521504s
STEP: Saw pod success
May 25 11:06:32.341: INFO: Pod "pod-e98a7980-d715-460f-b7b5-62ef66e7015d" satisfied condition "Succeeded or Failed"
May 25 11:06:32.347: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-e98a7980-d715-460f-b7b5-62ef66e7015d container test-container: <nil>
STEP: delete the pod
May 25 11:06:32.401: INFO: Waiting for pod pod-e98a7980-d715-460f-b7b5-62ef66e7015d to disappear
May 25 11:06:32.409: INFO: Pod pod-e98a7980-d715-460f-b7b5-62ef66e7015d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:06:32.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6566" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2456,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:06:32.432: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
May 25 11:06:32.495: INFO: created test-pod-1
May 25 11:06:32.504: INFO: created test-pod-2
May 25 11:06:32.511: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:06:32.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3573" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":141,"skipped":2471,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:06:32.640: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 25 11:06:32.734: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934  183a5b79-9386-40c4-bd32-1a2fa99cde64 96099 0 2021-05-25 11:06:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 11:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 11:06:32.734: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934  183a5b79-9386-40c4-bd32-1a2fa99cde64 96100 0 2021-05-25 11:06:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 11:06:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 11:06:32.735: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934  183a5b79-9386-40c4-bd32-1a2fa99cde64 96101 0 2021-05-25 11:06:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 11:06:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 25 11:06:42.792: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934  183a5b79-9386-40c4-bd32-1a2fa99cde64 96131 0 2021-05-25 11:06:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 11:06:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 11:06:42.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934  183a5b79-9386-40c4-bd32-1a2fa99cde64 96132 0 2021-05-25 11:06:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 11:06:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 11:06:42.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934  183a5b79-9386-40c4-bd32-1a2fa99cde64 96133 0 2021-05-25 11:06:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 11:06:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:06:42.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9934" for this suite.

• [SLOW TEST:10.168 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":142,"skipped":2474,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:06:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-pvxh
STEP: Creating a pod to test atomic-volume-subpath
May 25 11:06:42.891: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pvxh" in namespace "subpath-8306" to be "Succeeded or Failed"
May 25 11:06:42.898: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.211662ms
May 25 11:06:44.913: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 2.022499748s
May 25 11:06:46.932: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.041061371s
May 25 11:06:48.947: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 6.056382517s
May 25 11:06:50.964: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 8.072791773s
May 25 11:06:52.979: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 10.087708198s
May 25 11:06:54.991: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 12.099951862s
May 25 11:06:57.023: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 14.132015197s
May 25 11:06:59.040: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 16.149571314s
May 25 11:07:01.052: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.161079888s
May 25 11:07:03.069: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Running", Reason="", readiness=true. Elapsed: 20.177854506s
May 25 11:07:05.083: INFO: Pod "pod-subpath-test-secret-pvxh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.192446383s
STEP: Saw pod success
May 25 11:07:05.083: INFO: Pod "pod-subpath-test-secret-pvxh" satisfied condition "Succeeded or Failed"
May 25 11:07:05.089: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-subpath-test-secret-pvxh container test-container-subpath-secret-pvxh: <nil>
STEP: delete the pod
May 25 11:07:05.123: INFO: Waiting for pod pod-subpath-test-secret-pvxh to disappear
May 25 11:07:05.130: INFO: Pod pod-subpath-test-secret-pvxh no longer exists
STEP: Deleting pod pod-subpath-test-secret-pvxh
May 25 11:07:05.131: INFO: Deleting pod "pod-subpath-test-secret-pvxh" in namespace "subpath-8306"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:07:05.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8306" for this suite.

• [SLOW TEST:22.350 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":143,"skipped":2488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:07:05.163: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:07:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4905" for this suite.
STEP: Destroying namespace "nsdeletetest-2490" for this suite.
May 25 11:07:34.431: INFO: Namespace nsdeletetest-2490 was already deleted
STEP: Destroying namespace "nsdeletetest-2287" for this suite.

• [SLOW TEST:29.276 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":144,"skipped":2522,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:07:34.440: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0525 11:07:44.576075      19 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0525 11:07:44.576109      19 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0525 11:07:44.576121      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 11:07:44.576: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:07:44.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7783" for this suite.

• [SLOW TEST:10.155 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":145,"skipped":2523,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:07:44.596: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3670
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 11:07:44.649: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 11:07:44.697: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 11:07:46.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:07:48.709: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:07:50.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:07:52.705: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:07:54.716: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:07:56.715: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:07:58.706: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:08:00.715: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:08:02.720: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 11:08:02.734: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 25 11:08:02.746: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 25 11:08:04.816: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 25 11:08:04.816: INFO: Going to poll 10.64.0.144 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 25 11:08:04.822: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.0.144 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3670 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:08:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:08:06.047: INFO: Found all 1 expected endpoints: [netserver-0]
May 25 11:08:06.048: INFO: Going to poll 10.64.2.147 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 25 11:08:06.058: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.2.147 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3670 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:08:06.059: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:08:07.171: INFO: Found all 1 expected endpoints: [netserver-1]
May 25 11:08:07.171: INFO: Going to poll 10.64.1.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 25 11:08:07.185: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.1.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3670 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:08:07.185: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:08:08.367: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:08:08.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3670" for this suite.

• [SLOW TEST:23.803 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2524,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:08:08.401: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
May 25 11:08:08.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 25 11:08:08.585: INFO: stderr: ""
May 25 11:08:08.585: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
May 25 11:08:08.585: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 25 11:08:08.585: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5740" to be "running and ready, or succeeded"
May 25 11:08:08.591: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.384575ms
May 25 11:08:10.600: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014707251s
May 25 11:08:10.600: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 25 11:08:10.600: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 25 11:08:10.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 logs logs-generator logs-generator'
May 25 11:08:10.739: INFO: stderr: ""
May 25 11:08:10.739: INFO: stdout: "I0525 11:08:09.510598       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/7dmg 494\nI0525 11:08:09.710277       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/kw2 514\nI0525 11:08:09.910250       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/x9b4 517\nI0525 11:08:10.110377       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/qsz5 253\nI0525 11:08:10.310310       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/kvs6 410\nI0525 11:08:10.510268       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fhz 250\nI0525 11:08:10.710289       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/k7zv 501\n"
STEP: limiting log lines
May 25 11:08:10.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 logs logs-generator logs-generator --tail=1'
May 25 11:08:10.869: INFO: stderr: ""
May 25 11:08:10.869: INFO: stdout: "I0525 11:08:10.710289       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/k7zv 501\n"
May 25 11:08:10.869: INFO: got output "I0525 11:08:10.710289       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/k7zv 501\n"
STEP: limiting log bytes
May 25 11:08:10.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 logs logs-generator logs-generator --limit-bytes=1'
May 25 11:08:10.987: INFO: stderr: ""
May 25 11:08:10.987: INFO: stdout: "I"
May 25 11:08:10.987: INFO: got output "I"
STEP: exposing timestamps
May 25 11:08:10.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 logs logs-generator logs-generator --tail=1 --timestamps'
May 25 11:08:11.100: INFO: stderr: ""
May 25 11:08:11.100: INFO: stdout: "2021-05-25T11:08:10.910562109Z I0525 11:08:10.910303       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/lbj7 497\n"
May 25 11:08:11.100: INFO: got output "2021-05-25T11:08:10.910562109Z I0525 11:08:10.910303       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/lbj7 497\n"
STEP: restricting to a time range
May 25 11:08:13.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 logs logs-generator logs-generator --since=1s'
May 25 11:08:13.753: INFO: stderr: ""
May 25 11:08:13.753: INFO: stdout: "I0525 11:08:12.910291       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/b8ff 584\nI0525 11:08:13.110284       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/7h87 572\nI0525 11:08:13.310483       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/vv9 551\nI0525 11:08:13.510310       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/slrx 478\nI0525 11:08:13.711106       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/khdr 296\n"
May 25 11:08:13.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 logs logs-generator logs-generator --since=24h'
May 25 11:08:14.007: INFO: stderr: ""
May 25 11:08:14.007: INFO: stdout: "I0525 11:08:09.510598       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/7dmg 494\nI0525 11:08:09.710277       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/kw2 514\nI0525 11:08:09.910250       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/x9b4 517\nI0525 11:08:10.110377       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/qsz5 253\nI0525 11:08:10.310310       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/kvs6 410\nI0525 11:08:10.510268       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fhz 250\nI0525 11:08:10.710289       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/k7zv 501\nI0525 11:08:10.910303       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/lbj7 497\nI0525 11:08:11.110314       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/mtk2 326\nI0525 11:08:11.310420       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/zkd2 497\nI0525 11:08:11.510308       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/4g6r 571\nI0525 11:08:11.710333       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/dnh 277\nI0525 11:08:11.910463       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/gc9 275\nI0525 11:08:12.110311       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/c87m 362\nI0525 11:08:12.310295       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/mt8 530\nI0525 11:08:12.510352       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/x2w 542\nI0525 11:08:12.710511       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/pls 391\nI0525 11:08:12.910291       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/b8ff 584\nI0525 11:08:13.110284       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/7h87 572\nI0525 11:08:13.310483       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/vv9 551\nI0525 11:08:13.510310       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/slrx 478\nI0525 11:08:13.711106       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/khdr 296\nI0525 11:08:13.910286       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/mx99 521\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
May 25 11:08:14.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-5740 delete pod logs-generator'
May 25 11:08:22.313: INFO: stderr: ""
May 25 11:08:22.313: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:08:22.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5740" for this suite.

• [SLOW TEST:13.949 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":147,"skipped":2535,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:08:22.350: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 25 11:08:22.431: INFO: Waiting up to 5m0s for pod "pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f" in namespace "emptydir-8088" to be "Succeeded or Failed"
May 25 11:08:22.439: INFO: Pod "pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039099ms
May 25 11:08:24.449: INFO: Pod "pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017818905s
STEP: Saw pod success
May 25 11:08:24.449: INFO: Pod "pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f" satisfied condition "Succeeded or Failed"
May 25 11:08:24.456: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f container test-container: <nil>
STEP: delete the pod
May 25 11:08:24.486: INFO: Waiting for pod pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f to disappear
May 25 11:08:24.498: INFO: Pod pod-f0f2cdd7-4867-4dd5-91d1-05a1ae5fdd9f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:08:24.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8088" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:08:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 25 11:08:28.686: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 25 11:08:28.693: INFO: Pod pod-with-prestop-http-hook still exists
May 25 11:08:30.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 25 11:08:30.722: INFO: Pod pod-with-prestop-http-hook still exists
May 25 11:08:32.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 25 11:08:32.704: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:08:32.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3754" for this suite.

• [SLOW TEST:8.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":149,"skipped":2628,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:08:32.740: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:08:33.371: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 11:08:35.404: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537713, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537713, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537713, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537713, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:08:38.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:08:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5614" for this suite.
STEP: Destroying namespace "webhook-5614-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.909 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":150,"skipped":2639,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:08:38.649: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:08:39.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:08:42.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
May 25 11:08:52.313: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
May 25 11:08:52.478: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:08:52.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6855" for this suite.
STEP: Destroying namespace "webhook-6855-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.002 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":151,"skipped":2643,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:08:52.652: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 25 11:08:52.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8906 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 25 11:08:52.830: INFO: stderr: ""
May 25 11:08:52.830: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 25 11:08:52.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8906 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 25 11:08:53.381: INFO: stderr: ""
May 25 11:08:53.381: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 25 11:08:53.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8906 delete pods e2e-test-httpd-pod'
May 25 11:09:02.015: INFO: stderr: ""
May 25 11:09:02.015: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:09:02.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8906" for this suite.

• [SLOW TEST:9.434 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":152,"skipped":2648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:09:02.085: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 25 11:09:02.200: INFO: PodSpec: initContainers in spec.initContainers
May 25 11:09:48.508: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9738b590-6a36-4a96-8787-6d94c659bce9", GenerateName:"", Namespace:"init-container-7481", SelfLink:"", UID:"1553b243-a80b-4278-8e31-370f10afee36", ResourceVersion:"96684", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757537742, loc:(*time.Location)(0x797fe80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"200934204"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00439ed20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00439ed40)}, v1.ManagedFieldsEntry{Manager:"main", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00439ed60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00439ed80)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4d9rr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003f29240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4d9rr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4d9rr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4d9rr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0059ff718), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gke-cluster-1-default-pool-8f0bb8bb-p6wx", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00381bb20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0059ff790)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0059ff7b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0059ff7b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0059ff7bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00285a470), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537742, loc:(*time.Location)(0x797fe80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537742, loc:(*time.Location)(0x797fe80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537742, loc:(*time.Location)(0x797fe80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757537742, loc:(*time.Location)(0x797fe80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.156.0.14", PodIP:"10.64.0.152", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.64.0.152"}}, StartTime:(*v1.Time)(0xc00439eda0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00381bc00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00381bc70)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://17ca61d522ea8f188fbd8a2b7a9f3794bc39ed5211578f05fbd48e1dbfff0522", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00439ede0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00439edc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0059ff83f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:09:48.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7481" for this suite.

• [SLOW TEST:46.459 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":153,"skipped":2677,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:09:48.545: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:09:59.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2986" for this suite.

• [SLOW TEST:11.246 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":154,"skipped":2685,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:09:59.794: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 25 11:09:59.897: INFO: Waiting up to 5m0s for pod "downward-api-343e3b27-7243-457a-abb6-9ac161e539db" in namespace "downward-api-3852" to be "Succeeded or Failed"
May 25 11:09:59.905: INFO: Pod "downward-api-343e3b27-7243-457a-abb6-9ac161e539db": Phase="Pending", Reason="", readiness=false. Elapsed: 7.183308ms
May 25 11:10:01.920: INFO: Pod "downward-api-343e3b27-7243-457a-abb6-9ac161e539db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022239823s
STEP: Saw pod success
May 25 11:10:01.920: INFO: Pod "downward-api-343e3b27-7243-457a-abb6-9ac161e539db" satisfied condition "Succeeded or Failed"
May 25 11:10:01.925: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downward-api-343e3b27-7243-457a-abb6-9ac161e539db container dapi-container: <nil>
STEP: delete the pod
May 25 11:10:01.957: INFO: Waiting for pod downward-api-343e3b27-7243-457a-abb6-9ac161e539db to disappear
May 25 11:10:01.963: INFO: Pod downward-api-343e3b27-7243-457a-abb6-9ac161e539db no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:01.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3852" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2702,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:01.981: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
May 25 11:10:02.038: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-4005 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:02.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4005" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":156,"skipped":2707,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:02.146: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:10:02.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 create -f -'
May 25 11:10:02.534: INFO: stderr: ""
May 25 11:10:02.534: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 25 11:10:02.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 create -f -'
May 25 11:10:02.878: INFO: stderr: ""
May 25 11:10:02.878: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 25 11:10:03.887: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:10:03.887: INFO: Found 1 / 1
May 25 11:10:03.887: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 25 11:10:03.893: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:10:03.893: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 25 11:10:03.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 describe pod agnhost-primary-kjv8v'
May 25 11:10:04.039: INFO: stderr: ""
May 25 11:10:04.039: INFO: stdout: "Name:         agnhost-primary-kjv8v\nNamespace:    kubectl-1239\nPriority:     0\nNode:         gke-cluster-1-default-pool-8f0bb8bb-p6wx/10.156.0.14\nStart Time:   Tue, 25 May 2021 11:10:02 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.64.0.154\nIPs:\n  IP:           10.64.0.154\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6fea6a6764cf4b592c1dc9fab013fdfedaa4b8393eda101549d343f1e2386269\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 25 May 2021 11:10:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5q745 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5q745:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5q745\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1239/agnhost-primary-kjv8v to gke-cluster-1-default-pool-8f0bb8bb-p6wx\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 25 11:10:04.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 describe rc agnhost-primary'
May 25 11:10:04.202: INFO: stderr: ""
May 25 11:10:04.202: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1239\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-kjv8v\n"
May 25 11:10:04.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 describe service agnhost-primary'
May 25 11:10:04.329: INFO: stderr: ""
May 25 11:10:04.329: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1239\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.68.11.242\nIPs:               10.68.11.242\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.64.0.154:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 25 11:10:04.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 describe node gke-cluster-1-default-pool-8f0bb8bb-p6wx'
May 25 11:10:04.545: INFO: stderr: ""
May 25 11:10:04.545: INFO: stdout: "Name:               gke-cluster-1-default-pool-8f0bb8bb-p6wx\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=e2-medium\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/gke-boot-disk=pd-standard\n                    cloud.google.com/gke-container-runtime=containerd\n                    cloud.google.com/gke-nodepool=default-pool\n                    cloud.google.com/gke-os-distribution=cos\n                    cloud.google.com/machine-family=e2\n                    failure-domain.beta.kubernetes.io/region=europe-west3\n                    failure-domain.beta.kubernetes.io/zone=europe-west3-a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gke-cluster-1-default-pool-8f0bb8bb-p6wx\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=e2-medium\n                    topology.gke.io/zone=europe-west3-a\n                    topology.kubernetes.io/region=europe-west3\n                    topology.kubernetes.io/zone=europe-west3-a\nAnnotations:        container.googleapis.com/instance_id: 1205258299931973203\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"pd.csi.storage.gke.io\":\"projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-1-default-pool-8f0bb8bb-p6wx\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.gke.io/last-applied-node-labels:\n                      cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cl...\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 19 May 2021 11:58:15 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:              Failed to get lease: leases.coordination.k8s.io \"gke-cluster-1-default-pool-8f0bb8bb-p6wx\" not found\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  CorruptDockerOverlay2         False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  KernelDeadlock                False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentUnregisterNetDevice   False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Tue, 25 May 2021 11:08:34 +0000   Wed, 19 May 2021 10:32:37 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  NetworkUnavailable            False   Wed, 19 May 2021 10:32:38 +0000   Wed, 19 May 2021 10:32:38 +0000   RouteCreated                    NodeController create implicit route\n  MemoryPressure                False   Tue, 25 May 2021 11:09:06 +0000   Wed, 19 May 2021 14:54:11 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 25 May 2021 11:09:06 +0000   Wed, 19 May 2021 10:32:25 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 25 May 2021 11:09:06 +0000   Wed, 19 May 2021 10:32:25 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 25 May 2021 11:09:06 +0000   Wed, 19 May 2021 10:32:47 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.156.0.14\n  ExternalIP:   34.107.45.63\n  InternalDNS:  gke-cluster-1-default-pool-8f0bb8bb-p6wx.europe-west3-a.c.fabian-test-311613.internal\n  Hostname:     gke-cluster-1-default-pool-8f0bb8bb-p6wx.europe-west3-a.c.fabian-test-311613.internal\nCapacity:\n  attachable-volumes-gce-pd:  15\n  cpu:                        2\n  ephemeral-storage:          98868448Ki\n  example.com/fakecpu:        1k\n  hugepages-1Gi:              0\n  hugepages-2Mi:              0\n  memory:                     4034068Ki\n  pods:                       110\nAllocatable:\n  attachable-volumes-gce-pd:  15\n  cpu:                        737m\n  ephemeral-storage:          47093746742\n  example.com/fakecpu:        1k\n  hugepages-1Gi:              0\n  hugepages-2Mi:              0\n  memory:                     2632212Ki\n  pods:                       110\nSystem Info:\n  Machine ID:                 882d7868af6dd7b2fb34752ff0a68135\n  System UUID:                882d7868-af6d-d7b2-fb34-752ff0a68135\n  Boot ID:                    345d8c6a-59e7-49f9-9f8b-3a7720bb11d2\n  Kernel Version:             5.4.104+\n  OS Image:                   Container-Optimized OS from Google\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.3\n  Kubelet Version:            v1.20.6-gke.1000\n  Kube-Proxy Version:         v1.20.6-gke.1000\nPodCIDR:                      10.64.0.0/24\nPodCIDRs:                     10.64.0.0/24\nProviderID:                   gce://fabian-test-311613/europe-west3-a/gke-cluster-1-default-pool-8f0bb8bb-p6wx\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  sonobuoy                    sonobuoy-e2e-job-7c943d3357464792                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  kubectl-1239                agnhost-primary-kjv8v                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests  Limits\n  --------                   --------  ------\n  cpu                        0 (0%)    0 (0%)\n  memory                     0 (0%)    0 (0%)\n  ephemeral-storage          0 (0%)    0 (0%)\n  hugepages-1Gi              0 (0%)    0 (0%)\n  hugepages-2Mi              0 (0%)    0 (0%)\n  attachable-volumes-gce-pd  0         0\n  example.com/fakecpu        0         0\nEvents:                      <none>\n"
May 25 11:10:04.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1239 describe namespace kubectl-1239'
May 25 11:10:04.699: INFO: stderr: ""
May 25 11:10:04.699: INFO: stdout: "Name:         kubectl-1239\nLabels:       e2e-framework=kubectl\n              e2e-run=43eb5237-1d2a-4277-b2ce-d003dbdc078f\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:04.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1239" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":157,"skipped":2718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 25 11:10:07.883: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:08.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5572" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":158,"skipped":2746,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-ea853bcd-584f-4357-baa8-c88c0d6ba219
STEP: Creating a pod to test consume configMaps
May 25 11:10:09.068: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609" in namespace "configmap-3964" to be "Succeeded or Failed"
May 25 11:10:09.075: INFO: Pod "pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609": Phase="Pending", Reason="", readiness=false. Elapsed: 7.501469ms
May 25 11:10:11.093: INFO: Pod "pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024943422s
STEP: Saw pod success
May 25 11:10:11.093: INFO: Pod "pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609" satisfied condition "Succeeded or Failed"
May 25 11:10:11.100: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:10:11.130: INFO: Waiting for pod pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609 to disappear
May 25 11:10:11.141: INFO: Pod pod-configmaps-8ca78a01-3a22-48b5-9100-656042653609 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:11.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3964" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2754,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 25 11:10:13.271: INFO: &Pod{ObjectMeta:{send-events-fd7f6d14-7721-4070-8944-0bde74e392f3  events-4130  0c452524-4c35-441d-9409-b39a0e793e2b 96860 0 2021-05-25 11:10:11 +0000 UTC <nil> <nil> map[name:foo time:219091179] map[] [] []  [{e2e.test Update v1 2021-05-25 11:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 11:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fb892,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fb892,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fb892,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.157,StartTime:2021-05-25 11:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 11:10:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://9d1968c477efcef7d84489dcd1e6f675bddc53905a392cbc28c591922e16ae77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 25 11:10:15.295: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 25 11:10:17.310: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:17.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4130" for this suite.

• [SLOW TEST:6.179 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":160,"skipped":2759,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:17.339: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 25 11:10:17.447: INFO: Waiting up to 5m0s for pod "pod-999d8b8b-a88c-448f-94b8-026140ebefd7" in namespace "emptydir-6164" to be "Succeeded or Failed"
May 25 11:10:17.454: INFO: Pod "pod-999d8b8b-a88c-448f-94b8-026140ebefd7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.619206ms
May 25 11:10:19.465: INFO: Pod "pod-999d8b8b-a88c-448f-94b8-026140ebefd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018395487s
STEP: Saw pod success
May 25 11:10:19.466: INFO: Pod "pod-999d8b8b-a88c-448f-94b8-026140ebefd7" satisfied condition "Succeeded or Failed"
May 25 11:10:19.471: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-999d8b8b-a88c-448f-94b8-026140ebefd7 container test-container: <nil>
STEP: delete the pod
May 25 11:10:19.512: INFO: Waiting for pod pod-999d8b8b-a88c-448f-94b8-026140ebefd7 to disappear
May 25 11:10:19.519: INFO: Pod pod-999d8b8b-a88c-448f-94b8-026140ebefd7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:19.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6164" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2777,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:19.544: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:10:19.593: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:20.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3690" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":162,"skipped":2791,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:22.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3560" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":163,"skipped":2807,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:22.578: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5765
May 25 11:10:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 25 11:10:25.294: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 25 11:10:25.294: INFO: stdout: "iptables"
May 25 11:10:25.294: INFO: proxyMode: iptables
May 25 11:10:25.352: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 25 11:10:25.380: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5765
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5765
I0525 11:10:25.550499      19 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5765, replica count: 3
I0525 11:10:28.600948      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:10:28.696: INFO: Creating new exec pod
May 25 11:10:31.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 25 11:10:31.979: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 25 11:10:31.979: INFO: stdout: ""
May 25 11:10:31.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c nc -zv -t -w 2 10.68.4.127 80'
May 25 11:10:32.189: INFO: stderr: "+ nc -zv -t -w 2 10.68.4.127 80\nConnection to 10.68.4.127 80 port [tcp/http] succeeded!\n"
May 25 11:10:32.189: INFO: stdout: ""
May 25 11:10:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.15 30680'
May 25 11:10:32.419: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.15 30680\nConnection to 10.156.0.15 30680 port [tcp/30680] succeeded!\n"
May 25 11:10:32.419: INFO: stdout: ""
May 25 11:10:32.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.14 30680'
May 25 11:10:32.637: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.14 30680\nConnection to 10.156.0.14 30680 port [tcp/30680] succeeded!\n"
May 25 11:10:32.637: INFO: stdout: ""
May 25 11:10:32.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c nc -zv -t -w 2 35.198.151.65 30680'
May 25 11:10:32.836: INFO: stderr: "+ nc -zv -t -w 2 35.198.151.65 30680\nConnection to 35.198.151.65 30680 port [tcp/30680] succeeded!\n"
May 25 11:10:32.836: INFO: stdout: ""
May 25 11:10:32.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c nc -zv -t -w 2 34.107.45.63 30680'
May 25 11:10:33.104: INFO: stderr: "+ nc -zv -t -w 2 34.107.45.63 30680\nConnection to 34.107.45.63 30680 port [tcp/30680] succeeded!\n"
May 25 11:10:33.104: INFO: stdout: ""
May 25 11:10:33.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.156.0.14:30680/ ; done'
May 25 11:10:33.513: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n"
May 25 11:10:33.513: INFO: stdout: "\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr\naffinity-nodeport-timeout-h57cr"
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Received response from host: affinity-nodeport-timeout-h57cr
May 25 11:10:33.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.156.0.14:30680/'
May 25 11:10:33.727: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n"
May 25 11:10:33.727: INFO: stdout: "affinity-nodeport-timeout-h57cr"
May 25 11:10:53.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5765 exec execpod-affinityvnl4n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.156.0.14:30680/'
May 25 11:10:54.046: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.156.0.14:30680/\n"
May 25 11:10:54.046: INFO: stdout: "affinity-nodeport-timeout-9rpxx"
May 25 11:10:54.046: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5765, will wait for the garbage collector to delete the pods
May 25 11:10:54.146: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 11.306859ms
May 25 11:10:54.847: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 701.093546ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:10:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5765" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:36.562 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":164,"skipped":2814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:10:59.146: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 25 11:10:59.228: INFO: Pod name pod-release: Found 0 pods out of 1
May 25 11:11:04.237: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:11:04.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8628" for this suite.

• [SLOW TEST:5.162 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":165,"skipped":2846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:11:04.310: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 25 11:11:04.407: INFO: Waiting up to 5m0s for pod "downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59" in namespace "downward-api-3008" to be "Succeeded or Failed"
May 25 11:11:04.423: INFO: Pod "downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59": Phase="Pending", Reason="", readiness=false. Elapsed: 15.292472ms
May 25 11:11:06.441: INFO: Pod "downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032958009s
May 25 11:11:08.458: INFO: Pod "downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050616806s
STEP: Saw pod success
May 25 11:11:08.458: INFO: Pod "downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59" satisfied condition "Succeeded or Failed"
May 25 11:11:08.465: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59 container dapi-container: <nil>
STEP: delete the pod
May 25 11:11:08.498: INFO: Waiting for pod downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59 to disappear
May 25 11:11:08.508: INFO: Pod downward-api-c8947feb-633d-4e42-9a6f-89db62b91e59 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:11:08.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3008" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":2876,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:11:08.529: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:11:14.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3547" for this suite.
STEP: Destroying namespace "nsdeletetest-7854" for this suite.
May 25 11:11:14.778: INFO: Namespace nsdeletetest-7854 was already deleted
STEP: Destroying namespace "nsdeletetest-5805" for this suite.

• [SLOW TEST:6.257 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":167,"skipped":2913,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:11:14.790: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:11:14.860: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7" in namespace "projected-7164" to be "Succeeded or Failed"
May 25 11:11:14.868: INFO: Pod "downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031097ms
May 25 11:11:16.884: INFO: Pod "downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024124431s
STEP: Saw pod success
May 25 11:11:16.884: INFO: Pod "downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7" satisfied condition "Succeeded or Failed"
May 25 11:11:16.889: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7 container client-container: <nil>
STEP: delete the pod
May 25 11:11:16.920: INFO: Waiting for pod downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7 to disappear
May 25 11:11:16.942: INFO: Pod downwardapi-volume-e8aafce1-4064-42b2-b2ec-ea2d790188a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:11:16.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7164" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":2932,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:11:16.970: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 25 11:11:17.102: INFO: Number of nodes with available pods: 0
May 25 11:11:17.102: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-p6wx is running more than one daemon pod
May 25 11:11:18.126: INFO: Number of nodes with available pods: 0
May 25 11:11:18.126: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-p6wx is running more than one daemon pod
May 25 11:11:19.130: INFO: Number of nodes with available pods: 2
May 25 11:11:19.130: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 11:11:20.120: INFO: Number of nodes with available pods: 3
May 25 11:11:20.120: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 25 11:11:20.167: INFO: Number of nodes with available pods: 2
May 25 11:11:20.167: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-p6wx is running more than one daemon pod
May 25 11:11:21.190: INFO: Number of nodes with available pods: 2
May 25 11:11:21.190: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-p6wx is running more than one daemon pod
May 25 11:11:22.185: INFO: Number of nodes with available pods: 2
May 25 11:11:22.185: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-p6wx is running more than one daemon pod
May 25 11:11:23.190: INFO: Number of nodes with available pods: 3
May 25 11:11:23.190: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7771, will wait for the garbage collector to delete the pods
May 25 11:11:23.271: INFO: Deleting DaemonSet.extensions daemon-set took: 13.198482ms
May 25 11:11:23.371: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.354488ms
May 25 11:11:32.398: INFO: Number of nodes with available pods: 0
May 25 11:11:32.398: INFO: Number of running nodes: 0, number of available pods: 0
May 25 11:11:32.404: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"97340"},"items":null}

May 25 11:11:32.413: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"97340"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:11:32.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7771" for this suite.

• [SLOW TEST:15.535 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":169,"skipped":2936,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:11:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-6ea66043-8ce1-4643-a9e8-46090cb8f6f8 in namespace container-probe-6055
May 25 11:11:34.690: INFO: Started pod busybox-6ea66043-8ce1-4643-a9e8-46090cb8f6f8 in namespace container-probe-6055
STEP: checking the pod's current state and verifying that restartCount is present
May 25 11:11:34.695: INFO: Initial restart count of pod busybox-6ea66043-8ce1-4643-a9e8-46090cb8f6f8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:15:34.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6055" for this suite.

• [SLOW TEST:242.268 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:15:34.778: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 25 11:15:34.888: INFO: Waiting up to 5m0s for pod "downward-api-dbef1149-dacc-412d-8d33-80309da0a36f" in namespace "downward-api-1970" to be "Succeeded or Failed"
May 25 11:15:34.893: INFO: Pod "downward-api-dbef1149-dacc-412d-8d33-80309da0a36f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.586054ms
May 25 11:15:36.908: INFO: Pod "downward-api-dbef1149-dacc-412d-8d33-80309da0a36f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020019456s
STEP: Saw pod success
May 25 11:15:36.908: INFO: Pod "downward-api-dbef1149-dacc-412d-8d33-80309da0a36f" satisfied condition "Succeeded or Failed"
May 25 11:15:36.914: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downward-api-dbef1149-dacc-412d-8d33-80309da0a36f container dapi-container: <nil>
STEP: delete the pod
May 25 11:15:36.963: INFO: Waiting for pod downward-api-dbef1149-dacc-412d-8d33-80309da0a36f to disappear
May 25 11:15:36.972: INFO: Pod downward-api-dbef1149-dacc-412d-8d33-80309da0a36f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:15:36.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1970" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2980,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:15:36.991: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 25 11:15:37.045: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:15:43.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8330" for this suite.

• [SLOW TEST:6.360 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":172,"skipped":2997,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:15:43.356: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-1725
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1725
STEP: Deleting pre-stop pod
May 25 11:15:52.517: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:15:52.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1725" for this suite.

• [SLOW TEST:9.204 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":173,"skipped":3012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:15:52.560: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 25 11:15:52.631: INFO: Waiting up to 5m0s for pod "downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200" in namespace "downward-api-9018" to be "Succeeded or Failed"
May 25 11:15:52.641: INFO: Pod "downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200": Phase="Pending", Reason="", readiness=false. Elapsed: 9.325455ms
May 25 11:15:54.656: INFO: Pod "downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024564555s
STEP: Saw pod success
May 25 11:15:54.656: INFO: Pod "downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200" satisfied condition "Succeeded or Failed"
May 25 11:15:54.662: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200 container dapi-container: <nil>
STEP: delete the pod
May 25 11:15:54.690: INFO: Waiting for pod downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200 to disappear
May 25 11:15:54.696: INFO: Pod downward-api-66bdb281-06b2-4ed2-a2f2-fcc1dbaaf200 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:15:54.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9018" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:15:54.716: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
May 25 11:15:54.763: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-6735 proxy --unix-socket=/tmp/kubectl-proxy-unix439704936/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:15:54.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6735" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":175,"skipped":3077,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:15:54.849: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:15:55.464: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 11:15:57.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538155, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538155, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538155, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538155, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:16:00.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:16:10.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2370" for this suite.
STEP: Destroying namespace "webhook-2370-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.162 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":176,"skipped":3094,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:16:11.013: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7650
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7650
STEP: Creating statefulset with conflicting port in namespace statefulset-7650
STEP: Waiting until pod test-pod will start running in namespace statefulset-7650
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7650
May 25 11:16:13.159: INFO: Observed stateful pod in namespace: statefulset-7650, name: ss-0, uid: c31d4c74-fd0d-41f3-97ac-ba1cd3ff4d91, status phase: Pending. Waiting for statefulset controller to delete.
May 25 11:16:13.577: INFO: Observed stateful pod in namespace: statefulset-7650, name: ss-0, uid: c31d4c74-fd0d-41f3-97ac-ba1cd3ff4d91, status phase: Failed. Waiting for statefulset controller to delete.
May 25 11:16:13.595: INFO: Observed stateful pod in namespace: statefulset-7650, name: ss-0, uid: c31d4c74-fd0d-41f3-97ac-ba1cd3ff4d91, status phase: Failed. Waiting for statefulset controller to delete.
May 25 11:16:13.602: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7650
STEP: Removing pod with conflicting port in namespace statefulset-7650
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7650 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 25 11:16:19.691: INFO: Deleting all statefulset in ns statefulset-7650
May 25 11:16:19.696: INFO: Scaling statefulset ss to 0
May 25 11:16:39.749: INFO: Waiting for statefulset status.replicas updated to 0
May 25 11:16:39.754: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:16:39.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7650" for this suite.

• [SLOW TEST:28.794 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":177,"skipped":3094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:16:39.806: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-340/secret-test-a3aebd37-a82f-4501-ba88-119157683634
STEP: Creating a pod to test consume secrets
May 25 11:16:39.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849" in namespace "secrets-340" to be "Succeeded or Failed"
May 25 11:16:39.879: INFO: Pod "pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849": Phase="Pending", Reason="", readiness=false. Elapsed: 6.304433ms
May 25 11:16:41.893: INFO: Pod "pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020536298s
STEP: Saw pod success
May 25 11:16:41.893: INFO: Pod "pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849" satisfied condition "Succeeded or Failed"
May 25 11:16:41.900: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849 container env-test: <nil>
STEP: delete the pod
May 25 11:16:41.928: INFO: Waiting for pod pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849 to disappear
May 25 11:16:41.936: INFO: Pod pod-configmaps-81eae6d7-48cb-45dd-9699-21af753ea849 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:16:41.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-340" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3127,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:16:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 25 11:16:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:17:00.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8298" for this suite.

• [SLOW TEST:18.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":179,"skipped":3130,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:17:00.403: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:17:01.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:17:04.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:17:04.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3377" for this suite.
STEP: Destroying namespace "webhook-3377-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":180,"skipped":3138,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:17:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:17:05.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7" in namespace "projected-2616" to be "Succeeded or Failed"
May 25 11:17:05.113: INFO: Pod "downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.354716ms
May 25 11:17:07.124: INFO: Pod "downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02060313s
STEP: Saw pod success
May 25 11:17:07.124: INFO: Pod "downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7" satisfied condition "Succeeded or Failed"
May 25 11:17:07.131: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7 container client-container: <nil>
STEP: delete the pod
May 25 11:17:07.168: INFO: Waiting for pod downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7 to disappear
May 25 11:17:07.177: INFO: Pod downwardapi-volume-3a66e88b-53d7-4675-9040-a219795611e7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:17:07.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2616" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3154,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:17:07.197: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
May 25 11:17:07.263: INFO: Waiting up to 5m0s for pod "test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28" in namespace "svcaccounts-3261" to be "Succeeded or Failed"
May 25 11:17:07.272: INFO: Pod "test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28": Phase="Pending", Reason="", readiness=false. Elapsed: 8.714566ms
May 25 11:17:09.288: INFO: Pod "test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024820877s
STEP: Saw pod success
May 25 11:17:09.288: INFO: Pod "test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28" satisfied condition "Succeeded or Failed"
May 25 11:17:09.295: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:17:09.327: INFO: Waiting for pod test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28 to disappear
May 25 11:17:09.339: INFO: Pod test-pod-a92b34f4-cab9-44e5-84cb-b738c829be28 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:17:09.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3261" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":182,"skipped":3155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:17:09.362: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-dmb6
STEP: Creating a pod to test atomic-volume-subpath
May 25 11:17:09.452: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dmb6" in namespace "subpath-2261" to be "Succeeded or Failed"
May 25 11:17:09.461: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.585216ms
May 25 11:17:11.475: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022640462s
May 25 11:17:13.487: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 4.034881761s
May 25 11:17:15.502: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 6.049808036s
May 25 11:17:17.515: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 8.06239328s
May 25 11:17:19.530: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 10.077627082s
May 25 11:17:21.546: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 12.093335205s
May 25 11:17:23.561: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 14.108676876s
May 25 11:17:25.577: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 16.124528268s
May 25 11:17:27.590: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 18.137351712s
May 25 11:17:29.608: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Running", Reason="", readiness=true. Elapsed: 20.155617143s
May 25 11:17:31.623: INFO: Pod "pod-subpath-test-configmap-dmb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.170423086s
STEP: Saw pod success
May 25 11:17:31.623: INFO: Pod "pod-subpath-test-configmap-dmb6" satisfied condition "Succeeded or Failed"
May 25 11:17:31.628: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-subpath-test-configmap-dmb6 container test-container-subpath-configmap-dmb6: <nil>
STEP: delete the pod
May 25 11:17:31.667: INFO: Waiting for pod pod-subpath-test-configmap-dmb6 to disappear
May 25 11:17:31.674: INFO: Pod pod-subpath-test-configmap-dmb6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dmb6
May 25 11:17:31.674: INFO: Deleting pod "pod-subpath-test-configmap-dmb6" in namespace "subpath-2261"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:17:31.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2261" for this suite.

• [SLOW TEST:22.339 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":183,"skipped":3183,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:17:31.701: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-59e8fae6-5d3a-4d97-8919-245bdbb9fe13
STEP: Creating a pod to test consume secrets
May 25 11:17:31.784: INFO: Waiting up to 5m0s for pod "pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2" in namespace "secrets-3255" to be "Succeeded or Failed"
May 25 11:17:31.791: INFO: Pod "pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.223203ms
May 25 11:17:33.800: INFO: Pod "pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016591113s
STEP: Saw pod success
May 25 11:17:33.800: INFO: Pod "pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2" satisfied condition "Succeeded or Failed"
May 25 11:17:33.806: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2 container secret-env-test: <nil>
STEP: delete the pod
May 25 11:17:33.831: INFO: Waiting for pod pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2 to disappear
May 25 11:17:33.841: INFO: Pod pod-secrets-027c1188-6718-4f55-a0fb-e15e3909bac2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:17:33.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3255" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":3185,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:17:33.861: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 25 11:17:33.914: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 11:17:33.931: INFO: Waiting for terminating namespaces to be deleted...
May 25 11:17:33.938: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-p6wx before test
May 25 11:17:33.946: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:17:33.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:17:33.947: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:17:33.947: INFO: sonobuoy-e2e-job-7c943d3357464792 from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:17:33.947: INFO: 	Container e2e ready: true, restart count 0
May 25 11:17:33.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:17:33.947: INFO: sonobuoy from sonobuoy started at 2021-05-25 10:27:41 +0000 UTC (1 container statuses recorded)
May 25 11:17:33.947: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 11:17:33.947: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-wpkp before test
May 25 11:17:33.959: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-8lgrl from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:17:33.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:17:33.959: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:17:33.959: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-vl79 before test
May 25 11:17:33.966: INFO: coredns-854c77959c-q5878 from kube-system started at 2021-05-19 12:01:03 +0000 UTC (1 container statuses recorded)
May 25 11:17:33.966: INFO: 	Container coredns ready: true, restart count 0
May 25 11:17:33.966: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-czfnh from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:17:33.966: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:17:33.966: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-91446f46-6059-4a48-a064-ae34d0276d0b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.156.0.14 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-91446f46-6059-4a48-a064-ae34d0276d0b off the node gke-cluster-1-default-pool-8f0bb8bb-p6wx
STEP: verifying the node doesn't have the label kubernetes.io/e2e-91446f46-6059-4a48-a064-ae34d0276d0b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:22:38.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6251" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.810 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":185,"skipped":3191,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:22:38.676: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6393
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-6393
May 25 11:22:38.813: INFO: Found 0 stateful pods, waiting for 1
May 25 11:22:48.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 25 11:22:48.893: INFO: Deleting all statefulset in ns statefulset-6393
May 25 11:22:48.907: INFO: Scaling statefulset ss to 0
May 25 11:23:08.980: INFO: Waiting for statefulset status.replicas updated to 0
May 25 11:23:08.985: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:09.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6393" for this suite.

• [SLOW TEST:30.372 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":186,"skipped":3207,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:09.053: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:16.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1573" for this suite.

• [SLOW TEST:7.143 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":187,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:16.201: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
May 25 11:23:16.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-3641 cluster-info'
May 25 11:23:16.671: INFO: stderr: ""
May 25 11:23:16.671: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.68.9.253:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:16.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3641" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":188,"skipped":3329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:16.697: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
May 25 11:23:16.909: INFO: Waiting up to 5m0s for pod "pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4" in namespace "emptydir-6123" to be "Succeeded or Failed"
May 25 11:23:16.917: INFO: Pod "pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.107097ms
May 25 11:23:18.935: INFO: Pod "pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026224754s
STEP: Saw pod success
May 25 11:23:18.935: INFO: Pod "pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4" satisfied condition "Succeeded or Failed"
May 25 11:23:18.942: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4 container test-container: <nil>
STEP: delete the pod
May 25 11:23:18.990: INFO: Waiting for pod pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4 to disappear
May 25 11:23:19.003: INFO: Pod pod-fb5f17c5-5ca6-49dd-a2ce-5621faf61be4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:19.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6123" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3364,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:19.024: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:23:19.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb" in namespace "downward-api-9379" to be "Succeeded or Failed"
May 25 11:23:19.101: INFO: Pod "downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.224327ms
May 25 11:23:21.114: INFO: Pod "downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021007139s
STEP: Saw pod success
May 25 11:23:21.114: INFO: Pod "downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb" satisfied condition "Succeeded or Failed"
May 25 11:23:21.120: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb container client-container: <nil>
STEP: delete the pod
May 25 11:23:21.148: INFO: Waiting for pod downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb to disappear
May 25 11:23:21.159: INFO: Pod downwardapi-volume-4124b599-5cb4-48ee-b0fb-5981c3838ebb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:21.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9379" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3366,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:21.182: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:23:21.703: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:23:24.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:24.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1850" for this suite.
STEP: Destroying namespace "webhook-1850-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":191,"skipped":3369,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:25.054: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 25 11:23:25.121: INFO: Waiting up to 5m0s for pod "pod-975c5220-78b6-48ac-b86a-d1bc02e983e5" in namespace "emptydir-7168" to be "Succeeded or Failed"
May 25 11:23:25.130: INFO: Pod "pod-975c5220-78b6-48ac-b86a-d1bc02e983e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.899595ms
May 25 11:23:27.160: INFO: Pod "pod-975c5220-78b6-48ac-b86a-d1bc02e983e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039705116s
STEP: Saw pod success
May 25 11:23:27.160: INFO: Pod "pod-975c5220-78b6-48ac-b86a-d1bc02e983e5" satisfied condition "Succeeded or Failed"
May 25 11:23:27.170: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-975c5220-78b6-48ac-b86a-d1bc02e983e5 container test-container: <nil>
STEP: delete the pod
May 25 11:23:27.243: INFO: Waiting for pod pod-975c5220-78b6-48ac-b86a-d1bc02e983e5 to disappear
May 25 11:23:27.260: INFO: Pod pod-975c5220-78b6-48ac-b86a-d1bc02e983e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:27.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7168" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:27.294: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-795e035b-34ad-4b9c-b3a4-4b3086d9e75b
STEP: Creating a pod to test consume configMaps
May 25 11:23:27.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa" in namespace "configmap-3483" to be "Succeeded or Failed"
May 25 11:23:27.617: INFO: Pod "pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.16256ms
May 25 11:23:29.632: INFO: Pod "pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02129629s
STEP: Saw pod success
May 25 11:23:29.632: INFO: Pod "pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa" satisfied condition "Succeeded or Failed"
May 25 11:23:29.638: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa container agnhost-container: <nil>
STEP: delete the pod
May 25 11:23:29.660: INFO: Waiting for pod pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa to disappear
May 25 11:23:29.672: INFO: Pod pod-configmaps-6dc6e561-7f24-42be-b754-24676036acaa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:29.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3483" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:29.691: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:46.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3424" for this suite.

• [SLOW TEST:16.348 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":194,"skipped":3450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:46.044: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 25 11:23:47.701: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0525 11:23:47.701364      19 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0525 11:23:47.701393      19 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0525 11:23:47.701401      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3440" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":195,"skipped":3476,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:47.718: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-72491beb-245e-43e3-adbd-b6c98dfb93f1
STEP: Creating a pod to test consume secrets
May 25 11:23:47.798: INFO: Waiting up to 5m0s for pod "pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b" in namespace "secrets-7249" to be "Succeeded or Failed"
May 25 11:23:47.809: INFO: Pod "pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.681499ms
May 25 11:23:49.823: INFO: Pod "pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025000888s
STEP: Saw pod success
May 25 11:23:49.823: INFO: Pod "pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b" satisfied condition "Succeeded or Failed"
May 25 11:23:49.829: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b container secret-volume-test: <nil>
STEP: delete the pod
May 25 11:23:49.857: INFO: Waiting for pod pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b to disappear
May 25 11:23:49.866: INFO: Pod pod-secrets-97f88cc3-5e7a-42f3-b4ec-c6521ac2fa4b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:49.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7249" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:49.889: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-e94d7ad7-724b-4b88-8a3f-6b784e6f6944
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e94d7ad7-724b-4b88-8a3f-6b784e6f6944
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:23:54.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9020" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3552,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:23:54.138: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:23:56.225: INFO: Deleting pod "var-expansion-5aff1e7e-3baf-47cc-9664-03d0e248f375" in namespace "var-expansion-6694"
May 25 11:23:56.235: INFO: Wait up to 5m0s for pod "var-expansion-5aff1e7e-3baf-47cc-9664-03d0e248f375" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:24:00.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6694" for this suite.

• [SLOW TEST:6.136 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":198,"skipped":3554,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:24:00.274: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:24:00.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c" in namespace "projected-546" to be "Succeeded or Failed"
May 25 11:24:00.352: INFO: Pod "downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.84319ms
May 25 11:24:02.365: INFO: Pod "downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019948012s
STEP: Saw pod success
May 25 11:24:02.366: INFO: Pod "downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c" satisfied condition "Succeeded or Failed"
May 25 11:24:02.371: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c container client-container: <nil>
STEP: delete the pod
May 25 11:24:02.409: INFO: Waiting for pod downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c to disappear
May 25 11:24:02.425: INFO: Pod downwardapi-volume-23aa640b-8c00-4e96-a278-af40bb3ff87c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:24:02.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-546" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3555,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:24:02.474: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:24:05.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9009" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":200,"skipped":3555,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:24:05.682: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 25 11:24:05.739: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 11:24:05.756: INFO: Waiting for terminating namespaces to be deleted...
May 25 11:24:05.763: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-vl79 before test
May 25 11:24:05.771: INFO: coredns-854c77959c-q5878 from kube-system started at 2021-05-19 12:01:03 +0000 UTC (1 container statuses recorded)
May 25 11:24:05.772: INFO: 	Container coredns ready: true, restart count 0
May 25 11:24:05.772: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-czfnh from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:24:05.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:24:05.772: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:24:05.772: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-p6wx before test
May 25 11:24:05.781: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:24:05.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:24:05.782: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:24:05.782: INFO: sonobuoy-e2e-job-7c943d3357464792 from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:24:05.782: INFO: 	Container e2e ready: true, restart count 0
May 25 11:24:05.782: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:24:05.782: INFO: sonobuoy from sonobuoy started at 2021-05-25 10:27:41 +0000 UTC (1 container statuses recorded)
May 25 11:24:05.782: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 11:24:05.782: INFO: pod-adoption from replication-controller-9009 started at 2021-05-25 11:24:02 +0000 UTC (1 container statuses recorded)
May 25 11:24:05.783: INFO: 	Container pod-adoption ready: true, restart count 0
May 25 11:24:05.783: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-wpkp before test
May 25 11:24:05.790: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-8lgrl from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:24:05.790: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:24:05.790: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1aa08e43-c93a-47ab-ac51-12afe9ae6883 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.156.0.14 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.156.0.14 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 25 11:24:16.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.156.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:16.248: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321
May 25 11:24:16.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.156.0.14:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:16.365: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321 UDP
May 25 11:24:16.462: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.156.0.14 54321] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:16.462: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 25 11:24:21.584: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.156.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:21.584: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321
May 25 11:24:21.709: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.156.0.14:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:21.709: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321 UDP
May 25 11:24:21.828: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.156.0.14 54321] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:21.829: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 25 11:24:26.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.156.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:26.927: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321
May 25 11:24:27.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.156.0.14:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:27.033: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321 UDP
May 25 11:24:27.148: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.156.0.14 54321] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 25 11:24:32.296: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.156.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:32.296: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321
May 25 11:24:32.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.156.0.14:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:32.437: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321 UDP
May 25 11:24:32.584: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.156.0.14 54321] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:32.584: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 25 11:24:37.700: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.156.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:37.701: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321
May 25 11:24:37.805: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.156.0.14:54321/hostname] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:37.805: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.156.0.14, port: 54321 UDP
May 25 11:24:37.909: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.156.0.14 54321] Namespace:sched-pred-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:24:37.909: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: removing the label kubernetes.io/e2e-1aa08e43-c93a-47ab-ac51-12afe9ae6883 off the node gke-cluster-1-default-pool-8f0bb8bb-p6wx
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1aa08e43-c93a-47ab-ac51-12afe9ae6883
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:24:43.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3950" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:37.665 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":201,"skipped":3565,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:24:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:24:43.458: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 25 11:24:43.485: INFO: Number of nodes with available pods: 0
May 25 11:24:43.486: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 11:24:44.522: INFO: Number of nodes with available pods: 0
May 25 11:24:44.523: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-wpkp is running more than one daemon pod
May 25 11:24:45.508: INFO: Number of nodes with available pods: 3
May 25 11:24:45.508: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 25 11:24:45.572: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:45.572: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:45.573: INFO: Wrong image for pod: daemon-set-wkt59. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:46.590: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:46.590: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:46.590: INFO: Wrong image for pod: daemon-set-wkt59. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:47.597: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:47.597: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:47.597: INFO: Wrong image for pod: daemon-set-wkt59. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:48.594: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:48.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:48.594: INFO: Wrong image for pod: daemon-set-wkt59. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:49.599: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:49.599: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:49.600: INFO: Pod daemon-set-pgw9s is not available
May 25 11:24:50.590: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:50.590: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:50.590: INFO: Pod daemon-set-pgw9s is not available
May 25 11:24:51.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:51.594: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:52.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:52.595: INFO: Wrong image for pod: daemon-set-7dtpt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:53.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:53.595: INFO: Pod daemon-set-ff57n is not available
May 25 11:24:54.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:55.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:56.596: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:57.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:58.598: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:24:59.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:00.596: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:01.600: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:02.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:03.596: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:04.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:05.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:06.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:07.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:08.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:09.600: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:10.637: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:11.600: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:12.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:13.592: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:14.601: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:15.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:16.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:17.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:18.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:19.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:20.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:21.591: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:22.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:23.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:24.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:25.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:26.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:27.597: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:28.620: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:29.599: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:30.590: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:31.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:32.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:33.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:34.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:35.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:36.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:37.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:38.591: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:39.596: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:40.598: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:41.592: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:42.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:43.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:44.602: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:45.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:46.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:47.600: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:48.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:49.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:50.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:51.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:52.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:53.599: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:54.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:55.592: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:56.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:57.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:58.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:25:59.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:00.602: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:01.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:02.598: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:03.610: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:04.592: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:05.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:06.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:07.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:08.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:09.597: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:10.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:11.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:12.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:13.602: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:14.593: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:15.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:16.595: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:17.594: INFO: Wrong image for pod: daemon-set-qzqdk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 25 11:26:18.598: INFO: Pod daemon-set-9q28q is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 25 11:26:18.618: INFO: Number of nodes with available pods: 2
May 25 11:26:18.618: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 11:26:19.636: INFO: Number of nodes with available pods: 2
May 25 11:26:19.636: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 11:26:20.640: INFO: Number of nodes with available pods: 3
May 25 11:26:20.640: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-333, will wait for the garbage collector to delete the pods
May 25 11:26:20.740: INFO: Deleting DaemonSet.extensions daemon-set took: 11.338764ms
May 25 11:26:21.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.226123ms
May 25 11:26:31.449: INFO: Number of nodes with available pods: 0
May 25 11:26:31.449: INFO: Number of running nodes: 0, number of available pods: 0
May 25 11:26:31.458: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98952"},"items":null}

May 25 11:26:31.464: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98952"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:26:31.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-333" for this suite.

• [SLOW TEST:108.162 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":202,"skipped":3586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:26:31.518: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 25 11:26:31.602: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-143  d4871b5a-b48e-4a15-9a79-607f8983826c 98959 0 2021-05-25 11:26:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 11:26:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 11:26:31.602: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-143  d4871b5a-b48e-4a15-9a79-607f8983826c 98960 0 2021-05-25 11:26:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 11:26:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 25 11:26:31.634: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-143  d4871b5a-b48e-4a15-9a79-607f8983826c 98961 0 2021-05-25 11:26:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 11:26:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 11:26:31.635: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-143  d4871b5a-b48e-4a15-9a79-607f8983826c 98962 0 2021-05-25 11:26:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 11:26:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:26:31.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-143" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":203,"skipped":3614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:26:31.654: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
May 25 11:26:31.720: INFO: Waiting up to 5m0s for pod "pod-f8224f00-c794-445b-90f4-98444b9aeb67" in namespace "emptydir-214" to be "Succeeded or Failed"
May 25 11:26:31.727: INFO: Pod "pod-f8224f00-c794-445b-90f4-98444b9aeb67": Phase="Pending", Reason="", readiness=false. Elapsed: 6.890764ms
May 25 11:26:33.742: INFO: Pod "pod-f8224f00-c794-445b-90f4-98444b9aeb67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022260008s
STEP: Saw pod success
May 25 11:26:33.742: INFO: Pod "pod-f8224f00-c794-445b-90f4-98444b9aeb67" satisfied condition "Succeeded or Failed"
May 25 11:26:33.748: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-f8224f00-c794-445b-90f4-98444b9aeb67 container test-container: <nil>
STEP: delete the pod
May 25 11:26:33.791: INFO: Waiting for pod pod-f8224f00-c794-445b-90f4-98444b9aeb67 to disappear
May 25 11:26:33.798: INFO: Pod pod-f8224f00-c794-445b-90f4-98444b9aeb67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:26:33.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-214" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3667,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:26:33.820: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 25 11:26:34.367: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 25 11:26:36.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538794, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538794, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538794, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538794, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:26:39.479: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:26:39.489: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:26:40.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.947 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":205,"skipped":3674,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:26:40.770: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 25 11:26:40.865: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.865: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.869: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.869: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.887: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.887: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.918: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:40.919: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 11:26:42.938: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 25 11:26:42.938: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 25 11:26:42.952: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 25 11:26:42.993: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 0
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.008: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.025: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.025: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.056: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.056: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 2
May 25 11:26:43.066: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
STEP: listing Deployments
May 25 11:26:43.079: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 25 11:26:43.106: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 25 11:26:43.167: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.167: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.191: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.219: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.248: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.258: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.265: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 11:26:43.273: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 25 11:26:45.173: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.173: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.173: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.173: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.174: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.174: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.174: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
May 25 11:26:45.174: INFO: observed Deployment test-deployment in namespace deployment-9012 with ReadyReplicas 1
STEP: deleting the Deployment
May 25 11:26:45.210: INFO: observed event type MODIFIED
May 25 11:26:45.210: INFO: observed event type MODIFIED
May 25 11:26:45.210: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
May 25 11:26:45.211: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 25 11:26:45.234: INFO: Log out all the ReplicaSets if there is no deployment created
May 25 11:26:45.240: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-9012  4c9c58ae-0be1-4416-af39-71c3e04f76a0 99157 2 2021-05-25 11:26:40 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment c250e824-4bb0-47ab-a76a-795c0c943ca7 0xc002b886d7 0xc002b886d8}] []  [{k3s Update apps/v1 2021-05-25 11:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c250e824-4bb0-47ab-a76a-795c0c943ca7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b88788 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 25 11:26:45.247: INFO: pod: "test-deployment-8b6954bfb-jpc6f":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-jpc6f test-deployment-8b6954bfb- deployment-9012  e4d2088b-a809-4a7d-ab8e-541098582f97 99129 0 2021-05-25 11:26:40 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 4c9c58ae-0be1-4416-af39-71c3e04f76a0 0xc002b88db7 0xc002b88db8}] []  [{k3s Update v1 2021-05-25 11:26:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c9c58ae-0be1-4416-af39-71c3e04f76a0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 11:26:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-585bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-585bp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-585bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.205,StartTime:2021-05-25 11:26:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 11:26:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://62f14488a1b6a1c48db0cf18938dfea7e81b5bc747d2542fda20b475bfcb8d14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 25 11:26:45.247: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-9012  9504aa4e-a532-4569-a6c3-cef9155498a2 99214 3 2021-05-25 11:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment c250e824-4bb0-47ab-a76a-795c0c943ca7 0xc002b88817 0xc002b88818}] []  [{k3s Update apps/v1 2021-05-25 11:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c250e824-4bb0-47ab-a76a-795c0c943ca7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b88888 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 25 11:26:45.255: INFO: pod: "test-deployment-768947d6f5-fqxz6":
&Pod{ObjectMeta:{test-deployment-768947d6f5-fqxz6 test-deployment-768947d6f5- deployment-9012  fb7b47f0-8728-4f7b-90be-6198975cbf3b 99200 0 2021-05-25 11:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 9504aa4e-a532-4569-a6c3-cef9155498a2 0xc002b89bc7 0xc002b89bc8}] []  [{k3s Update v1 2021-05-25 11:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9504aa4e-a532-4569-a6c3-cef9155498a2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 11:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-585bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-585bp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-585bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-wpkp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.16,PodIP:10.64.2.160,StartTime:2021-05-25 11:26:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 11:26:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://791170f2269a8077847eb6c487a390a29ed1537419604af9a524ce6f766fe6cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 25 11:26:45.256: INFO: pod: "test-deployment-768947d6f5-gswf8":
&Pod{ObjectMeta:{test-deployment-768947d6f5-gswf8 test-deployment-768947d6f5- deployment-9012  235dbaa7-2834-4cf2-b4f6-9528e812b79e 99220 0 2021-05-25 11:26:45 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 9504aa4e-a532-4569-a6c3-cef9155498a2 0xc002b89f87 0xc002b89f88}] []  [{k3s Update v1 2021-05-25 11:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9504aa4e-a532-4569-a6c3-cef9155498a2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 11:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-585bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-585bp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-585bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:26:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 11:26:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 25 11:26:45.256: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-9012  799c7053-d91e-4505-ad1d-0a361c7d1a6d 99221 4 2021-05-25 11:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment c250e824-4bb0-47ab-a76a-795c0c943ca7 0xc002b88977 0xc002b88978}] []  [{k3s Update apps/v1 2021-05-25 11:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c250e824-4bb0-47ab-a76a-795c0c943ca7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b88a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:26:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9012" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":206,"skipped":3681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:26:45.283: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:26:45.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42" in namespace "downward-api-2206" to be "Succeeded or Failed"
May 25 11:26:45.400: INFO: Pod "downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.57757ms
May 25 11:26:47.414: INFO: Pod "downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42": Phase="Running", Reason="", readiness=true. Elapsed: 2.022928111s
May 25 11:26:49.431: INFO: Pod "downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039326301s
STEP: Saw pod success
May 25 11:26:49.431: INFO: Pod "downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42" satisfied condition "Succeeded or Failed"
May 25 11:26:49.438: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42 container client-container: <nil>
STEP: delete the pod
May 25 11:26:49.469: INFO: Waiting for pod downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42 to disappear
May 25 11:26:49.479: INFO: Pod downwardapi-volume-0f2ead49-14c4-44e4-8d9b-86cb7a220f42 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:26:49.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2206" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3732,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:26:49.499: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:26:50.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:26:53.236: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:26:53.249: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Registering the custom resource webhook via the AdmissionRegistration API
May 25 11:27:03.814: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:27:04.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3669" for this suite.
STEP: Destroying namespace "webhook-3669-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.235 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":208,"skipped":3736,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:27:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1676
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 11:27:04.830: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 11:27:04.993: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 11:27:07.009: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:27:09.006: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:27:11.009: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:27:13.002: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:27:15.010: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:27:17.027: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 11:27:19.008: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 11:27:19.021: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 25 11:27:19.034: INFO: The status of Pod netserver-2 is Running (Ready = false)
May 25 11:27:21.052: INFO: The status of Pod netserver-2 is Running (Ready = false)
May 25 11:27:23.047: INFO: The status of Pod netserver-2 is Running (Ready = false)
May 25 11:27:25.052: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 25 11:27:27.097: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 25 11:27:27.097: INFO: Breadth first check of 10.64.0.212 on host 10.156.0.14...
May 25 11:27:27.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.0.213:9080/dial?request=hostname&protocol=udp&host=10.64.0.212&port=8081&tries=1'] Namespace:pod-network-test-1676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:27:27.103: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:27:27.214: INFO: Waiting for responses: map[]
May 25 11:27:27.214: INFO: reached 10.64.0.212 after 0/1 tries
May 25 11:27:27.214: INFO: Breadth first check of 10.64.2.161 on host 10.156.0.16...
May 25 11:27:27.229: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.0.213:9080/dial?request=hostname&protocol=udp&host=10.64.2.161&port=8081&tries=1'] Namespace:pod-network-test-1676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:27:27.229: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:27:27.360: INFO: Waiting for responses: map[]
May 25 11:27:27.360: INFO: reached 10.64.2.161 after 0/1 tries
May 25 11:27:27.360: INFO: Breadth first check of 10.64.1.175 on host 10.156.0.15...
May 25 11:27:27.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.0.213:9080/dial?request=hostname&protocol=udp&host=10.64.1.175&port=8081&tries=1'] Namespace:pod-network-test-1676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:27:27.529: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:27:27.657: INFO: Waiting for responses: map[]
May 25 11:27:27.657: INFO: reached 10.64.1.175 after 0/1 tries
May 25 11:27:27.657: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:27:27.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1676" for this suite.

• [SLOW TEST:22.940 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3754,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:27:27.683: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
May 25 11:27:27.766: INFO: Waiting up to 5m0s for pod "client-containers-e621fde8-836d-49e4-8950-c537cde96d3d" in namespace "containers-6730" to be "Succeeded or Failed"
May 25 11:27:27.773: INFO: Pod "client-containers-e621fde8-836d-49e4-8950-c537cde96d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.343669ms
May 25 11:27:29.789: INFO: Pod "client-containers-e621fde8-836d-49e4-8950-c537cde96d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022351171s
STEP: Saw pod success
May 25 11:27:29.789: INFO: Pod "client-containers-e621fde8-836d-49e4-8950-c537cde96d3d" satisfied condition "Succeeded or Failed"
May 25 11:27:29.795: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod client-containers-e621fde8-836d-49e4-8950-c537cde96d3d container agnhost-container: <nil>
STEP: delete the pod
May 25 11:27:29.828: INFO: Waiting for pod client-containers-e621fde8-836d-49e4-8950-c537cde96d3d to disappear
May 25 11:27:29.839: INFO: Pod client-containers-e621fde8-836d-49e4-8950-c537cde96d3d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:27:29.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6730" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:27:29.856: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:27:30.392: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 11:27:32.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538850, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538850, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538850, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538850, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:27:35.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:27:35.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4172" for this suite.
STEP: Destroying namespace "webhook-4172-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.003 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":211,"skipped":3791,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:27:35.859: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-b48b4242-2dc1-4da8-97f4-4157186736b6
STEP: Creating a pod to test consume configMaps
May 25 11:27:35.923: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216" in namespace "projected-2921" to be "Succeeded or Failed"
May 25 11:27:35.931: INFO: Pod "pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216": Phase="Pending", Reason="", readiness=false. Elapsed: 7.757207ms
May 25 11:27:37.965: INFO: Pod "pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041933401s
STEP: Saw pod success
May 25 11:27:37.965: INFO: Pod "pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216" satisfied condition "Succeeded or Failed"
May 25 11:27:37.971: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:27:38.025: INFO: Waiting for pod pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216 to disappear
May 25 11:27:38.035: INFO: Pod pod-projected-configmaps-48088443-76c6-466b-8dbd-8804d2637216 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:27:38.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2921" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3793,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:27:38.058: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:27:38.535: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 11:27:40.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538858, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538858, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538858, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538858, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:27:43.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
May 25 11:27:53.678: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:27:53.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8256" for this suite.
STEP: Destroying namespace "webhook-8256-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.883 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":213,"skipped":3796,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:27:53.944: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3276.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3276.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.2.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.2.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.2.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.2.247_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3276.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3276.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.2.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.2.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.2.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.2.247_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 11:27:58.218: INFO: Unable to read wheezy_udp@dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.225: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.232: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.237: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.285: INFO: Unable to read jessie_udp@dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.298: INFO: Unable to read jessie_tcp@dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.309: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.317: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local from pod dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62: the server could not find the requested resource (get pods dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62)
May 25 11:27:58.369: INFO: Lookups using dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62 failed for: [wheezy_udp@dns-test-service.dns-3276.svc.cluster.local wheezy_tcp@dns-test-service.dns-3276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local jessie_udp@dns-test-service.dns-3276.svc.cluster.local jessie_tcp@dns-test-service.dns-3276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3276.svc.cluster.local]

May 25 11:28:03.530: INFO: DNS probes using dns-3276/dns-test-83cb7d3b-afe3-486d-82e0-fcb72da03c62 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3276" for this suite.

• [SLOW TEST:9.743 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":214,"skipped":3805,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:03.688: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-73ef7a9d-a532-4475-9009-badca5b2b970
STEP: Creating a pod to test consume secrets
May 25 11:28:03.764: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3" in namespace "projected-7787" to be "Succeeded or Failed"
May 25 11:28:03.772: INFO: Pod "pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.507923ms
May 25 11:28:05.813: INFO: Pod "pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048857858s
STEP: Saw pod success
May 25 11:28:05.813: INFO: Pod "pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3" satisfied condition "Succeeded or Failed"
May 25 11:28:05.833: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 11:28:05.880: INFO: Waiting for pod pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3 to disappear
May 25 11:28:05.890: INFO: Pod pod-projected-secrets-bc8ded78-a022-4271-9968-f1975bb4ddb3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7787" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3819,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:05.916: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:28:06.347: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:28:09.425: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:28:09.445: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9549-crds.webhook.example.com via the AdmissionRegistration API
May 25 11:28:20.051: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:20.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1305" for this suite.
STEP: Destroying namespace "webhook-1305-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.979 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":216,"skipped":3839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:20.900: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 25 11:28:20.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 create -f -'
May 25 11:28:21.471: INFO: stderr: ""
May 25 11:28:21.471: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 11:28:21.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 11:28:21.608: INFO: stderr: ""
May 25 11:28:21.608: INFO: stdout: "update-demo-nautilus-jhwz5 update-demo-nautilus-92mt5 "
May 25 11:28:21.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 11:28:21.838: INFO: stderr: ""
May 25 11:28:21.838: INFO: stdout: ""
May 25 11:28:21.838: INFO: update-demo-nautilus-jhwz5 is created but not running
May 25 11:28:26.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 11:28:26.955: INFO: stderr: ""
May 25 11:28:26.955: INFO: stdout: "update-demo-nautilus-92mt5 update-demo-nautilus-jhwz5 "
May 25 11:28:26.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-92mt5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 11:28:27.058: INFO: stderr: ""
May 25 11:28:27.058: INFO: stdout: "true"
May 25 11:28:27.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-92mt5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 11:28:27.162: INFO: stderr: ""
May 25 11:28:27.162: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 11:28:27.162: INFO: validating pod update-demo-nautilus-92mt5
May 25 11:28:27.181: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 11:28:27.181: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 11:28:27.181: INFO: update-demo-nautilus-92mt5 is verified up and running
May 25 11:28:27.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 11:28:27.282: INFO: stderr: ""
May 25 11:28:27.282: INFO: stdout: "true"
May 25 11:28:27.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 11:28:27.387: INFO: stderr: ""
May 25 11:28:27.387: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 11:28:27.387: INFO: validating pod update-demo-nautilus-jhwz5
May 25 11:28:27.396: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 11:28:27.396: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 11:28:27.396: INFO: update-demo-nautilus-jhwz5 is verified up and running
STEP: scaling down the replication controller
May 25 11:28:27.399: INFO: scanned /root for discovery docs: <nil>
May 25 11:28:27.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 25 11:28:28.555: INFO: stderr: ""
May 25 11:28:28.555: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 11:28:28.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 11:28:28.665: INFO: stderr: ""
May 25 11:28:28.665: INFO: stdout: "update-demo-nautilus-jhwz5 update-demo-nautilus-92mt5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 25 11:28:33.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 11:28:33.779: INFO: stderr: ""
May 25 11:28:33.779: INFO: stdout: "update-demo-nautilus-jhwz5 "
May 25 11:28:33.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 11:28:33.881: INFO: stderr: ""
May 25 11:28:33.881: INFO: stdout: "true"
May 25 11:28:33.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 11:28:33.999: INFO: stderr: ""
May 25 11:28:33.999: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 11:28:33.999: INFO: validating pod update-demo-nautilus-jhwz5
May 25 11:28:34.008: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 11:28:34.008: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 11:28:34.008: INFO: update-demo-nautilus-jhwz5 is verified up and running
STEP: scaling up the replication controller
May 25 11:28:34.011: INFO: scanned /root for discovery docs: <nil>
May 25 11:28:34.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 25 11:28:35.271: INFO: stderr: ""
May 25 11:28:35.271: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 11:28:35.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 11:28:35.397: INFO: stderr: ""
May 25 11:28:35.397: INFO: stdout: "update-demo-nautilus-jhwz5 update-demo-nautilus-tgcxm "
May 25 11:28:35.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 11:28:35.499: INFO: stderr: ""
May 25 11:28:35.499: INFO: stdout: "true"
May 25 11:28:35.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-jhwz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 11:28:35.601: INFO: stderr: ""
May 25 11:28:35.601: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 11:28:35.601: INFO: validating pod update-demo-nautilus-jhwz5
May 25 11:28:35.607: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 11:28:35.607: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 11:28:35.607: INFO: update-demo-nautilus-jhwz5 is verified up and running
May 25 11:28:35.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-tgcxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 11:28:35.718: INFO: stderr: ""
May 25 11:28:35.718: INFO: stdout: "true"
May 25 11:28:35.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods update-demo-nautilus-tgcxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 11:28:35.822: INFO: stderr: ""
May 25 11:28:35.822: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 25 11:28:35.822: INFO: validating pod update-demo-nautilus-tgcxm
May 25 11:28:35.831: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 11:28:35.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 11:28:35.831: INFO: update-demo-nautilus-tgcxm is verified up and running
STEP: using delete to clean up resources
May 25 11:28:35.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 delete --grace-period=0 --force -f -'
May 25 11:28:35.944: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:28:35.944: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 25 11:28:35.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get rc,svc -l name=update-demo --no-headers'
May 25 11:28:36.067: INFO: stderr: "No resources found in kubectl-378 namespace.\n"
May 25 11:28:36.067: INFO: stdout: ""
May 25 11:28:36.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-378 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 25 11:28:36.170: INFO: stderr: ""
May 25 11:28:36.170: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:36.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-378" for this suite.

• [SLOW TEST:15.287 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":217,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:36.187: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2077" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":218,"skipped":3933,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:49.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5484" for this suite.

• [SLOW TEST:13.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":219,"skipped":3942,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:49.485: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 11:28:49.683: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 25 11:28:49.691: INFO: starting watch
STEP: patching
STEP: updating
May 25 11:28:49.714: INFO: waiting for watch events with expected annotations
May 25 11:28:49.714: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:49.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6952" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":220,"skipped":3946,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:49.794: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:28:49.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24" in namespace "downward-api-2563" to be "Succeeded or Failed"
May 25 11:28:49.863: INFO: Pod "downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095843ms
May 25 11:28:51.879: INFO: Pod "downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023641663s
STEP: Saw pod success
May 25 11:28:51.879: INFO: Pod "downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24" satisfied condition "Succeeded or Failed"
May 25 11:28:51.885: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24 container client-container: <nil>
STEP: delete the pod
May 25 11:28:51.923: INFO: Waiting for pod downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24 to disappear
May 25 11:28:51.934: INFO: Pod downwardapi-volume-30c62fd8-e830-4f08-8b44-65c403f68a24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:51.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2563" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3953,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:51.957: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
May 25 11:28:52.026: INFO: created test-podtemplate-1
May 25 11:28:52.037: INFO: created test-podtemplate-2
May 25 11:28:52.046: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 25 11:28:52.105: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 25 11:28:52.130: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:28:52.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8283" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":222,"skipped":3967,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:28:52.184: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:28:52.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 11:28:54.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538932, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538932, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538932, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757538932, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:28:57.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 25 11:28:59.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=webhook-1364 attach --namespace=webhook-1364 to-be-attached-pod -i -c=container1'
May 25 11:29:00.036: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:00.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1364" for this suite.
STEP: Destroying namespace "webhook-1364-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.021 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":223,"skipped":3972,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:00.206: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:29:00.276: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Pending, waiting for it to be Running (with Ready = true)
May 25 11:29:02.284: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:04.291: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:06.290: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:08.292: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:10.291: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:12.287: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:14.290: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:16.291: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:18.290: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = false)
May 25 11:29:20.289: INFO: The status of Pod test-webserver-52df389a-ff86-4864-93fa-e545fc16031f is Running (Ready = true)
May 25 11:29:20.298: INFO: Container started at 2021-05-25 11:29:01 +0000 UTC, pod became ready at 2021-05-25 11:29:20 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:20.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3120" for this suite.

• [SLOW TEST:20.111 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3980,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:20.317: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 25 11:29:22.443: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1284 PodName:pod-sharedvolume-9f77e87d-f21a-462d-8b2a-52e2809053cb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:29:22.443: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:29:22.683: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1284" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":225,"skipped":3981,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:22.722: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:29:22.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-9378 version'
May 25 11:29:22.873: INFO: stderr: ""
May 25 11:29:22.873: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.5\", GitCommit:\"6b1d87acf3c8253c123756b9e61dac642678305f\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:10:43Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.5+k3s1\", GitCommit:\"355fff3017b06cde44dbd879408a3a6826fa7125\", GitTreeState:\"clean\", BuildDate:\"2021-03-31T06:21:52Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:22.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9378" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":226,"skipped":3985,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:22.893: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-c6303466-1dff-47fd-8e68-1d77222a81f2
STEP: Creating a pod to test consume configMaps
May 25 11:29:23.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def" in namespace "configmap-7242" to be "Succeeded or Failed"
May 25 11:29:23.025: INFO: Pod "pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def": Phase="Pending", Reason="", readiness=false. Elapsed: 6.872218ms
May 25 11:29:25.041: INFO: Pod "pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022761204s
STEP: Saw pod success
May 25 11:29:25.041: INFO: Pod "pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def" satisfied condition "Succeeded or Failed"
May 25 11:29:25.048: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def container agnhost-container: <nil>
STEP: delete the pod
May 25 11:29:25.081: INFO: Waiting for pod pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def to disappear
May 25 11:29:25.096: INFO: Pod pod-configmaps-253dd426-8499-4f1b-8a41-7fc766094def no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:25.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7242" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":227,"skipped":3989,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:25.113: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
May 25 11:29:25.178: INFO: created test-event-1
May 25 11:29:25.187: INFO: created test-event-2
May 25 11:29:25.195: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 25 11:29:25.203: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 25 11:29:25.231: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:25.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8261" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":228,"skipped":3992,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:25.256: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2101
STEP: creating service affinity-nodeport in namespace services-2101
STEP: creating replication controller affinity-nodeport in namespace services-2101
I0525 11:29:25.412063      19 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2101, replica count: 3
I0525 11:29:28.462669      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:29:28.508: INFO: Creating new exec pod
May 25 11:29:31.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 25 11:29:31.798: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 25 11:29:31.798: INFO: stdout: ""
May 25 11:29:31.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c nc -zv -t -w 2 10.68.4.49 80'
May 25 11:29:32.009: INFO: stderr: "+ nc -zv -t -w 2 10.68.4.49 80\nConnection to 10.68.4.49 80 port [tcp/http] succeeded!\n"
May 25 11:29:32.009: INFO: stdout: ""
May 25 11:29:32.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.16 31594'
May 25 11:29:32.216: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.16 31594\nConnection to 10.156.0.16 31594 port [tcp/31594] succeeded!\n"
May 25 11:29:32.216: INFO: stdout: ""
May 25 11:29:32.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.14 31594'
May 25 11:29:32.420: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.14 31594\nConnection to 10.156.0.14 31594 port [tcp/31594] succeeded!\n"
May 25 11:29:32.420: INFO: stdout: ""
May 25 11:29:32.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c nc -zv -t -w 2 35.246.205.6 31594'
May 25 11:29:32.636: INFO: stderr: "+ nc -zv -t -w 2 35.246.205.6 31594\nConnection to 35.246.205.6 31594 port [tcp/31594] succeeded!\n"
May 25 11:29:32.636: INFO: stdout: ""
May 25 11:29:32.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c nc -zv -t -w 2 34.107.45.63 31594'
May 25 11:29:32.883: INFO: stderr: "+ nc -zv -t -w 2 34.107.45.63 31594\nConnection to 34.107.45.63 31594 port [tcp/31594] succeeded!\n"
May 25 11:29:32.883: INFO: stdout: ""
May 25 11:29:32.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2101 exec execpod-affinityh8gt2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.156.0.16:31594/ ; done'
May 25 11:29:33.244: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31594/\n"
May 25 11:29:33.244: INFO: stdout: "\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc\naffinity-nodeport-qh8nc"
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Received response from host: affinity-nodeport-qh8nc
May 25 11:29:33.244: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2101, will wait for the garbage collector to delete the pods
May 25 11:29:33.341: INFO: Deleting ReplicationController affinity-nodeport took: 16.870052ms
May 25 11:29:34.041: INFO: Terminating ReplicationController affinity-nodeport pods took: 700.202508ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:42.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2101" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:17.161 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":229,"skipped":3992,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:42.419: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 25 11:29:42.539: INFO: Waiting up to 5m0s for pod "downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52" in namespace "downward-api-2885" to be "Succeeded or Failed"
May 25 11:29:42.545: INFO: Pod "downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.331697ms
May 25 11:29:44.556: INFO: Pod "downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017682364s
STEP: Saw pod success
May 25 11:29:44.557: INFO: Pod "downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52" satisfied condition "Succeeded or Failed"
May 25 11:29:44.571: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52 container dapi-container: <nil>
STEP: delete the pod
May 25 11:29:44.608: INFO: Waiting for pod downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52 to disappear
May 25 11:29:44.620: INFO: Pod downward-api-9db85671-c98a-4f4b-bcd8-2d11c8c2fe52 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:29:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2885" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3999,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:29:44.639: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-baf40398-3752-4c4b-a5aa-50b2158d483b in namespace container-probe-1256
May 25 11:29:46.752: INFO: Started pod busybox-baf40398-3752-4c4b-a5aa-50b2158d483b in namespace container-probe-1256
STEP: checking the pod's current state and verifying that restartCount is present
May 25 11:29:46.758: INFO: Initial restart count of pod busybox-baf40398-3752-4c4b-a5aa-50b2158d483b is 0
May 25 11:30:37.199: INFO: Restart count of pod container-probe-1256/busybox-baf40398-3752-4c4b-a5aa-50b2158d483b is now 1 (50.440606304s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:37.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1256" for this suite.

• [SLOW TEST:52.596 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":4001,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 11:30:39.351: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:39.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5401" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4005,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:39.405: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-c8a0f8eb-efd6-4d05-a87a-b542e6be17da
STEP: Creating a pod to test consume configMaps
May 25 11:30:39.484: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775" in namespace "projected-4943" to be "Succeeded or Failed"
May 25 11:30:39.492: INFO: Pod "pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775": Phase="Pending", Reason="", readiness=false. Elapsed: 7.980921ms
May 25 11:30:41.505: INFO: Pod "pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021575551s
STEP: Saw pod success
May 25 11:30:41.505: INFO: Pod "pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775" satisfied condition "Succeeded or Failed"
May 25 11:30:41.513: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:30:41.541: INFO: Waiting for pod pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775 to disappear
May 25 11:30:41.554: INFO: Pod pod-projected-configmaps-cfc2966e-ce07-42fd-9291-bb9d4ae7d775 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:41.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4943" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":4015,"failed":0}
SSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:41.575: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:41.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7780" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":234,"skipped":4020,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:41.828: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:30:41.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0" in namespace "downward-api-8209" to be "Succeeded or Failed"
May 25 11:30:41.960: INFO: Pod "downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.197525ms
May 25 11:30:43.976: INFO: Pod "downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029152094s
STEP: Saw pod success
May 25 11:30:43.977: INFO: Pod "downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0" satisfied condition "Succeeded or Failed"
May 25 11:30:43.983: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0 container client-container: <nil>
STEP: delete the pod
May 25 11:30:44.019: INFO: Waiting for pod downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0 to disappear
May 25 11:30:44.028: INFO: Pod downwardapi-volume-3d1b244f-cca4-4fc4-baca-b5ce39dc0ef0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:44.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8209" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":4054,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:44.083: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:30:44.349: INFO: Checking APIGroup: apiregistration.k8s.io
May 25 11:30:44.353: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 25 11:30:44.353: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.353: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 25 11:30:44.353: INFO: Checking APIGroup: apps
May 25 11:30:44.356: INFO: PreferredVersion.GroupVersion: apps/v1
May 25 11:30:44.357: INFO: Versions found [{apps/v1 v1}]
May 25 11:30:44.357: INFO: apps/v1 matches apps/v1
May 25 11:30:44.357: INFO: Checking APIGroup: events.k8s.io
May 25 11:30:44.361: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 25 11:30:44.361: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.361: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 25 11:30:44.361: INFO: Checking APIGroup: authentication.k8s.io
May 25 11:30:44.364: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 25 11:30:44.364: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.364: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 25 11:30:44.364: INFO: Checking APIGroup: authorization.k8s.io
May 25 11:30:44.368: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 25 11:30:44.368: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.368: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 25 11:30:44.368: INFO: Checking APIGroup: autoscaling
May 25 11:30:44.374: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 25 11:30:44.374: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 25 11:30:44.374: INFO: autoscaling/v1 matches autoscaling/v1
May 25 11:30:44.374: INFO: Checking APIGroup: batch
May 25 11:30:44.379: INFO: PreferredVersion.GroupVersion: batch/v1
May 25 11:30:44.379: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 25 11:30:44.379: INFO: batch/v1 matches batch/v1
May 25 11:30:44.379: INFO: Checking APIGroup: certificates.k8s.io
May 25 11:30:44.384: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 25 11:30:44.384: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.384: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 25 11:30:44.384: INFO: Checking APIGroup: networking.k8s.io
May 25 11:30:44.389: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 25 11:30:44.389: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.389: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 25 11:30:44.389: INFO: Checking APIGroup: extensions
May 25 11:30:44.402: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 25 11:30:44.402: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 25 11:30:44.402: INFO: extensions/v1beta1 matches extensions/v1beta1
May 25 11:30:44.402: INFO: Checking APIGroup: policy
May 25 11:30:44.407: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 25 11:30:44.407: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 25 11:30:44.407: INFO: policy/v1beta1 matches policy/v1beta1
May 25 11:30:44.408: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 25 11:30:44.416: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 25 11:30:44.416: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.416: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 25 11:30:44.416: INFO: Checking APIGroup: storage.k8s.io
May 25 11:30:44.440: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 25 11:30:44.440: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.440: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 25 11:30:44.440: INFO: Checking APIGroup: admissionregistration.k8s.io
May 25 11:30:44.455: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 25 11:30:44.455: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.455: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 25 11:30:44.455: INFO: Checking APIGroup: apiextensions.k8s.io
May 25 11:30:44.460: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 25 11:30:44.460: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.461: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 25 11:30:44.461: INFO: Checking APIGroup: scheduling.k8s.io
May 25 11:30:44.469: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 25 11:30:44.469: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.469: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 25 11:30:44.469: INFO: Checking APIGroup: coordination.k8s.io
May 25 11:30:44.475: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 25 11:30:44.475: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.475: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 25 11:30:44.475: INFO: Checking APIGroup: node.k8s.io
May 25 11:30:44.483: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 25 11:30:44.483: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.483: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 25 11:30:44.483: INFO: Checking APIGroup: discovery.k8s.io
May 25 11:30:44.496: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 25 11:30:44.496: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.496: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 25 11:30:44.496: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 25 11:30:44.505: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 25 11:30:44.506: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 25 11:30:44.506: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 25 11:30:44.506: INFO: Checking APIGroup: helm.cattle.io
May 25 11:30:44.518: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
May 25 11:30:44.518: INFO: Versions found [{helm.cattle.io/v1 v1}]
May 25 11:30:44.518: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
May 25 11:30:44.518: INFO: Checking APIGroup: k3s.cattle.io
May 25 11:30:44.523: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
May 25 11:30:44.523: INFO: Versions found [{k3s.cattle.io/v1 v1}]
May 25 11:30:44.523: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
May 25 11:30:44.523: INFO: Checking APIGroup: policy.jspolicy.com
May 25 11:30:44.535: INFO: PreferredVersion.GroupVersion: policy.jspolicy.com/v1beta1
May 25 11:30:44.535: INFO: Versions found [{policy.jspolicy.com/v1beta1 v1beta1}]
May 25 11:30:44.535: INFO: policy.jspolicy.com/v1beta1 matches policy.jspolicy.com/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:44.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1008" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":236,"skipped":4065,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:44.590: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 25 11:30:47.255: INFO: Successfully updated pod "annotationupdated6d5f51f-dcdb-4c43-83d2-7bebe4e55f23"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:51.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8262" for this suite.

• [SLOW TEST:6.740 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4080,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:30:51.392: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:30:58.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3548" for this suite.

• [SLOW TEST:7.337 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":238,"skipped":4092,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:30:58.673: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:30:58.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3" in namespace "projected-1703" to be "Succeeded or Failed"
May 25 11:30:58.795: INFO: Pod "downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.373997ms
May 25 11:31:00.812: INFO: Pod "downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034634102s
STEP: Saw pod success
May 25 11:31:00.812: INFO: Pod "downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3" satisfied condition "Succeeded or Failed"
May 25 11:31:00.818: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3 container client-container: <nil>
STEP: delete the pod
May 25 11:31:00.854: INFO: Waiting for pod downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3 to disappear
May 25 11:31:00.865: INFO: Pod downwardapi-volume-d6192de6-59d0-45d0-8eab-d2a90a92f6c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:31:00.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1703" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4101,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:31:00.882: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:31:00.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69" in namespace "downward-api-9437" to be "Succeeded or Failed"
May 25 11:31:00.967: INFO: Pod "downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.649668ms
May 25 11:31:03.044: INFO: Pod "downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.086949113s
STEP: Saw pod success
May 25 11:31:03.044: INFO: Pod "downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69" satisfied condition "Succeeded or Failed"
May 25 11:31:03.052: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69 container client-container: <nil>
STEP: delete the pod
May 25 11:31:03.087: INFO: Waiting for pod downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69 to disappear
May 25 11:31:03.117: INFO: Pod downwardapi-volume-153ae67e-41f5-4fe9-bc50-0e7d1de03c69 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:31:03.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9437" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4104,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:31:03.136: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 25 11:31:07.305: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 25 11:31:07.311: INFO: Pod pod-with-poststart-exec-hook still exists
May 25 11:31:09.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 25 11:31:09.325: INFO: Pod pod-with-poststart-exec-hook still exists
May 25 11:31:11.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 25 11:31:11.324: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:31:11.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1500" for this suite.

• [SLOW TEST:8.210 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":241,"skipped":4117,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:31:11.350: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:31:13.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8928" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:31:13.520: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 11:31:14.244: INFO: starting watch
STEP: patching
STEP: updating
May 25 11:31:14.266: INFO: waiting for watch events with expected annotations
May 25 11:31:14.266: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:31:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9467" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":243,"skipped":4145,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:31:14.391: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3215
STEP: creating service affinity-clusterip-transition in namespace services-3215
STEP: creating replication controller affinity-clusterip-transition in namespace services-3215
I0525 11:31:14.522150      19 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-3215, replica count: 3
I0525 11:31:17.572824      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:31:17.597: INFO: Creating new exec pod
May 25 11:31:20.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-3215 exec execpod-affinitymqfxr -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 25 11:31:20.846: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 25 11:31:20.846: INFO: stdout: ""
May 25 11:31:20.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-3215 exec execpod-affinitymqfxr -- /bin/sh -x -c nc -zv -t -w 2 10.68.15.247 80'
May 25 11:31:21.067: INFO: stderr: "+ nc -zv -t -w 2 10.68.15.247 80\nConnection to 10.68.15.247 80 port [tcp/http] succeeded!\n"
May 25 11:31:21.067: INFO: stdout: ""
May 25 11:31:21.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-3215 exec execpod-affinitymqfxr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.15.247:80/ ; done'
May 25 11:31:21.379: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n"
May 25 11:31:21.379: INFO: stdout: "\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-8mvqs"
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.379: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:21.380: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-3215 exec execpod-affinitymqfxr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.15.247:80/ ; done'
May 25 11:31:51.697: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n"
May 25 11:31:51.697: INFO: stdout: "\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-kwd85\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-kwd85\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-kwd85\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-8mvqs\naffinity-clusterip-transition-kwd85\naffinity-clusterip-transition-8mvqs"
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-kwd85
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-kwd85
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-kwd85
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-kwd85
May 25 11:31:51.697: INFO: Received response from host: affinity-clusterip-transition-8mvqs
May 25 11:31:51.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-3215 exec execpod-affinitymqfxr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.15.247:80/ ; done'
May 25 11:31:52.063: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.15.247:80/\n"
May 25 11:31:52.063: INFO: stdout: "\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx\naffinity-clusterip-transition-5ddmx"
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Received response from host: affinity-clusterip-transition-5ddmx
May 25 11:31:52.063: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3215, will wait for the garbage collector to delete the pods
May 25 11:31:52.152: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.997931ms
May 25 11:31:52.852: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 700.404482ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:02.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3215" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:47.742 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":244,"skipped":4148,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:02.136: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:32:02.219: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a1663e97-a4fc-4321-9922-49ede190a9b8" in namespace "security-context-test-7584" to be "Succeeded or Failed"
May 25 11:32:02.226: INFO: Pod "busybox-privileged-false-a1663e97-a4fc-4321-9922-49ede190a9b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.734062ms
May 25 11:32:04.236: INFO: Pod "busybox-privileged-false-a1663e97-a4fc-4321-9922-49ede190a9b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016759647s
May 25 11:32:04.236: INFO: Pod "busybox-privileged-false-a1663e97-a4fc-4321-9922-49ede190a9b8" satisfied condition "Succeeded or Failed"
May 25 11:32:04.253: INFO: Got logs for pod "busybox-privileged-false-a1663e97-a4fc-4321-9922-49ede190a9b8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:04.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7584" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4168,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:04.273: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7188
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7188
I0525 11:32:04.457078      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7188, replica count: 2
I0525 11:32:07.507598      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:32:10.507: INFO: Creating new exec pod
I0525 11:32:10.507872      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:32:13.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-7188 exec execpodx29zq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 25 11:32:13.848: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 11:32:13.848: INFO: stdout: ""
May 25 11:32:13.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-7188 exec execpodx29zq -- /bin/sh -x -c nc -zv -t -w 2 10.68.10.59 80'
May 25 11:32:14.088: INFO: stderr: "+ nc -zv -t -w 2 10.68.10.59 80\nConnection to 10.68.10.59 80 port [tcp/http] succeeded!\n"
May 25 11:32:14.089: INFO: stdout: ""
May 25 11:32:14.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-7188 exec execpodx29zq -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.15 31396'
May 25 11:32:14.326: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.15 31396\nConnection to 10.156.0.15 31396 port [tcp/31396] succeeded!\n"
May 25 11:32:14.326: INFO: stdout: ""
May 25 11:32:14.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-7188 exec execpodx29zq -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.14 31396'
May 25 11:32:14.538: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.14 31396\nConnection to 10.156.0.14 31396 port [tcp/31396] succeeded!\n"
May 25 11:32:14.538: INFO: stdout: ""
May 25 11:32:14.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-7188 exec execpodx29zq -- /bin/sh -x -c nc -zv -t -w 2 35.198.151.65 31396'
May 25 11:32:14.739: INFO: stderr: "+ nc -zv -t -w 2 35.198.151.65 31396\nConnection to 35.198.151.65 31396 port [tcp/31396] succeeded!\n"
May 25 11:32:14.739: INFO: stdout: ""
May 25 11:32:14.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-7188 exec execpodx29zq -- /bin/sh -x -c nc -zv -t -w 2 34.107.45.63 31396'
May 25 11:32:14.951: INFO: stderr: "+ nc -zv -t -w 2 34.107.45.63 31396\nConnection to 34.107.45.63 31396 port [tcp/31396] succeeded!\n"
May 25 11:32:14.951: INFO: stdout: ""
May 25 11:32:14.951: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:15.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7188" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.772 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":246,"skipped":4172,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:15.046: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 25 11:32:15.147: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 11:32:15.165: INFO: Waiting for terminating namespaces to be deleted...
May 25 11:32:15.172: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-p6wx before test
May 25 11:32:15.183: INFO: sonobuoy-e2e-job-7c943d3357464792 from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:32:15.183: INFO: 	Container e2e ready: true, restart count 0
May 25 11:32:15.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 11:32:15.183: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:32:15.183: INFO: 	Container sonobuoy-worker ready: false, restart count 5
May 25 11:32:15.183: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:32:15.183: INFO: externalname-service-jp7fl from services-7188 started at 2021-05-25 11:32:04 +0000 UTC (1 container statuses recorded)
May 25 11:32:15.183: INFO: 	Container externalname-service ready: true, restart count 0
May 25 11:32:15.183: INFO: externalname-service-h2dmr from services-7188 started at 2021-05-25 11:32:04 +0000 UTC (1 container statuses recorded)
May 25 11:32:15.183: INFO: 	Container externalname-service ready: true, restart count 0
May 25 11:32:15.183: INFO: sonobuoy from sonobuoy started at 2021-05-25 10:27:41 +0000 UTC (1 container statuses recorded)
May 25 11:32:15.183: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 11:32:15.183: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-wpkp before test
May 25 11:32:15.192: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-8lgrl from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:32:15.192: INFO: 	Container sonobuoy-worker ready: false, restart count 5
May 25 11:32:15.192: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 11:32:15.192: INFO: execpodx29zq from services-7188 started at 2021-05-25 11:32:10 +0000 UTC (1 container statuses recorded)
May 25 11:32:15.192: INFO: 	Container agnhost-container ready: true, restart count 0
May 25 11:32:15.192: INFO: 
Logging pods the apiserver thinks is on node gke-cluster-1-default-pool-8f0bb8bb-vl79 before test
May 25 11:32:15.202: INFO: coredns-854c77959c-q5878 from kube-system started at 2021-05-19 12:01:03 +0000 UTC (1 container statuses recorded)
May 25 11:32:15.202: INFO: 	Container coredns ready: true, restart count 0
May 25 11:32:15.202: INFO: sonobuoy-systemd-logs-daemon-set-227340fcea274850-czfnh from sonobuoy started at 2021-05-25 10:27:42 +0000 UTC (2 container statuses recorded)
May 25 11:32:15.202: INFO: 	Container sonobuoy-worker ready: false, restart count 5
May 25 11:32:15.202: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node gke-cluster-1-default-pool-8f0bb8bb-p6wx
STEP: verifying the node has the label node gke-cluster-1-default-pool-8f0bb8bb-wpkp
STEP: verifying the node has the label node gke-cluster-1-default-pool-8f0bb8bb-vl79
May 25 11:32:16.024: INFO: Pod coredns-854c77959c-q5878 requesting resource cpu=100m on Node gke-cluster-1-default-pool-8f0bb8bb-vl79
May 25 11:32:16.024: INFO: Pod sonobuoy-e2e-job-7c943d3357464792 requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-p6wx
May 25 11:32:16.024: INFO: Pod sonobuoy-systemd-logs-daemon-set-227340fcea274850-czfnh requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-vl79
May 25 11:32:16.024: INFO: Pod sonobuoy-systemd-logs-daemon-set-227340fcea274850-h6dhc requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-p6wx
May 25 11:32:16.024: INFO: Pod sonobuoy-systemd-logs-daemon-set-227340fcea274850-8lgrl requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-wpkp
May 25 11:32:16.024: INFO: Pod externalname-service-jp7fl requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-p6wx
May 25 11:32:16.024: INFO: Pod externalname-service-h2dmr requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-p6wx
May 25 11:32:16.024: INFO: Pod sonobuoy requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-p6wx
May 25 11:32:16.024: INFO: Pod execpodx29zq requesting resource cpu=0m on Node gke-cluster-1-default-pool-8f0bb8bb-wpkp
STEP: Starting Pods to consume most of the cluster CPU.
May 25 11:32:16.024: INFO: Creating a pod which consumes cpu=174m on Node gke-cluster-1-default-pool-8f0bb8bb-vl79
May 25 11:32:16.039: INFO: Creating a pod which consumes cpu=515m on Node gke-cluster-1-default-pool-8f0bb8bb-p6wx
May 25 11:32:16.048: INFO: Creating a pod which consumes cpu=286m on Node gke-cluster-1-default-pool-8f0bb8bb-wpkp
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e.16824bf6135eb629], Reason = [Scheduled], Message = [Successfully assigned sched-pred-74/filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e to gke-cluster-1-default-pool-8f0bb8bb-vl79]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a.16824bf616229686], Reason = [Scheduled], Message = [Successfully assigned sched-pred-74/filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a to gke-cluster-1-default-pool-8f0bb8bb-p6wx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8.16824bf617c33e76], Reason = [Scheduled], Message = [Successfully assigned sched-pred-74/filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8 to gke-cluster-1-default-pool-8f0bb8bb-wpkp]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a.16824bf650049199], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a.16824bf65347685f], Reason = [Created], Message = [Created container filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a.16824bf6583916f7], Reason = [Started], Message = [Started container filler-pod-69b7e7c6-13d0-4c36-858d-988d16d3a96a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8.16824bf669f6426d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8.16824bf66e499bfa], Reason = [Created], Message = [Created container filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8.16824bf67600442b], Reason = [Started], Message = [Started container filler-pod-d5014f76-cc1c-4c6d-864b-4d313cd880a8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e.16824bf6a6eef631], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e.16824bf6ab2b7f00], Reason = [Created], Message = [Created container filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e.16824bf6b4594754], Reason = [Started], Message = [Started container filler-pod-f99f4983-89a7-41a5-9347-24a07390c43e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16824bf7074d52b5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node gke-cluster-1-default-pool-8f0bb8bb-p6wx
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node gke-cluster-1-default-pool-8f0bb8bb-wpkp
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node gke-cluster-1-default-pool-8f0bb8bb-vl79
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:22.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-74" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:7.135 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":247,"skipped":4184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:22.185: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-3bb60277-8bac-4a49-a2f2-ca03cf8d6fec
STEP: Creating secret with name s-test-opt-upd-aba18298-6e06-4fc2-bb32-1e1f5c8ca0cc
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3bb60277-8bac-4a49-a2f2-ca03cf8d6fec
STEP: Updating secret s-test-opt-upd-aba18298-6e06-4fc2-bb32-1e1f5c8ca0cc
STEP: Creating secret with name s-test-opt-create-a5af0f32-eff5-4c8a-a2de-c4fa00be52f6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:26.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9655" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4209,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:26.685: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:32:26.745: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 25 11:32:30.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-8516 --namespace=crd-publish-openapi-8516 create -f -'
May 25 11:32:31.289: INFO: stderr: ""
May 25 11:32:31.290: INFO: stdout: "e2e-test-crd-publish-openapi-5423-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 25 11:32:31.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-8516 --namespace=crd-publish-openapi-8516 delete e2e-test-crd-publish-openapi-5423-crds test-cr'
May 25 11:32:31.411: INFO: stderr: ""
May 25 11:32:31.411: INFO: stdout: "e2e-test-crd-publish-openapi-5423-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 25 11:32:31.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-8516 --namespace=crd-publish-openapi-8516 apply -f -'
May 25 11:32:31.776: INFO: stderr: ""
May 25 11:32:31.776: INFO: stdout: "e2e-test-crd-publish-openapi-5423-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 25 11:32:31.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-8516 --namespace=crd-publish-openapi-8516 delete e2e-test-crd-publish-openapi-5423-crds test-cr'
May 25 11:32:31.919: INFO: stderr: ""
May 25 11:32:31.920: INFO: stdout: "e2e-test-crd-publish-openapi-5423-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 25 11:32:31.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-8516 explain e2e-test-crd-publish-openapi-5423-crds'
May 25 11:32:32.269: INFO: stderr: ""
May 25 11:32:32.269: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5423-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:34.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8516" for this suite.

• [SLOW TEST:7.771 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":249,"skipped":4210,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:34.457: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-056a4290-691b-47cf-8222-3006e118e67b
STEP: Creating configMap with name cm-test-opt-upd-c76039ac-b26a-44ee-a21f-f9c50f653339
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-056a4290-691b-47cf-8222-3006e118e67b
STEP: Updating configmap cm-test-opt-upd-c76039ac-b26a-44ee-a21f-f9c50f653339
STEP: Creating configMap with name cm-test-opt-create-be5680f1-24f3-42bf-9452-993855282f9a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:38.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4915" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4230,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:38.732: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 25 11:32:38.793: INFO: namespace kubectl-1008
May 25 11:32:38.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1008 create -f -'
May 25 11:32:39.156: INFO: stderr: ""
May 25 11:32:39.156: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 25 11:32:40.166: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:32:40.166: INFO: Found 0 / 1
May 25 11:32:41.172: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:32:41.172: INFO: Found 1 / 1
May 25 11:32:41.172: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 25 11:32:41.182: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:32:41.182: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 25 11:32:41.182: INFO: wait on agnhost-primary startup in kubectl-1008 
May 25 11:32:41.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1008 logs agnhost-primary-psjkv agnhost-primary'
May 25 11:32:41.318: INFO: stderr: ""
May 25 11:32:41.318: INFO: stdout: "Paused\n"
STEP: exposing RC
May 25 11:32:41.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1008 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 25 11:32:41.501: INFO: stderr: ""
May 25 11:32:41.501: INFO: stdout: "service/rm2 exposed\n"
May 25 11:32:41.527: INFO: Service rm2 in namespace kubectl-1008 found.
STEP: exposing service
May 25 11:32:43.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-1008 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 25 11:32:43.766: INFO: stderr: ""
May 25 11:32:43.766: INFO: stdout: "service/rm3 exposed\n"
May 25 11:32:43.789: INFO: Service rm3 in namespace kubectl-1008 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:45.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1008" for this suite.

• [SLOW TEST:7.095 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":251,"skipped":4232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-7347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7347 to expose endpoints map[]
May 25 11:32:45.935: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 25 11:32:46.961: INFO: successfully validated that service endpoint-test2 in namespace services-7347 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7347 to expose endpoints map[pod1:[80]]
May 25 11:32:49.022: INFO: successfully validated that service endpoint-test2 in namespace services-7347 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7347 to expose endpoints map[pod1:[80] pod2:[80]]
May 25 11:32:51.158: INFO: successfully validated that service endpoint-test2 in namespace services-7347 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7347 to expose endpoints map[pod2:[80]]
May 25 11:32:51.361: INFO: successfully validated that service endpoint-test2 in namespace services-7347 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7347 to expose endpoints map[]
May 25 11:32:51.415: INFO: successfully validated that service endpoint-test2 in namespace services-7347 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:32:51.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7347" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.646 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":252,"skipped":4266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:32:51.481: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 25 11:32:51.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8380 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 25 11:32:51.746: INFO: stderr: ""
May 25 11:32:51.746: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 25 11:32:56.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8380 get pod e2e-test-httpd-pod -o json'
May 25 11:32:56.908: INFO: stderr: ""
May 25 11:32:56.908: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-05-25T11:32:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-25T11:32:51Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.64.0.253\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"main\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-25T11:32:54Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8380\",\n        \"resourceVersion\": \"101552\",\n        \"uid\": \"f4da9c0b-f7b8-4b21-a092-e5ab756bce81\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-nxl2d\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gke-cluster-1-default-pool-8f0bb8bb-p6wx\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-nxl2d\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-nxl2d\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T11:32:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T11:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T11:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T11:32:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d9f57d028b92fe1e3fa092dd3484f343a2873239ff628de8a855d6e81c2a3309\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-25T11:32:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.156.0.14\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.64.0.253\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.64.0.253\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-25T11:32:51Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 25 11:32:56.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8380 replace -f -'
May 25 11:32:57.216: INFO: stderr: ""
May 25 11:32:57.216: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
May 25 11:32:57.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-8380 delete pods e2e-test-httpd-pod'
May 25 11:33:01.991: INFO: stderr: ""
May 25 11:33:01.991: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8380" for this suite.

• [SLOW TEST:10.542 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":253,"skipped":4293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:02.021: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:10.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9985" for this suite.

• [SLOW TEST:8.104 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":254,"skipped":4316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:10.132: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-3441cd67-a362-44f8-80c0-3268cf283a99
STEP: Creating a pod to test consume secrets
May 25 11:33:10.217: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84" in namespace "projected-6465" to be "Succeeded or Failed"
May 25 11:33:10.225: INFO: Pod "pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84": Phase="Pending", Reason="", readiness=false. Elapsed: 8.190221ms
May 25 11:33:12.251: INFO: Pod "pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034213856s
STEP: Saw pod success
May 25 11:33:12.251: INFO: Pod "pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84" satisfied condition "Succeeded or Failed"
May 25 11:33:12.257: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84 container secret-volume-test: <nil>
STEP: delete the pod
May 25 11:33:12.330: INFO: Waiting for pod pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84 to disappear
May 25 11:33:12.341: INFO: Pod pod-projected-secrets-ebe178b3-eac8-4d93-b5fe-010fd000bf84 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:12.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6465" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:12.379: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 25 11:33:12.440: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:33:16.084: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:30.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5703" for this suite.

• [SLOW TEST:17.844 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":256,"skipped":4431,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:30.226: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 25 11:33:34.835: INFO: Successfully updated pod "adopt-release-5ndj9"
STEP: Checking that the Job readopts the Pod
May 25 11:33:34.835: INFO: Waiting up to 15m0s for pod "adopt-release-5ndj9" in namespace "job-3883" to be "adopted"
May 25 11:33:34.846: INFO: Pod "adopt-release-5ndj9": Phase="Running", Reason="", readiness=true. Elapsed: 11.480731ms
May 25 11:33:36.861: INFO: Pod "adopt-release-5ndj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.026907892s
May 25 11:33:36.862: INFO: Pod "adopt-release-5ndj9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 25 11:33:37.387: INFO: Successfully updated pod "adopt-release-5ndj9"
STEP: Checking that the Job releases the Pod
May 25 11:33:37.388: INFO: Waiting up to 15m0s for pod "adopt-release-5ndj9" in namespace "job-3883" to be "released"
May 25 11:33:37.403: INFO: Pod "adopt-release-5ndj9": Phase="Running", Reason="", readiness=true. Elapsed: 15.577261ms
May 25 11:33:37.404: INFO: Pod "adopt-release-5ndj9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:37.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3883" for this suite.

• [SLOW TEST:7.231 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":257,"skipped":4438,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:37.458: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:33:37.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc" in namespace "projected-6178" to be "Succeeded or Failed"
May 25 11:33:37.580: INFO: Pod "downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286174ms
May 25 11:33:39.596: INFO: Pod "downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023218148s
STEP: Saw pod success
May 25 11:33:39.596: INFO: Pod "downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc" satisfied condition "Succeeded or Failed"
May 25 11:33:39.601: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc container client-container: <nil>
STEP: delete the pod
May 25 11:33:39.635: INFO: Waiting for pod downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc to disappear
May 25 11:33:39.644: INFO: Pod downwardapi-volume-77f64882-b6bd-40f0-aa7a-09b4179d26cc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:39.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6178" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4440,"failed":0}

------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:39.665: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
May 25 11:33:39.731: INFO: Waiting up to 5m0s for pod "client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7" in namespace "containers-8073" to be "Succeeded or Failed"
May 25 11:33:39.736: INFO: Pod "client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.695164ms
May 25 11:33:41.753: INFO: Pod "client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022065667s
STEP: Saw pod success
May 25 11:33:41.753: INFO: Pod "client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7" satisfied condition "Succeeded or Failed"
May 25 11:33:41.759: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:33:41.787: INFO: Waiting for pod client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7 to disappear
May 25 11:33:41.796: INFO: Pod client-containers-b8248e10-053d-43c9-a0fb-d55bade478d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:41.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8073" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:41.820: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-580d398e-0560-4d87-9bee-cffa94b4ed93
STEP: Creating a pod to test consume configMaps
May 25 11:33:41.908: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445" in namespace "projected-4016" to be "Succeeded or Failed"
May 25 11:33:41.919: INFO: Pod "pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445": Phase="Pending", Reason="", readiness=false. Elapsed: 10.757754ms
May 25 11:33:43.932: INFO: Pod "pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02401245s
STEP: Saw pod success
May 25 11:33:43.932: INFO: Pod "pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445" satisfied condition "Succeeded or Failed"
May 25 11:33:43.938: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:33:43.978: INFO: Waiting for pod pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445 to disappear
May 25 11:33:43.987: INFO: Pod pod-projected-configmaps-267cd901-b3a8-4808-803d-72a5570cf445 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:33:43.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4016" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:33:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5620
STEP: creating service affinity-nodeport-transition in namespace services-5620
STEP: creating replication controller affinity-nodeport-transition in namespace services-5620
I0525 11:33:44.206621      19 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5620, replica count: 3
I0525 11:33:47.259547      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:33:47.288: INFO: Creating new exec pod
May 25 11:33:50.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 25 11:33:51.780: INFO: rc: 1
May 25 11:33:51.780: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
May 25 11:33:52.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 25 11:33:54.023: INFO: rc: 1
May 25 11:33:54.023: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
May 25 11:33:54.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 25 11:33:55.015: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 25 11:33:55.015: INFO: stdout: ""
May 25 11:33:55.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 10.68.11.222 80'
May 25 11:33:55.223: INFO: stderr: "+ nc -zv -t -w 2 10.68.11.222 80\nConnection to 10.68.11.222 80 port [tcp/http] succeeded!\n"
May 25 11:33:55.223: INFO: stdout: ""
May 25 11:33:55.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.14 31465'
May 25 11:33:55.471: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.14 31465\nConnection to 10.156.0.14 31465 port [tcp/31465] succeeded!\n"
May 25 11:33:55.471: INFO: stdout: ""
May 25 11:33:55.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.16 31465'
May 25 11:33:55.692: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.16 31465\nConnection to 10.156.0.16 31465 port [tcp/31465] succeeded!\n"
May 25 11:33:55.692: INFO: stdout: ""
May 25 11:33:55.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 34.107.45.63 31465'
May 25 11:33:55.931: INFO: stderr: "+ nc -zv -t -w 2 34.107.45.63 31465\nConnection to 34.107.45.63 31465 port [tcp/31465] succeeded!\n"
May 25 11:33:55.931: INFO: stdout: ""
May 25 11:33:55.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c nc -zv -t -w 2 35.246.205.6 31465'
May 25 11:33:56.189: INFO: stderr: "+ nc -zv -t -w 2 35.246.205.6 31465\nConnection to 35.246.205.6 31465 port [tcp/31465] succeeded!\n"
May 25 11:33:56.189: INFO: stdout: ""
May 25 11:33:56.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.156.0.16:31465/ ; done'
May 25 11:33:56.708: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n"
May 25 11:33:56.708: INFO: stdout: "\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn"
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:33:56.708: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:26.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.156.0.16:31465/ ; done'
May 25 11:34:27.081: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n"
May 25 11:34:27.081: INFO: stdout: "\naffinity-nodeport-transition-tbwgr\naffinity-nodeport-transition-5nwcj\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-tbwgr\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-tbwgr\naffinity-nodeport-transition-5nwcj\naffinity-nodeport-transition-tbwgr\naffinity-nodeport-transition-tbwgr\naffinity-nodeport-transition-5nwcj\naffinity-nodeport-transition-5nwcj\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-5nwcj\naffinity-nodeport-transition-jp7zn"
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-tbwgr
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-5nwcj
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-tbwgr
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-tbwgr
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-5nwcj
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-tbwgr
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-tbwgr
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-5nwcj
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-5nwcj
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-5nwcj
May 25 11:34:27.081: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-5620 exec execpod-affinitycfrhr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.156.0.16:31465/ ; done'
May 25 11:34:27.654: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.156.0.16:31465/\n"
May 25 11:34:27.654: INFO: stdout: "\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn\naffinity-nodeport-transition-jp7zn"
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Received response from host: affinity-nodeport-transition-jp7zn
May 25 11:34:27.654: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5620, will wait for the garbage collector to delete the pods
May 25 11:34:27.794: INFO: Deleting ReplicationController affinity-nodeport-transition took: 30.360632ms
May 25 11:34:28.594: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 800.224551ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:32.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5620" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:48.741 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":261,"skipped":4550,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:32.750: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 11:34:34.863: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:34.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2790" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4560,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:34.910: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:35.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1848" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":263,"skipped":4565,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:35.031: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:35.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4383" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":264,"skipped":4575,"failed":0}
SSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:35.151: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:34:35.215: INFO: Waiting up to 5m0s for pod "busybox-user-65534-94233f30-13e1-4e54-a12b-601d75b52299" in namespace "security-context-test-3317" to be "Succeeded or Failed"
May 25 11:34:35.223: INFO: Pod "busybox-user-65534-94233f30-13e1-4e54-a12b-601d75b52299": Phase="Pending", Reason="", readiness=false. Elapsed: 7.863926ms
May 25 11:34:37.238: INFO: Pod "busybox-user-65534-94233f30-13e1-4e54-a12b-601d75b52299": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02283978s
May 25 11:34:37.238: INFO: Pod "busybox-user-65534-94233f30-13e1-4e54-a12b-601d75b52299" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:37.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3317" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4581,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:37.257: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:34:37.309: INFO: Creating deployment "test-recreate-deployment"
May 25 11:34:37.318: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 25 11:34:37.338: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 25 11:34:39.363: INFO: Waiting deployment "test-recreate-deployment" to complete
May 25 11:34:39.381: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 25 11:34:39.399: INFO: Updating deployment test-recreate-deployment
May 25 11:34:39.399: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 25 11:34:39.547: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1056  3a090c3f-bda8-46b7-b52c-2718ec2cad7b 102145 2 2021-05-25 11:34:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-25 11:34:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {k3s Update apps/v1 2021-05-25 11:34:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8cb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-25 11:34:39 +0000 UTC,LastTransitionTime:2021-05-25 11:34:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-05-25 11:34:39 +0000 UTC,LastTransitionTime:2021-05-25 11:34:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 25 11:34:39.555: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-1056  39cd376f-7887-4043-afb6-ed325974c9c0 102142 1 2021-05-25 11:34:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3a090c3f-bda8-46b7-b52c-2718ec2cad7b 0xc004adf7a0 0xc004adf7a1}] []  [{k3s Update apps/v1 2021-05-25 11:34:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a090c3f-bda8-46b7-b52c-2718ec2cad7b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004adf848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 11:34:39.555: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 25 11:34:39.556: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-1056  798d82c0-6e97-443c-a2cb-ea42bae11c6b 102135 2 2021-05-25 11:34:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3a090c3f-bda8-46b7-b52c-2718ec2cad7b 0xc004adf5e7 0xc004adf5e8}] []  [{k3s Update apps/v1 2021-05-25 11:34:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a090c3f-bda8-46b7-b52c-2718ec2cad7b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004adf688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 11:34:39.567: INFO: Pod "test-recreate-deployment-f79dd4667-fkqzr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-fkqzr test-recreate-deployment-f79dd4667- deployment-1056  6bb84d0d-3734-4d95-a231-dfed70c23af0 102147 0 2021-05-25 11:34:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 39cd376f-7887-4043-afb6-ed325974c9c0 0xc004a8cf70 0xc004a8cf71}] []  [{k3s Update v1 2021-05-25 11:34:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39cd376f-7887-4043-afb6-ed325974c9c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 11:34:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jbvhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jbvhx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jbvhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:34:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:34:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:34:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:34:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:,StartTime:2021-05-25 11:34:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:39.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1056" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":266,"skipped":4583,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:39.590: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
May 25 11:34:39.651: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 25 11:34:39.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 create -f -'
May 25 11:34:40.177: INFO: stderr: ""
May 25 11:34:40.177: INFO: stdout: "service/agnhost-replica created\n"
May 25 11:34:40.177: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 25 11:34:40.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 create -f -'
May 25 11:34:40.651: INFO: stderr: ""
May 25 11:34:40.652: INFO: stdout: "service/agnhost-primary created\n"
May 25 11:34:40.652: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 25 11:34:40.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 create -f -'
May 25 11:34:40.955: INFO: stderr: ""
May 25 11:34:40.955: INFO: stdout: "service/frontend created\n"
May 25 11:34:40.955: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 25 11:34:40.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 create -f -'
May 25 11:34:41.218: INFO: stderr: ""
May 25 11:34:41.218: INFO: stdout: "deployment.apps/frontend created\n"
May 25 11:34:41.218: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 25 11:34:41.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 create -f -'
May 25 11:34:41.558: INFO: stderr: ""
May 25 11:34:41.558: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 25 11:34:41.558: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 25 11:34:41.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 create -f -'
May 25 11:34:42.689: INFO: stderr: ""
May 25 11:34:42.689: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 25 11:34:42.689: INFO: Waiting for all frontend pods to be Running.
May 25 11:34:47.740: INFO: Waiting for frontend to serve content.
May 25 11:34:52.760: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
May 25 11:34:57.777: INFO: Trying to add a new entry to the guestbook.
May 25 11:34:57.801: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 25 11:34:57.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 delete --grace-period=0 --force -f -'
May 25 11:34:58.024: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:34:58.024: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 25 11:34:58.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 delete --grace-period=0 --force -f -'
May 25 11:34:58.170: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:34:58.170: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 25 11:34:58.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 delete --grace-period=0 --force -f -'
May 25 11:34:58.309: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:34:58.309: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 25 11:34:58.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 delete --grace-period=0 --force -f -'
May 25 11:34:58.431: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:34:58.431: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 25 11:34:58.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 delete --grace-period=0 --force -f -'
May 25 11:34:58.558: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:34:58.558: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 25 11:34:58.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-7268 delete --grace-period=0 --force -f -'
May 25 11:34:58.760: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 11:34:58.760: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:34:58.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7268" for this suite.

• [SLOW TEST:19.198 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":267,"skipped":4599,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:34:58.788: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:35:04.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9362" for this suite.

• [SLOW TEST:5.402 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":268,"skipped":4601,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:35:04.191: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 25 11:35:04.271: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:35:09.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8379" for this suite.

• [SLOW TEST:5.572 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":269,"skipped":4612,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:35:09.763: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:35:09.818: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:35:10.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4636" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":270,"skipped":4618,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:35:10.890: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
May 25 11:35:11.014: INFO: Waiting up to 5m0s for pod "client-containers-993d9553-8f86-47e5-bb04-9074bb042204" in namespace "containers-7904" to be "Succeeded or Failed"
May 25 11:35:11.023: INFO: Pod "client-containers-993d9553-8f86-47e5-bb04-9074bb042204": Phase="Pending", Reason="", readiness=false. Elapsed: 8.192675ms
May 25 11:35:13.044: INFO: Pod "client-containers-993d9553-8f86-47e5-bb04-9074bb042204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029445356s
STEP: Saw pod success
May 25 11:35:13.044: INFO: Pod "client-containers-993d9553-8f86-47e5-bb04-9074bb042204" satisfied condition "Succeeded or Failed"
May 25 11:35:13.050: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod client-containers-993d9553-8f86-47e5-bb04-9074bb042204 container agnhost-container: <nil>
STEP: delete the pod
May 25 11:35:13.102: INFO: Waiting for pod client-containers-993d9553-8f86-47e5-bb04-9074bb042204 to disappear
May 25 11:35:13.112: INFO: Pod client-containers-993d9553-8f86-47e5-bb04-9074bb042204 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:35:13.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7904" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:35:13.134: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:35:13.237: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 25 11:35:18.255: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 25 11:35:18.255: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 25 11:35:20.273: INFO: Creating deployment "test-rollover-deployment"
May 25 11:35:20.292: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 25 11:35:22.318: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 25 11:35:22.329: INFO: Ensure that both replica sets have 1 created replica
May 25 11:35:22.340: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 25 11:35:22.359: INFO: Updating deployment test-rollover-deployment
May 25 11:35:22.359: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 25 11:35:24.383: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 25 11:35:24.396: INFO: Make sure deployment "test-rollover-deployment" is complete
May 25 11:35:24.409: INFO: all replica sets need to contain the pod-template-hash label
May 25 11:35:24.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539323, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 11:35:26.429: INFO: all replica sets need to contain the pod-template-hash label
May 25 11:35:26.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539323, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 11:35:28.485: INFO: all replica sets need to contain the pod-template-hash label
May 25 11:35:28.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539323, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 11:35:30.444: INFO: all replica sets need to contain the pod-template-hash label
May 25 11:35:30.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539323, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 11:35:32.427: INFO: all replica sets need to contain the pod-template-hash label
May 25 11:35:32.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539323, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539320, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 11:35:34.432: INFO: 
May 25 11:35:34.432: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 25 11:35:34.449: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-339  c83343d5-d13c-446f-aa2e-647847c6177b 102698 2 2021-05-25 11:35:20 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-25 11:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {k3s Update apps/v1 2021-05-25 11:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a156d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-25 11:35:20 +0000 UTC,LastTransitionTime:2021-05-25 11:35:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-05-25 11:35:33 +0000 UTC,LastTransitionTime:2021-05-25 11:35:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 25 11:35:34.456: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-339  607d7d5a-03be-4fed-b20b-2b1c465e4770 102689 2 2021-05-25 11:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c83343d5-d13c-446f-aa2e-647847c6177b 0xc007318d07 0xc007318d08}] []  [{k3s Update apps/v1 2021-05-25 11:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83343d5-d13c-446f-aa2e-647847c6177b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007318d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 25 11:35:34.457: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 25 11:35:34.457: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-339  a910dc64-be6a-406e-b117-333454206617 102697 2 2021-05-25 11:35:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c83343d5-d13c-446f-aa2e-647847c6177b 0xc007318df7 0xc007318df8}] []  [{e2e.test Update apps/v1 2021-05-25 11:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {k3s Update apps/v1 2021-05-25 11:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83343d5-d13c-446f-aa2e-647847c6177b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007318e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 11:35:34.457: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-339  a5c26f01-873c-48cd-af09-9eba617b4710 102676 2 2021-05-25 11:35:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c83343d5-d13c-446f-aa2e-647847c6177b 0xc007318c07 0xc007318c08}] []  [{k3s Update apps/v1 2021-05-25 11:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83343d5-d13c-446f-aa2e-647847c6177b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007318c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 11:35:34.465: INFO: Pod "test-rollover-deployment-668db69979-klsxx" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-klsxx test-rollover-deployment-668db69979- deployment-339  0904dbec-e93b-43c6-95f2-632e08278441 102685 0 2021-05-25 11:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 607d7d5a-03be-4fed-b20b-2b1c465e4770 0xc002a15a37 0xc002a15a38}] []  [{k3s Update v1 2021-05-25 11:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"607d7d5a-03be-4fed-b20b-2b1c465e4770\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {main Update v1 2021-05-25 11:35:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.0.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmhhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmhhw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmhhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-cluster-1-default-pool-8f0bb8bb-p6wx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:35:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:35:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:35:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 11:35:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.156.0.14,PodIP:10.64.0.24,StartTime:2021-05-25 11:35:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 11:35:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://a6ac473da56f0ccebfbe7dc44693a84b2e524a2ceb32439ab23a2cb722453ab3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:35:34.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-339" for this suite.

• [SLOW TEST:21.353 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":272,"skipped":4678,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:35:34.488: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:35:34.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3882" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":273,"skipped":4690,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:35:34.583: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5579
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 25 11:35:34.724: INFO: Found 0 stateful pods, waiting for 3
May 25 11:35:44.733: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 11:35:44.733: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 11:35:44.734: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 25 11:35:44.779: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 25 11:35:54.860: INFO: Updating stateful set ss2
May 25 11:35:54.882: INFO: Waiting for Pod statefulset-5579/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 25 11:36:05.013: INFO: Found 1 stateful pods, waiting for 3
May 25 11:36:15.055: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 11:36:15.055: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 11:36:15.055: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 25 11:36:15.098: INFO: Updating stateful set ss2
May 25 11:36:15.128: INFO: Waiting for Pod statefulset-5579/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 25 11:36:25.176: INFO: Updating stateful set ss2
May 25 11:36:25.197: INFO: Waiting for StatefulSet statefulset-5579/ss2 to complete update
May 25 11:36:25.198: INFO: Waiting for Pod statefulset-5579/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 25 11:36:35.226: INFO: Deleting all statefulset in ns statefulset-5579
May 25 11:36:35.233: INFO: Scaling statefulset ss2 to 0
May 25 11:36:55.316: INFO: Waiting for statefulset status.replicas updated to 0
May 25 11:36:55.322: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:36:55.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5579" for this suite.

• [SLOW TEST:80.802 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":274,"skipped":4697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:36:55.390: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-5954/configmap-test-f468b674-7e4b-4cc1-a9ef-9fa3f6b9a3e4
STEP: Creating a pod to test consume configMaps
May 25 11:36:55.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331" in namespace "configmap-5954" to be "Succeeded or Failed"
May 25 11:36:55.545: INFO: Pod "pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331": Phase="Pending", Reason="", readiness=false. Elapsed: 11.458407ms
May 25 11:36:57.561: INFO: Pod "pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027518344s
STEP: Saw pod success
May 25 11:36:57.561: INFO: Pod "pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331" satisfied condition "Succeeded or Failed"
May 25 11:36:57.567: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331 container env-test: <nil>
STEP: delete the pod
May 25 11:36:57.616: INFO: Waiting for pod pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331 to disappear
May 25 11:36:57.625: INFO: Pod pod-configmaps-024e7e71-619b-4489-8a4f-5cc327abe331 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:36:57.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5954" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4725,"failed":0}

------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:36:57.645: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 25 11:37:00.293: INFO: Successfully updated pod "pod-update-e0396300-85d8-4ed7-b042-de6d1646a2f2"
STEP: verifying the updated pod is in kubernetes
May 25 11:37:00.305: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:37:00.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8670" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:37:00.325: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:37:00.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7531" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":277,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:37:00.601: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:37:00.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7043" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":278,"skipped":4783,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:37:00.823: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-2831
STEP: creating replication controller nodeport-test in namespace services-2831
I0525 11:37:00.972931      19 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2831, replica count: 2
I0525 11:37:04.023321      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:37:04.023: INFO: Creating new exec pod
May 25 11:37:07.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2831 exec execpodxwq9p -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 25 11:37:07.392: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 11:37:07.392: INFO: stdout: ""
May 25 11:37:07.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2831 exec execpodxwq9p -- /bin/sh -x -c nc -zv -t -w 2 10.68.4.131 80'
May 25 11:37:07.588: INFO: stderr: "+ nc -zv -t -w 2 10.68.4.131 80\nConnection to 10.68.4.131 80 port [tcp/http] succeeded!\n"
May 25 11:37:07.588: INFO: stdout: ""
May 25 11:37:07.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2831 exec execpodxwq9p -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.16 32237'
May 25 11:37:07.795: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.16 32237\nConnection to 10.156.0.16 32237 port [tcp/32237] succeeded!\n"
May 25 11:37:07.795: INFO: stdout: ""
May 25 11:37:07.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2831 exec execpodxwq9p -- /bin/sh -x -c nc -zv -t -w 2 10.156.0.14 32237'
May 25 11:37:08.024: INFO: stderr: "+ nc -zv -t -w 2 10.156.0.14 32237\nConnection to 10.156.0.14 32237 port [tcp/32237] succeeded!\n"
May 25 11:37:08.024: INFO: stdout: ""
May 25 11:37:08.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2831 exec execpodxwq9p -- /bin/sh -x -c nc -zv -t -w 2 35.246.205.6 32237'
May 25 11:37:08.237: INFO: stderr: "+ nc -zv -t -w 2 35.246.205.6 32237\nConnection to 35.246.205.6 32237 port [tcp/32237] succeeded!\n"
May 25 11:37:08.237: INFO: stdout: ""
May 25 11:37:08.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-2831 exec execpodxwq9p -- /bin/sh -x -c nc -zv -t -w 2 34.107.45.63 32237'
May 25 11:37:08.459: INFO: stderr: "+ nc -zv -t -w 2 34.107.45.63 32237\nConnection to 34.107.45.63 32237 port [tcp/32237] succeeded!\n"
May 25 11:37:08.459: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:37:08.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2831" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.657 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":279,"skipped":4789,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:37:08.480: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 25 11:37:08.570: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:37:08.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8806" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":280,"skipped":4798,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:37:08.621: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-3b7edc8c-cc96-4a6d-8c49-60a3080657d1
STEP: Creating a pod to test consume secrets
May 25 11:37:08.704: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038" in namespace "projected-3124" to be "Succeeded or Failed"
May 25 11:37:08.713: INFO: Pod "pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038": Phase="Pending", Reason="", readiness=false. Elapsed: 8.550856ms
May 25 11:37:10.731: INFO: Pod "pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026622119s
STEP: Saw pod success
May 25 11:37:10.731: INFO: Pod "pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038" satisfied condition "Succeeded or Failed"
May 25 11:37:10.738: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 11:37:10.783: INFO: Waiting for pod pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038 to disappear
May 25 11:37:10.793: INFO: Pod pod-projected-secrets-9c5e15d5-8e48-44ee-a5df-502373daa038 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:37:10.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3124" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4816,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:37:10.816: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 11:37:10.921: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 11:38:10.978: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 25 11:38:11.267: INFO: Created pod: pod0-sched-preemption-low-priority
May 25 11:38:11.536: INFO: Created pod: pod1-sched-preemption-medium-priority
May 25 11:38:11.813: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:38:25.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-769" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:75.329 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":282,"skipped":4825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:38:26.145: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 25 11:38:26.208: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 25 11:38:26.286: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 25 11:38:26.286: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 25 11:38:26.376: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 25 11:38:26.376: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 25 11:38:26.391: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 25 11:38:26.391: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 25 11:38:33.580: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:38:33.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3256" for this suite.

• [SLOW TEST:7.479 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":283,"skipped":4848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:38:33.632: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:38:49.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4611" for this suite.

• [SLOW TEST:16.323 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":284,"skipped":4892,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:38:49.956: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-32af0f04-0805-4e93-9c02-b03bd8137294 in namespace container-probe-968
May 25 11:38:52.064: INFO: Started pod test-webserver-32af0f04-0805-4e93-9c02-b03bd8137294 in namespace container-probe-968
STEP: checking the pod's current state and verifying that restartCount is present
May 25 11:38:52.085: INFO: Initial restart count of pod test-webserver-32af0f04-0805-4e93-9c02-b03bd8137294 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:42:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-968" for this suite.

• [SLOW TEST:242.285 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4900,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:42:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:42:53.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 11:42:55.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539773, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539773, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539773, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539773, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:42:58.241: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:42:58.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6809" for this suite.
STEP: Destroying namespace "webhook-6809-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.281 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":286,"skipped":4908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:42:58.527: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 25 11:43:02.729: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 11:43:02.738: INFO: Pod pod-with-poststart-http-hook still exists
May 25 11:43:04.738: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 11:43:04.754: INFO: Pod pod-with-poststart-http-hook still exists
May 25 11:43:06.738: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 11:43:06.751: INFO: Pod pod-with-poststart-http-hook still exists
May 25 11:43:08.738: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 11:43:08.748: INFO: Pod pod-with-poststart-http-hook still exists
May 25 11:43:10.738: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 11:43:10.755: INFO: Pod pod-with-poststart-http-hook still exists
May 25 11:43:12.738: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 11:43:12.753: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:43:12.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8965" for this suite.

• [SLOW TEST:14.265 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4971,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:43:12.797: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 25 11:43:14.946: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7439 PodName:var-expansion-419ffd47-afeb-42fc-a05e-35de6a4a4309 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:43:14.947: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: test for file in mounted path
May 25 11:43:15.060: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7439 PodName:var-expansion-419ffd47-afeb-42fc-a05e-35de6a4a4309 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 11:43:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: updating the annotation value
May 25 11:43:15.673: INFO: Successfully updated pod "var-expansion-419ffd47-afeb-42fc-a05e-35de6a4a4309"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 25 11:43:15.681: INFO: Deleting pod "var-expansion-419ffd47-afeb-42fc-a05e-35de6a4a4309" in namespace "var-expansion-7439"
May 25 11:43:15.690: INFO: Wait up to 5m0s for pod "var-expansion-419ffd47-afeb-42fc-a05e-35de6a4a4309" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:43:53.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7439" for this suite.

• [SLOW TEST:40.944 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":288,"skipped":4975,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:43:53.741: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-dd31275b-e88d-41e2-869b-31c2d389809d
STEP: Creating a pod to test consume secrets
May 25 11:43:53.820: INFO: Waiting up to 5m0s for pod "pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394" in namespace "secrets-3301" to be "Succeeded or Failed"
May 25 11:43:53.828: INFO: Pod "pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394": Phase="Pending", Reason="", readiness=false. Elapsed: 7.786349ms
May 25 11:43:55.836: INFO: Pod "pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015950441s
STEP: Saw pod success
May 25 11:43:55.836: INFO: Pod "pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394" satisfied condition "Succeeded or Failed"
May 25 11:43:55.842: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394 container secret-volume-test: <nil>
STEP: delete the pod
May 25 11:43:55.954: INFO: Waiting for pod pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394 to disappear
May 25 11:43:55.962: INFO: Pod pod-secrets-c4d824fa-0004-41d2-a609-a3002daf2394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:43:55.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3301" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":289,"skipped":4986,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:43:55.983: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3600
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3600
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3600
May 25 11:43:56.101: INFO: Found 0 stateful pods, waiting for 1
May 25 11:44:06.112: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 25 11:44:06.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 11:44:06.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 11:44:06.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 11:44:06.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 11:44:06.609: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 25 11:44:16.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 11:44:16.631: INFO: Waiting for statefulset status.replicas updated to 0
May 25 11:44:16.664: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999521s
May 25 11:44:17.674: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990065495s
May 25 11:44:18.687: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.979437516s
May 25 11:44:19.694: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966997895s
May 25 11:44:20.708: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.960334317s
May 25 11:44:21.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.946144112s
May 25 11:44:22.733: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.935237648s
May 25 11:44:23.755: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.92061473s
May 25 11:44:24.768: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.899095296s
May 25 11:44:25.779: INFO: Verifying statefulset ss doesn't scale past 1 for another 886.014554ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3600
May 25 11:44:26.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 11:44:26.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 11:44:26.991: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 11:44:26.991: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 11:44:26.999: INFO: Found 1 stateful pods, waiting for 3
May 25 11:44:37.023: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 11:44:37.023: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 11:44:37.023: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 25 11:44:37.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 11:44:37.268: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 11:44:37.268: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 11:44:37.268: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 11:44:37.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 11:44:37.479: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 11:44:37.479: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 11:44:37.479: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 11:44:37.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 11:44:37.707: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 11:44:37.707: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 11:44:37.707: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 11:44:37.707: INFO: Waiting for statefulset status.replicas updated to 0
May 25 11:44:37.714: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 25 11:44:47.728: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 11:44:47.729: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 25 11:44:47.729: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 25 11:44:47.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999964s
May 25 11:44:48.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98477047s
May 25 11:44:49.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975347853s
May 25 11:44:50.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962576379s
May 25 11:44:51.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.950625594s
May 25 11:44:52.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.93764645s
May 25 11:44:53.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.926208506s
May 25 11:44:54.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.914946642s
May 25 11:44:55.850: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.904776883s
May 25 11:44:56.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 892.930037ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3600
May 25 11:44:57.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 11:44:58.134: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 11:44:58.134: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 11:44:58.134: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 11:44:58.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 11:44:58.343: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 11:44:58.343: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 11:44:58.343: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 11:44:58.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=statefulset-3600 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 11:44:58.544: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 11:44:58.544: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 11:44:58.544: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 11:44:58.544: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 25 11:45:18.575: INFO: Deleting all statefulset in ns statefulset-3600
May 25 11:45:18.582: INFO: Scaling statefulset ss to 0
May 25 11:45:18.605: INFO: Waiting for statefulset status.replicas updated to 0
May 25 11:45:18.612: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:45:18.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3600" for this suite.

• [SLOW TEST:82.689 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":290,"skipped":4986,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:45:18.676: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:45:18.814: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-64ebab6e-a216-432a-8b4d-4f86e507a1cf" in namespace "security-context-test-857" to be "Succeeded or Failed"
May 25 11:45:18.821: INFO: Pod "busybox-readonly-false-64ebab6e-a216-432a-8b4d-4f86e507a1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.571194ms
May 25 11:45:20.835: INFO: Pod "busybox-readonly-false-64ebab6e-a216-432a-8b4d-4f86e507a1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02102967s
May 25 11:45:20.835: INFO: Pod "busybox-readonly-false-64ebab6e-a216-432a-8b4d-4f86e507a1cf" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:45:20.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-857" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":4996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:45:20.854: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 25 11:45:20.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669" in namespace "downward-api-742" to be "Succeeded or Failed"
May 25 11:45:20.923: INFO: Pod "downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669": Phase="Pending", Reason="", readiness=false. Elapsed: 5.758489ms
May 25 11:45:22.942: INFO: Pod "downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02430919s
STEP: Saw pod success
May 25 11:45:22.942: INFO: Pod "downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669" satisfied condition "Succeeded or Failed"
May 25 11:45:22.947: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669 container client-container: <nil>
STEP: delete the pod
May 25 11:45:22.983: INFO: Waiting for pod downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669 to disappear
May 25 11:45:22.993: INFO: Pod downwardapi-volume-c79321af-1788-48dc-a079-24fc7464b669 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:45:22.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-742" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":5041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:45:23.014: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-ldlz
STEP: Creating a pod to test atomic-volume-subpath
May 25 11:45:23.100: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ldlz" in namespace "subpath-6093" to be "Succeeded or Failed"
May 25 11:45:23.108: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Pending", Reason="", readiness=false. Elapsed: 7.754235ms
May 25 11:45:25.121: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 2.021167742s
May 25 11:45:27.137: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 4.036557623s
May 25 11:45:29.154: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 6.053868239s
May 25 11:45:31.172: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 8.071554508s
May 25 11:45:33.191: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 10.090833648s
May 25 11:45:35.219: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 12.118890275s
May 25 11:45:37.236: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 14.135720112s
May 25 11:45:39.329: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 16.228616436s
May 25 11:45:41.347: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 18.246888579s
May 25 11:45:43.363: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Running", Reason="", readiness=true. Elapsed: 20.262798278s
May 25 11:45:45.379: INFO: Pod "pod-subpath-test-projected-ldlz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.279057331s
STEP: Saw pod success
May 25 11:45:45.379: INFO: Pod "pod-subpath-test-projected-ldlz" satisfied condition "Succeeded or Failed"
May 25 11:45:45.389: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod pod-subpath-test-projected-ldlz container test-container-subpath-projected-ldlz: <nil>
STEP: delete the pod
May 25 11:45:45.458: INFO: Waiting for pod pod-subpath-test-projected-ldlz to disappear
May 25 11:45:45.473: INFO: Pod pod-subpath-test-projected-ldlz no longer exists
STEP: Deleting pod pod-subpath-test-projected-ldlz
May 25 11:45:45.473: INFO: Deleting pod "pod-subpath-test-projected-ldlz" in namespace "subpath-6093"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:45:45.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6093" for this suite.

• [SLOW TEST:22.500 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":293,"skipped":5100,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:45:45.520: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:45:45.596: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2914
I0525 11:45:45.613185      19 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2914, replica count: 1
I0525 11:45:46.663571      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0525 11:45:47.663885      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:45:47.812: INFO: Created: latency-svc-xlymi
May 25 11:45:47.855: INFO: Got endpoints: latency-svc-xlymi [91.141046ms]
May 25 11:45:47.982: INFO: Created: latency-svc-ywyww
May 25 11:45:48.015: INFO: Got endpoints: latency-svc-ywyww [159.870883ms]
May 25 11:45:48.027: INFO: Created: latency-svc-xxpne
May 25 11:45:48.036: INFO: Got endpoints: latency-svc-xxpne [180.143646ms]
May 25 11:45:48.044: INFO: Created: latency-svc-tftqp
May 25 11:45:48.052: INFO: Got endpoints: latency-svc-tftqp [195.403118ms]
May 25 11:45:48.067: INFO: Created: latency-svc-ivvtr
May 25 11:45:48.073: INFO: Got endpoints: latency-svc-ivvtr [217.0943ms]
May 25 11:45:48.081: INFO: Created: latency-svc-auods
May 25 11:45:48.087: INFO: Got endpoints: latency-svc-auods [230.691133ms]
May 25 11:45:48.120: INFO: Created: latency-svc-uywel
May 25 11:45:48.126: INFO: Created: latency-svc-cvfkd
May 25 11:45:48.138: INFO: Got endpoints: latency-svc-uywel [282.033996ms]
May 25 11:45:48.148: INFO: Created: latency-svc-wwtjz
May 25 11:45:48.150: INFO: Got endpoints: latency-svc-cvfkd [293.698867ms]
May 25 11:45:48.157: INFO: Got endpoints: latency-svc-wwtjz [300.398459ms]
May 25 11:45:48.178: INFO: Created: latency-svc-hokle
May 25 11:45:48.183: INFO: Got endpoints: latency-svc-hokle [326.440802ms]
May 25 11:45:48.187: INFO: Created: latency-svc-uhlrz
May 25 11:45:48.201: INFO: Got endpoints: latency-svc-uhlrz [345.585635ms]
May 25 11:45:48.206: INFO: Created: latency-svc-suwas
May 25 11:45:48.217: INFO: Got endpoints: latency-svc-suwas [360.637105ms]
May 25 11:45:48.231: INFO: Created: latency-svc-nvnvp
May 25 11:45:48.235: INFO: Got endpoints: latency-svc-nvnvp [378.726048ms]
May 25 11:45:48.256: INFO: Created: latency-svc-pujip
May 25 11:45:48.267: INFO: Created: latency-svc-irxja
May 25 11:45:48.276: INFO: Got endpoints: latency-svc-irxja [419.24311ms]
May 25 11:45:48.276: INFO: Got endpoints: latency-svc-pujip [419.518169ms]
May 25 11:45:48.293: INFO: Created: latency-svc-zudxl
May 25 11:45:48.306: INFO: Got endpoints: latency-svc-zudxl [291.194903ms]
May 25 11:45:48.346: INFO: Created: latency-svc-naarw
May 25 11:45:48.368: INFO: Got endpoints: latency-svc-naarw [294.571048ms]
May 25 11:45:48.384: INFO: Created: latency-svc-cetpd
May 25 11:45:48.405: INFO: Got endpoints: latency-svc-cetpd [368.209412ms]
May 25 11:45:48.438: INFO: Created: latency-svc-nvbdd
May 25 11:45:48.449: INFO: Created: latency-svc-atueo
May 25 11:45:48.478: INFO: Got endpoints: latency-svc-atueo [390.821483ms]
May 25 11:45:48.488: INFO: Created: latency-svc-fzsjn
May 25 11:45:48.501: INFO: Created: latency-svc-dbnfw
May 25 11:45:48.507: INFO: Got endpoints: latency-svc-fzsjn [650.37867ms]
May 25 11:45:48.507: INFO: Got endpoints: latency-svc-nvbdd [455.82357ms]
May 25 11:45:48.528: INFO: Created: latency-svc-wilyj
May 25 11:45:48.529: INFO: Created: latency-svc-qmjez
May 25 11:45:48.537: INFO: Got endpoints: latency-svc-dbnfw [380.742651ms]
May 25 11:45:48.565: INFO: Got endpoints: latency-svc-wilyj [347.49112ms]
May 25 11:45:48.571: INFO: Got endpoints: latency-svc-qmjez [294.52038ms]
May 25 11:45:48.586: INFO: Created: latency-svc-ovxxb
May 25 11:45:48.606: INFO: Created: latency-svc-uznoo
May 25 11:45:48.607: INFO: Got endpoints: latency-svc-uznoo [468.330805ms]
May 25 11:45:48.618: INFO: Got endpoints: latency-svc-ovxxb [382.658183ms]
May 25 11:45:48.632: INFO: Created: latency-svc-vjspc
May 25 11:45:48.647: INFO: Got endpoints: latency-svc-vjspc [446.243622ms]
May 25 11:45:48.659: INFO: Created: latency-svc-rhdlb
May 25 11:45:48.663: INFO: Created: latency-svc-pihlg
May 25 11:45:48.665: INFO: Got endpoints: latency-svc-rhdlb [482.617568ms]
May 25 11:45:48.687: INFO: Created: latency-svc-foqem
May 25 11:45:48.700: INFO: Got endpoints: latency-svc-foqem [424.506144ms]
May 25 11:45:48.701: INFO: Got endpoints: latency-svc-pihlg [394.447011ms]
May 25 11:45:48.720: INFO: Created: latency-svc-yfcob
May 25 11:45:48.741: INFO: Got endpoints: latency-svc-yfcob [372.731581ms]
May 25 11:45:48.744: INFO: Created: latency-svc-ilknz
May 25 11:45:48.759: INFO: Created: latency-svc-loqwz
May 25 11:45:48.791: INFO: Created: latency-svc-xetai
May 25 11:45:48.801: INFO: Created: latency-svc-hcutx
May 25 11:45:48.801: INFO: Got endpoints: latency-svc-loqwz [323.261403ms]
May 25 11:45:48.801: INFO: Got endpoints: latency-svc-ilknz [396.760285ms]
May 25 11:45:48.819: INFO: Created: latency-svc-acqlz
May 25 11:45:48.824: INFO: Got endpoints: latency-svc-xetai [673.279584ms]
May 25 11:45:48.839: INFO: Created: latency-svc-hsfrp
May 25 11:45:48.840: INFO: Got endpoints: latency-svc-hcutx [333.153238ms]
May 25 11:45:48.855: INFO: Created: latency-svc-vektg
May 25 11:45:48.858: INFO: Got endpoints: latency-svc-acqlz [350.259474ms]
May 25 11:45:48.870: INFO: Got endpoints: latency-svc-hsfrp [332.002693ms]
May 25 11:45:48.870: INFO: Created: latency-svc-pwarx
May 25 11:45:48.877: INFO: Got endpoints: latency-svc-vektg [312.566874ms]
May 25 11:45:48.893: INFO: Created: latency-svc-hwmvh
May 25 11:45:48.893: INFO: Got endpoints: latency-svc-pwarx [275.367923ms]
May 25 11:45:48.900: INFO: Created: latency-svc-crosd
May 25 11:45:48.907: INFO: Got endpoints: latency-svc-hwmvh [336.024019ms]
May 25 11:45:48.909: INFO: Created: latency-svc-cgulv
May 25 11:45:48.954: INFO: Got endpoints: latency-svc-cgulv [306.431753ms]
May 25 11:45:48.954: INFO: Got endpoints: latency-svc-crosd [347.088658ms]
May 25 11:45:48.971: INFO: Created: latency-svc-nohli
May 25 11:45:48.975: INFO: Created: latency-svc-ucmak
May 25 11:45:48.981: INFO: Got endpoints: latency-svc-nohli [315.226653ms]
May 25 11:45:48.993: INFO: Created: latency-svc-qfxxc
May 25 11:45:48.996: INFO: Got endpoints: latency-svc-ucmak [295.365051ms]
May 25 11:45:49.004: INFO: Got endpoints: latency-svc-qfxxc [303.822177ms]
May 25 11:45:49.008: INFO: Created: latency-svc-vrlpv
May 25 11:45:49.017: INFO: Got endpoints: latency-svc-vrlpv [275.554549ms]
May 25 11:45:49.018: INFO: Created: latency-svc-tsbek
May 25 11:45:49.032: INFO: Got endpoints: latency-svc-tsbek [229.38897ms]
May 25 11:45:49.032: INFO: Created: latency-svc-wwrld
May 25 11:45:49.045: INFO: Created: latency-svc-uupyd
May 25 11:45:49.054: INFO: Got endpoints: latency-svc-uupyd [214.264864ms]
May 25 11:45:49.055: INFO: Got endpoints: latency-svc-wwrld [253.295647ms]
May 25 11:45:49.067: INFO: Created: latency-svc-wppao
May 25 11:45:49.078: INFO: Created: latency-svc-itwfu
May 25 11:45:49.080: INFO: Got endpoints: latency-svc-wppao [256.789641ms]
May 25 11:45:49.086: INFO: Created: latency-svc-mczjn
May 25 11:45:49.089: INFO: Got endpoints: latency-svc-itwfu [231.537777ms]
May 25 11:45:49.099: INFO: Got endpoints: latency-svc-mczjn [205.628363ms]
May 25 11:45:49.104: INFO: Created: latency-svc-mvrrv
May 25 11:45:49.114: INFO: Got endpoints: latency-svc-mvrrv [244.314954ms]
May 25 11:45:49.120: INFO: Created: latency-svc-izkxa
May 25 11:45:49.139: INFO: Got endpoints: latency-svc-izkxa [184.750192ms]
May 25 11:45:49.151: INFO: Created: latency-svc-xdkhi
May 25 11:45:49.176: INFO: Created: latency-svc-hnreb
May 25 11:45:49.190: INFO: Got endpoints: latency-svc-xdkhi [312.858543ms]
May 25 11:45:49.191: INFO: Created: latency-svc-cblma
May 25 11:45:49.207: INFO: Created: latency-svc-xgnau
May 25 11:45:49.245: INFO: Got endpoints: latency-svc-hnreb [290.723341ms]
May 25 11:45:49.262: INFO: Created: latency-svc-bpaec
May 25 11:45:49.298: INFO: Got endpoints: latency-svc-cblma [390.652501ms]
May 25 11:45:49.314: INFO: Created: latency-svc-arhnu
May 25 11:45:49.315: INFO: Created: latency-svc-oycad
May 25 11:45:49.320: INFO: Created: latency-svc-kyfdn
May 25 11:45:49.333: INFO: Created: latency-svc-iggqi
May 25 11:45:49.397: INFO: Created: latency-svc-bkaqv
May 25 11:45:49.427: INFO: Got endpoints: latency-svc-bpaec [422.779008ms]
May 25 11:45:49.427: INFO: Got endpoints: latency-svc-xgnau [446.595811ms]
May 25 11:45:49.443: INFO: Created: latency-svc-idlal
May 25 11:45:49.466: INFO: Got endpoints: latency-svc-arhnu [469.316709ms]
May 25 11:45:49.479: INFO: Created: latency-svc-okewr
May 25 11:45:49.479: INFO: Created: latency-svc-emeiq
May 25 11:45:49.488: INFO: Created: latency-svc-rmwtk
May 25 11:45:49.490: INFO: Got endpoints: latency-svc-oycad [473.034239ms]
May 25 11:45:49.521: INFO: Created: latency-svc-nbhib
May 25 11:45:49.542: INFO: Got endpoints: latency-svc-kyfdn [509.978666ms]
May 25 11:45:49.556: INFO: Created: latency-svc-wjwne
May 25 11:45:49.560: INFO: Created: latency-svc-cerwb
May 25 11:45:49.570: INFO: Created: latency-svc-gcwwu
May 25 11:45:49.609: INFO: Created: latency-svc-nktrf
May 25 11:45:49.609: INFO: Got endpoints: latency-svc-iggqi [553.866234ms]
May 25 11:45:49.616: INFO: Created: latency-svc-autnn
May 25 11:45:49.635: INFO: Created: latency-svc-ylgwa
May 25 11:45:49.643: INFO: Created: latency-svc-phfjp
May 25 11:45:49.644: INFO: Got endpoints: latency-svc-bkaqv [589.267176ms]
May 25 11:45:49.657: INFO: Created: latency-svc-vocqg
May 25 11:45:49.705: INFO: Got endpoints: latency-svc-idlal [615.759024ms]
May 25 11:45:49.706: INFO: Created: latency-svc-hiqyg
May 25 11:45:49.722: INFO: Created: latency-svc-uudeo
May 25 11:45:49.746: INFO: Got endpoints: latency-svc-emeiq [646.548109ms]
May 25 11:45:49.781: INFO: Created: latency-svc-jmnkf
May 25 11:45:49.792: INFO: Got endpoints: latency-svc-okewr [678.338076ms]
May 25 11:45:49.812: INFO: Created: latency-svc-rxgkm
May 25 11:45:49.851: INFO: Created: latency-svc-nnpmx
May 25 11:45:49.854: INFO: Got endpoints: latency-svc-rmwtk [715.66398ms]
May 25 11:45:49.892: INFO: Got endpoints: latency-svc-nbhib [811.1047ms]
May 25 11:45:49.920: INFO: Created: latency-svc-tysvi
May 25 11:45:49.954: INFO: Got endpoints: latency-svc-wjwne [763.909466ms]
May 25 11:45:49.963: INFO: Created: latency-svc-abauy
May 25 11:45:49.996: INFO: Got endpoints: latency-svc-cerwb [750.970027ms]
May 25 11:45:50.017: INFO: Created: latency-svc-lyryj
May 25 11:45:50.043: INFO: Got endpoints: latency-svc-gcwwu [745.022452ms]
May 25 11:45:50.096: INFO: Created: latency-svc-dunez
May 25 11:45:50.099: INFO: Got endpoints: latency-svc-nktrf [671.620552ms]
May 25 11:45:50.129: INFO: Created: latency-svc-bbcqx
May 25 11:45:50.144: INFO: Got endpoints: latency-svc-autnn [716.246479ms]
May 25 11:45:50.165: INFO: Created: latency-svc-lrbyn
May 25 11:45:50.208: INFO: Got endpoints: latency-svc-ylgwa [718.524238ms]
May 25 11:45:50.214: INFO: Created: latency-svc-fiypp
May 25 11:45:50.247: INFO: Got endpoints: latency-svc-phfjp [780.985755ms]
May 25 11:45:50.261: INFO: Created: latency-svc-pasru
May 25 11:45:50.294: INFO: Got endpoints: latency-svc-vocqg [752.096983ms]
May 25 11:45:50.314: INFO: Created: latency-svc-hvdnr
May 25 11:45:50.351: INFO: Got endpoints: latency-svc-hiqyg [741.183508ms]
May 25 11:45:50.364: INFO: Created: latency-svc-ombpj
May 25 11:45:50.395: INFO: Got endpoints: latency-svc-uudeo [750.610088ms]
May 25 11:45:50.461: INFO: Got endpoints: latency-svc-jmnkf [755.61058ms]
May 25 11:45:50.466: INFO: Created: latency-svc-doclz
May 25 11:45:50.485: INFO: Created: latency-svc-wtkdz
May 25 11:45:50.491: INFO: Got endpoints: latency-svc-rxgkm [744.422803ms]
May 25 11:45:50.531: INFO: Created: latency-svc-wvqwv
May 25 11:45:50.561: INFO: Got endpoints: latency-svc-nnpmx [768.556213ms]
May 25 11:45:50.570: INFO: Created: latency-svc-alvzc
May 25 11:45:50.598: INFO: Got endpoints: latency-svc-tysvi [743.136266ms]
May 25 11:45:50.618: INFO: Created: latency-svc-izyjx
May 25 11:45:50.653: INFO: Got endpoints: latency-svc-abauy [761.403551ms]
May 25 11:45:50.670: INFO: Created: latency-svc-bmbap
May 25 11:45:50.694: INFO: Got endpoints: latency-svc-lyryj [739.478788ms]
May 25 11:45:50.714: INFO: Created: latency-svc-vwtvp
May 25 11:45:50.751: INFO: Got endpoints: latency-svc-dunez [755.106371ms]
May 25 11:45:50.756: INFO: Created: latency-svc-jfirm
May 25 11:45:50.791: INFO: Got endpoints: latency-svc-bbcqx [747.415798ms]
May 25 11:45:50.811: INFO: Created: latency-svc-qgebn
May 25 11:45:50.846: INFO: Got endpoints: latency-svc-lrbyn [746.961944ms]
May 25 11:45:50.850: INFO: Created: latency-svc-wvcmy
May 25 11:45:50.892: INFO: Got endpoints: latency-svc-fiypp [748.201545ms]
May 25 11:45:50.928: INFO: Created: latency-svc-xlvwq
May 25 11:45:50.943: INFO: Got endpoints: latency-svc-pasru [734.347179ms]
May 25 11:45:50.966: INFO: Created: latency-svc-spkte
May 25 11:45:50.996: INFO: Got endpoints: latency-svc-hvdnr [749.026431ms]
May 25 11:45:50.997: INFO: Created: latency-svc-oqlok
May 25 11:45:51.042: INFO: Got endpoints: latency-svc-ombpj [748.003632ms]
May 25 11:45:51.049: INFO: Created: latency-svc-ozqes
May 25 11:45:51.100: INFO: Got endpoints: latency-svc-doclz [749.495788ms]
May 25 11:45:51.101: INFO: Created: latency-svc-ahftw
May 25 11:45:51.145: INFO: Got endpoints: latency-svc-wtkdz [750.853273ms]
May 25 11:45:51.160: INFO: Created: latency-svc-dheby
May 25 11:45:51.204: INFO: Got endpoints: latency-svc-wvqwv [743.548234ms]
May 25 11:45:51.223: INFO: Created: latency-svc-dksql
May 25 11:45:51.244: INFO: Got endpoints: latency-svc-alvzc [753.469649ms]
May 25 11:45:51.257: INFO: Created: latency-svc-mduhj
May 25 11:45:51.291: INFO: Created: latency-svc-kjjzi
May 25 11:45:51.298: INFO: Got endpoints: latency-svc-izyjx [737.210391ms]
May 25 11:45:51.344: INFO: Got endpoints: latency-svc-bmbap [746.047282ms]
May 25 11:45:51.350: INFO: Created: latency-svc-nbsys
May 25 11:45:51.396: INFO: Got endpoints: latency-svc-vwtvp [741.797995ms]
May 25 11:45:51.407: INFO: Created: latency-svc-hqhej
May 25 11:45:51.454: INFO: Got endpoints: latency-svc-jfirm [759.537188ms]
May 25 11:45:51.476: INFO: Created: latency-svc-lycqg
May 25 11:45:51.493: INFO: Got endpoints: latency-svc-qgebn [741.514166ms]
May 25 11:45:51.523: INFO: Created: latency-svc-eiktk
May 25 11:45:51.547: INFO: Created: latency-svc-lsomg
May 25 11:45:51.569: INFO: Got endpoints: latency-svc-wvcmy [778.541235ms]
May 25 11:45:51.596: INFO: Got endpoints: latency-svc-xlvwq [749.6253ms]
May 25 11:45:51.611: INFO: Created: latency-svc-jphql
May 25 11:45:51.641: INFO: Got endpoints: latency-svc-spkte [748.254142ms]
May 25 11:45:51.695: INFO: Created: latency-svc-himnk
May 25 11:45:51.698: INFO: Got endpoints: latency-svc-oqlok [755.464903ms]
May 25 11:45:51.727: INFO: Created: latency-svc-ykdbw
May 25 11:45:51.749: INFO: Created: latency-svc-pwcyv
May 25 11:45:51.753: INFO: Got endpoints: latency-svc-ozqes [757.303847ms]
May 25 11:45:51.793: INFO: Got endpoints: latency-svc-ahftw [751.191402ms]
May 25 11:45:51.802: INFO: Created: latency-svc-enrlr
May 25 11:45:51.843: INFO: Got endpoints: latency-svc-dheby [742.035749ms]
May 25 11:45:51.851: INFO: Created: latency-svc-qpzls
May 25 11:45:51.886: INFO: Created: latency-svc-eqpms
May 25 11:45:51.895: INFO: Got endpoints: latency-svc-dksql [749.25107ms]
May 25 11:45:51.943: INFO: Created: latency-svc-qdcdm
May 25 11:45:51.948: INFO: Got endpoints: latency-svc-mduhj [743.406328ms]
May 25 11:45:51.982: INFO: Created: latency-svc-cbzte
May 25 11:45:51.992: INFO: Got endpoints: latency-svc-kjjzi [747.622901ms]
May 25 11:45:52.026: INFO: Created: latency-svc-cwopq
May 25 11:45:52.040: INFO: Got endpoints: latency-svc-nbsys [741.820625ms]
May 25 11:45:52.076: INFO: Created: latency-svc-xddvh
May 25 11:45:52.090: INFO: Got endpoints: latency-svc-hqhej [745.962413ms]
May 25 11:45:52.127: INFO: Created: latency-svc-bxocn
May 25 11:45:52.142: INFO: Got endpoints: latency-svc-lycqg [746.491254ms]
May 25 11:45:52.180: INFO: Created: latency-svc-iqflv
May 25 11:45:52.195: INFO: Got endpoints: latency-svc-eiktk [741.522295ms]
May 25 11:45:52.230: INFO: Created: latency-svc-qualo
May 25 11:45:52.239: INFO: Got endpoints: latency-svc-lsomg [746.031537ms]
May 25 11:45:52.291: INFO: Created: latency-svc-dsvot
May 25 11:45:52.298: INFO: Got endpoints: latency-svc-jphql [728.417744ms]
May 25 11:45:52.339: INFO: Created: latency-svc-rfgmq
May 25 11:45:52.345: INFO: Got endpoints: latency-svc-himnk [748.451936ms]
May 25 11:45:52.381: INFO: Created: latency-svc-nwpor
May 25 11:45:52.391: INFO: Got endpoints: latency-svc-ykdbw [750.504388ms]
May 25 11:45:52.431: INFO: Created: latency-svc-ifjqf
May 25 11:45:52.445: INFO: Got endpoints: latency-svc-pwcyv [746.734061ms]
May 25 11:45:52.490: INFO: Got endpoints: latency-svc-enrlr [736.370495ms]
May 25 11:45:52.499: INFO: Created: latency-svc-unmba
May 25 11:45:52.550: INFO: Created: latency-svc-wflzm
May 25 11:45:52.550: INFO: Got endpoints: latency-svc-qpzls [756.409168ms]
May 25 11:45:52.598: INFO: Created: latency-svc-nojcm
May 25 11:45:52.602: INFO: Got endpoints: latency-svc-eqpms [758.949404ms]
May 25 11:45:52.644: INFO: Got endpoints: latency-svc-qdcdm [749.164649ms]
May 25 11:45:52.649: INFO: Created: latency-svc-jweai
May 25 11:45:52.685: INFO: Created: latency-svc-akfpv
May 25 11:45:52.696: INFO: Got endpoints: latency-svc-cbzte [748.068113ms]
May 25 11:45:52.739: INFO: Created: latency-svc-mggyq
May 25 11:45:52.740: INFO: Got endpoints: latency-svc-cwopq [747.931525ms]
May 25 11:45:52.788: INFO: Got endpoints: latency-svc-xddvh [748.206548ms]
May 25 11:45:52.793: INFO: Created: latency-svc-rcbuz
May 25 11:45:52.848: INFO: Created: latency-svc-qqivs
May 25 11:45:52.855: INFO: Got endpoints: latency-svc-bxocn [764.995847ms]
May 25 11:45:52.891: INFO: Got endpoints: latency-svc-iqflv [748.350246ms]
May 25 11:45:52.927: INFO: Created: latency-svc-tnuxn
May 25 11:45:52.945: INFO: Got endpoints: latency-svc-qualo [750.141761ms]
May 25 11:45:52.954: INFO: Created: latency-svc-mujac
May 25 11:45:52.992: INFO: Got endpoints: latency-svc-dsvot [753.168166ms]
May 25 11:45:53.039: INFO: Got endpoints: latency-svc-rfgmq [741.23329ms]
May 25 11:45:53.056: INFO: Created: latency-svc-wxxcb
May 25 11:45:53.095: INFO: Got endpoints: latency-svc-nwpor [749.869455ms]
May 25 11:45:53.122: INFO: Created: latency-svc-ebrye
May 25 11:45:53.150: INFO: Got endpoints: latency-svc-ifjqf [758.318015ms]
May 25 11:45:53.204: INFO: Created: latency-svc-nvweg
May 25 11:45:53.227: INFO: Got endpoints: latency-svc-unmba [782.244741ms]
May 25 11:45:53.301: INFO: Got endpoints: latency-svc-wflzm [810.97248ms]
May 25 11:45:53.319: INFO: Created: latency-svc-jiktt
May 25 11:45:53.373: INFO: Created: latency-svc-tvvtu
May 25 11:45:53.384: INFO: Created: latency-svc-roufz
May 25 11:45:53.452: INFO: Created: latency-svc-cjrsk
May 25 11:45:53.453: INFO: Got endpoints: latency-svc-nojcm [902.341512ms]
May 25 11:45:53.453: INFO: Got endpoints: latency-svc-akfpv [808.727468ms]
May 25 11:45:53.453: INFO: Got endpoints: latency-svc-jweai [851.006647ms]
May 25 11:45:53.505: INFO: Got endpoints: latency-svc-mggyq [809.323433ms]
May 25 11:45:53.608: INFO: Got endpoints: latency-svc-rcbuz [867.730665ms]
May 25 11:45:53.613: INFO: Got endpoints: latency-svc-qqivs [824.750506ms]
May 25 11:45:53.630: INFO: Created: latency-svc-bilxj
May 25 11:45:53.633: INFO: Got endpoints: latency-svc-tnuxn [777.825193ms]
May 25 11:45:53.657: INFO: Created: latency-svc-yuzmu
May 25 11:45:53.662: INFO: Got endpoints: latency-svc-mujac [770.698067ms]
May 25 11:45:53.662: INFO: Created: latency-svc-edgcf
May 25 11:45:53.667: INFO: Created: latency-svc-ebuui
May 25 11:45:53.690: INFO: Got endpoints: latency-svc-wxxcb [744.610514ms]
May 25 11:45:53.748: INFO: Got endpoints: latency-svc-ebrye [755.077367ms]
May 25 11:45:53.753: INFO: Created: latency-svc-nanzw
May 25 11:45:53.779: INFO: Created: latency-svc-jptpa
May 25 11:45:53.782: INFO: Created: latency-svc-plaju
May 25 11:45:53.805: INFO: Created: latency-svc-dlyjc
May 25 11:45:53.805: INFO: Got endpoints: latency-svc-nvweg [766.304174ms]
May 25 11:45:53.814: INFO: Created: latency-svc-wjbxz
May 25 11:45:53.848: INFO: Got endpoints: latency-svc-jiktt [752.821588ms]
May 25 11:45:53.891: INFO: Got endpoints: latency-svc-tvvtu [741.655407ms]
May 25 11:45:53.905: INFO: Created: latency-svc-jkoai
May 25 11:45:53.955: INFO: Got endpoints: latency-svc-roufz [653.717492ms]
May 25 11:45:53.955: INFO: Created: latency-svc-itfzc
May 25 11:45:53.992: INFO: Got endpoints: latency-svc-cjrsk [764.745854ms]
May 25 11:45:54.051: INFO: Created: latency-svc-fbvqe
May 25 11:45:54.053: INFO: Got endpoints: latency-svc-bilxj [599.608144ms]
May 25 11:45:54.068: INFO: Created: latency-svc-ehxwq
May 25 11:45:54.097: INFO: Got endpoints: latency-svc-yuzmu [643.332196ms]
May 25 11:45:54.106: INFO: Created: latency-svc-xpijl
May 25 11:45:54.117: INFO: Created: latency-svc-wmacd
May 25 11:45:54.144: INFO: Got endpoints: latency-svc-edgcf [638.64846ms]
May 25 11:45:54.163: INFO: Created: latency-svc-vjvhc
May 25 11:45:54.205: INFO: Got endpoints: latency-svc-ebuui [752.593683ms]
May 25 11:45:54.217: INFO: Created: latency-svc-affmf
May 25 11:45:54.243: INFO: Got endpoints: latency-svc-nanzw [635.146548ms]
May 25 11:45:54.255: INFO: Created: latency-svc-jagmv
May 25 11:45:54.293: INFO: Got endpoints: latency-svc-jptpa [679.430745ms]
May 25 11:45:54.301: INFO: Created: latency-svc-rzdvf
May 25 11:45:54.322: INFO: Created: latency-svc-ibbqm
May 25 11:45:54.340: INFO: Got endpoints: latency-svc-plaju [677.954399ms]
May 25 11:45:54.361: INFO: Created: latency-svc-bgeqj
May 25 11:45:54.398: INFO: Got endpoints: latency-svc-dlyjc [764.982532ms]
May 25 11:45:54.410: INFO: Created: latency-svc-cfrez
May 25 11:45:54.446: INFO: Got endpoints: latency-svc-wjbxz [755.701168ms]
May 25 11:45:54.458: INFO: Created: latency-svc-aacel
May 25 11:45:54.494: INFO: Created: latency-svc-lzfye
May 25 11:45:54.507: INFO: Got endpoints: latency-svc-jkoai [758.956501ms]
May 25 11:45:54.542: INFO: Got endpoints: latency-svc-itfzc [736.781435ms]
May 25 11:45:54.550: INFO: Created: latency-svc-nvbcc
May 25 11:45:54.592: INFO: Got endpoints: latency-svc-fbvqe [700.649445ms]
May 25 11:45:54.616: INFO: Created: latency-svc-sivwq
May 25 11:45:54.632: INFO: Created: latency-svc-xtcdd
May 25 11:45:54.641: INFO: Got endpoints: latency-svc-ehxwq [792.467083ms]
May 25 11:45:54.676: INFO: Created: latency-svc-curbw
May 25 11:45:54.687: INFO: Got endpoints: latency-svc-xpijl [732.593604ms]
May 25 11:45:54.727: INFO: Created: latency-svc-wykki
May 25 11:45:54.744: INFO: Got endpoints: latency-svc-wmacd [751.45919ms]
May 25 11:45:54.779: INFO: Created: latency-svc-sfxhi
May 25 11:45:54.793: INFO: Got endpoints: latency-svc-vjvhc [739.290873ms]
May 25 11:45:54.838: INFO: Created: latency-svc-lfaec
May 25 11:45:54.855: INFO: Got endpoints: latency-svc-affmf [757.87096ms]
May 25 11:45:54.894: INFO: Got endpoints: latency-svc-jagmv [749.338825ms]
May 25 11:45:54.904: INFO: Created: latency-svc-abtoe
May 25 11:45:54.936: INFO: Created: latency-svc-ivviq
May 25 11:45:54.946: INFO: Got endpoints: latency-svc-rzdvf [740.420992ms]
May 25 11:45:54.981: INFO: Created: latency-svc-ksxua
May 25 11:45:54.996: INFO: Got endpoints: latency-svc-ibbqm [752.217513ms]
May 25 11:45:55.042: INFO: Got endpoints: latency-svc-bgeqj [749.028739ms]
May 25 11:45:55.059: INFO: Created: latency-svc-lqbbg
May 25 11:45:55.095: INFO: Created: latency-svc-opcqh
May 25 11:45:55.095: INFO: Got endpoints: latency-svc-cfrez [754.960582ms]
May 25 11:45:55.139: INFO: Created: latency-svc-wbxty
May 25 11:45:55.146: INFO: Got endpoints: latency-svc-aacel [748.39876ms]
May 25 11:45:55.179: INFO: Created: latency-svc-acjqh
May 25 11:45:55.190: INFO: Got endpoints: latency-svc-lzfye [744.61325ms]
May 25 11:45:55.234: INFO: Created: latency-svc-fqwsr
May 25 11:45:55.248: INFO: Got endpoints: latency-svc-nvbcc [741.175749ms]
May 25 11:45:55.287: INFO: Created: latency-svc-llaul
May 25 11:45:55.294: INFO: Got endpoints: latency-svc-sivwq [751.620526ms]
May 25 11:45:55.329: INFO: Created: latency-svc-gegyk
May 25 11:45:55.340: INFO: Got endpoints: latency-svc-xtcdd [747.690124ms]
May 25 11:45:55.385: INFO: Created: latency-svc-efyes
May 25 11:45:55.388: INFO: Got endpoints: latency-svc-curbw [747.192531ms]
May 25 11:45:55.447: INFO: Created: latency-svc-zqgoh
May 25 11:45:55.464: INFO: Got endpoints: latency-svc-wykki [776.889941ms]
May 25 11:45:55.492: INFO: Got endpoints: latency-svc-sfxhi [748.582344ms]
May 25 11:45:55.511: INFO: Created: latency-svc-smhqt
May 25 11:45:55.542: INFO: Created: latency-svc-dgcig
May 25 11:45:55.552: INFO: Got endpoints: latency-svc-lfaec [758.905675ms]
May 25 11:45:55.597: INFO: Got endpoints: latency-svc-abtoe [742.018518ms]
May 25 11:45:55.671: INFO: Created: latency-svc-gypor
May 25 11:45:55.674: INFO: Got endpoints: latency-svc-ivviq [780.457879ms]
May 25 11:45:55.698: INFO: Created: latency-svc-nrecs
May 25 11:45:55.710: INFO: Got endpoints: latency-svc-ksxua [764.28971ms]
May 25 11:45:55.750: INFO: Created: latency-svc-ltver
May 25 11:45:55.758: INFO: Got endpoints: latency-svc-lqbbg [762.089335ms]
May 25 11:45:55.779: INFO: Created: latency-svc-zflpc
May 25 11:45:55.792: INFO: Got endpoints: latency-svc-opcqh [749.263875ms]
May 25 11:45:55.807: INFO: Created: latency-svc-muyaq
May 25 11:45:55.842: INFO: Got endpoints: latency-svc-wbxty [746.600667ms]
May 25 11:45:55.889: INFO: Got endpoints: latency-svc-acjqh [742.966273ms]
May 25 11:45:55.940: INFO: Got endpoints: latency-svc-fqwsr [749.872259ms]
May 25 11:45:56.026: INFO: Got endpoints: latency-svc-llaul [778.17361ms]
May 25 11:45:56.040: INFO: Got endpoints: latency-svc-gegyk [746.443271ms]
May 25 11:45:56.089: INFO: Got endpoints: latency-svc-efyes [748.794318ms]
May 25 11:45:56.142: INFO: Got endpoints: latency-svc-zqgoh [754.159019ms]
May 25 11:45:56.189: INFO: Got endpoints: latency-svc-smhqt [724.81793ms]
May 25 11:45:56.242: INFO: Got endpoints: latency-svc-dgcig [749.526567ms]
May 25 11:45:56.289: INFO: Got endpoints: latency-svc-gypor [737.104943ms]
May 25 11:45:56.343: INFO: Got endpoints: latency-svc-nrecs [745.885405ms]
May 25 11:45:56.389: INFO: Got endpoints: latency-svc-ltver [715.252908ms]
May 25 11:45:56.442: INFO: Got endpoints: latency-svc-zflpc [731.620898ms]
May 25 11:45:56.491: INFO: Got endpoints: latency-svc-muyaq [733.373294ms]
May 25 11:45:56.492: INFO: Latencies: [159.870883ms 180.143646ms 184.750192ms 195.403118ms 205.628363ms 214.264864ms 217.0943ms 229.38897ms 230.691133ms 231.537777ms 244.314954ms 253.295647ms 256.789641ms 275.367923ms 275.554549ms 282.033996ms 290.723341ms 291.194903ms 293.698867ms 294.52038ms 294.571048ms 295.365051ms 300.398459ms 303.822177ms 306.431753ms 312.566874ms 312.858543ms 315.226653ms 323.261403ms 326.440802ms 332.002693ms 333.153238ms 336.024019ms 345.585635ms 347.088658ms 347.49112ms 350.259474ms 360.637105ms 368.209412ms 372.731581ms 378.726048ms 380.742651ms 382.658183ms 390.652501ms 390.821483ms 394.447011ms 396.760285ms 419.24311ms 419.518169ms 422.779008ms 424.506144ms 446.243622ms 446.595811ms 455.82357ms 468.330805ms 469.316709ms 473.034239ms 482.617568ms 509.978666ms 553.866234ms 589.267176ms 599.608144ms 615.759024ms 635.146548ms 638.64846ms 643.332196ms 646.548109ms 650.37867ms 653.717492ms 671.620552ms 673.279584ms 677.954399ms 678.338076ms 679.430745ms 700.649445ms 715.252908ms 715.66398ms 716.246479ms 718.524238ms 724.81793ms 728.417744ms 731.620898ms 732.593604ms 733.373294ms 734.347179ms 736.370495ms 736.781435ms 737.104943ms 737.210391ms 739.290873ms 739.478788ms 740.420992ms 741.175749ms 741.183508ms 741.23329ms 741.514166ms 741.522295ms 741.655407ms 741.797995ms 741.820625ms 742.018518ms 742.035749ms 742.966273ms 743.136266ms 743.406328ms 743.548234ms 744.422803ms 744.610514ms 744.61325ms 745.022452ms 745.885405ms 745.962413ms 746.031537ms 746.047282ms 746.443271ms 746.491254ms 746.600667ms 746.734061ms 746.961944ms 747.192531ms 747.415798ms 747.622901ms 747.690124ms 747.931525ms 748.003632ms 748.068113ms 748.201545ms 748.206548ms 748.254142ms 748.350246ms 748.39876ms 748.451936ms 748.582344ms 748.794318ms 749.026431ms 749.028739ms 749.164649ms 749.25107ms 749.263875ms 749.338825ms 749.495788ms 749.526567ms 749.6253ms 749.869455ms 749.872259ms 750.141761ms 750.504388ms 750.610088ms 750.853273ms 750.970027ms 751.191402ms 751.45919ms 751.620526ms 752.096983ms 752.217513ms 752.593683ms 752.821588ms 753.168166ms 753.469649ms 754.159019ms 754.960582ms 755.077367ms 755.106371ms 755.464903ms 755.61058ms 755.701168ms 756.409168ms 757.303847ms 757.87096ms 758.318015ms 758.905675ms 758.949404ms 758.956501ms 759.537188ms 761.403551ms 762.089335ms 763.909466ms 764.28971ms 764.745854ms 764.982532ms 764.995847ms 766.304174ms 768.556213ms 770.698067ms 776.889941ms 777.825193ms 778.17361ms 778.541235ms 780.457879ms 780.985755ms 782.244741ms 792.467083ms 808.727468ms 809.323433ms 810.97248ms 811.1047ms 824.750506ms 851.006647ms 867.730665ms 902.341512ms]
May 25 11:45:56.492: INFO: 50 %ile: 742.018518ms
May 25 11:45:56.492: INFO: 90 %ile: 764.995847ms
May 25 11:45:56.493: INFO: 99 %ile: 867.730665ms
May 25 11:45:56.493: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:45:56.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2914" for this suite.

• [SLOW TEST:11.002 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":294,"skipped":5126,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:45:56.527: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 25 11:45:57.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539957, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539957, loc:(*time.Location)(0x797fe80)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-7d6697c5b7\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539957, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757539957, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:46:00.144: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:46:00.154: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
May 25 11:46:31.057: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-4307-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-2904.svc:9443/crdconvert?timeout=30s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:46:31.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2904" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:35.515 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":295,"skipped":5138,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:46:32.046: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-ea824eae-5514-4d66-a5b8-dbd936dbf7ed
STEP: Creating secret with name secret-projected-all-test-volume-a65b94be-f0a0-49b0-a831-270d0a1d8b12
STEP: Creating a pod to test Check all projections for projected volume plugin
May 25 11:46:32.231: INFO: Waiting up to 5m0s for pod "projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6" in namespace "projected-7827" to be "Succeeded or Failed"
May 25 11:46:32.249: INFO: Pod "projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.569206ms
May 25 11:46:34.264: INFO: Pod "projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032553849s
STEP: Saw pod success
May 25 11:46:34.264: INFO: Pod "projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6" satisfied condition "Succeeded or Failed"
May 25 11:46:34.271: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-p6wx pod projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6 container projected-all-volume-test: <nil>
STEP: delete the pod
May 25 11:46:34.311: INFO: Waiting for pod projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6 to disappear
May 25 11:46:34.324: INFO: Pod projected-volume-049efb03-e077-46e0-b00c-25b48df1b4b6 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:46:34.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7827" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5147,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:46:34.342: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
May 25 11:48:34.972: INFO: Successfully updated pod "var-expansion-9af1255f-b4b5-4307-8745-ea4e6396080e"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 25 11:48:36.993: INFO: Deleting pod "var-expansion-9af1255f-b4b5-4307-8745-ea4e6396080e" in namespace "var-expansion-9774"
May 25 11:48:37.003: INFO: Wait up to 5m0s for pod "var-expansion-9af1255f-b4b5-4307-8745-ea4e6396080e" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:49:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9774" for this suite.

• [SLOW TEST:154.709 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":297,"skipped":5147,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:49:09.052: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
May 25 11:49:09.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-9954 api-versions'
May 25 11:49:09.245: INFO: stderr: ""
May 25 11:49:09.245: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy.jspolicy.com/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:49:09.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9954" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":298,"skipped":5153,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:49:09.282: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 25 11:49:09.402: INFO: observed Pod pod-test in namespace pods-8259 in phase Pending conditions []
May 25 11:49:09.455: INFO: observed Pod pod-test in namespace pods-8259 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 11:49:09 +0000 UTC  }]
May 25 11:49:09.502: INFO: observed Pod pod-test in namespace pods-8259 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 11:49:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 11:49:09 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 11:49:09 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 11:49:09 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 25 11:49:10.744: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 25 11:49:10.810: INFO: observed event type ADDED
May 25 11:49:10.810: INFO: observed event type MODIFIED
May 25 11:49:10.811: INFO: observed event type MODIFIED
May 25 11:49:10.811: INFO: observed event type MODIFIED
May 25 11:49:10.811: INFO: observed event type MODIFIED
May 25 11:49:10.811: INFO: observed event type MODIFIED
May 25 11:49:10.812: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:49:10.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8259" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":299,"skipped":5154,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:49:10.837: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:49:11.004: INFO: Create a RollingUpdate DaemonSet
May 25 11:49:11.015: INFO: Check that daemon pods launch on every node of the cluster
May 25 11:49:11.046: INFO: Number of nodes with available pods: 0
May 25 11:49:11.047: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 11:49:12.156: INFO: Number of nodes with available pods: 0
May 25 11:49:12.157: INFO: Node gke-cluster-1-default-pool-8f0bb8bb-vl79 is running more than one daemon pod
May 25 11:49:13.070: INFO: Number of nodes with available pods: 3
May 25 11:49:13.070: INFO: Number of running nodes: 3, number of available pods: 3
May 25 11:49:13.070: INFO: Update the DaemonSet to trigger a rollout
May 25 11:49:13.088: INFO: Updating DaemonSet daemon-set
May 25 11:49:22.136: INFO: Roll back the DaemonSet before rollout is complete
May 25 11:49:22.163: INFO: Updating DaemonSet daemon-set
May 25 11:49:22.163: INFO: Make sure DaemonSet rollback is complete
May 25 11:49:22.202: INFO: Wrong image for pod: daemon-set-xzf8f. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 25 11:49:22.202: INFO: Pod daemon-set-xzf8f is not available
May 25 11:49:23.233: INFO: Wrong image for pod: daemon-set-xzf8f. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 25 11:49:23.233: INFO: Pod daemon-set-xzf8f is not available
May 25 11:49:24.241: INFO: Pod daemon-set-hxbgk is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4242, will wait for the garbage collector to delete the pods
May 25 11:49:24.348: INFO: Deleting DaemonSet.extensions daemon-set took: 15.591938ms
May 25 11:49:25.149: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.667873ms
May 25 11:50:31.469: INFO: Number of nodes with available pods: 0
May 25 11:50:31.469: INFO: Number of running nodes: 0, number of available pods: 0
May 25 11:50:31.475: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"106246"},"items":null}

May 25 11:50:31.481: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"106246"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:50:31.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4242" for this suite.

• [SLOW TEST:80.696 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":300,"skipped":5154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:50:31.538: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:50:31.648: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 25 11:50:35.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-6537 --namespace=crd-publish-openapi-6537 create -f -'
May 25 11:50:36.110: INFO: stderr: ""
May 25 11:50:36.110: INFO: stdout: "e2e-test-crd-publish-openapi-3237-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 25 11:50:36.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-6537 --namespace=crd-publish-openapi-6537 delete e2e-test-crd-publish-openapi-3237-crds test-cr'
May 25 11:50:36.228: INFO: stderr: ""
May 25 11:50:36.228: INFO: stdout: "e2e-test-crd-publish-openapi-3237-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 25 11:50:36.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-6537 --namespace=crd-publish-openapi-6537 apply -f -'
May 25 11:50:36.525: INFO: stderr: ""
May 25 11:50:36.525: INFO: stdout: "e2e-test-crd-publish-openapi-3237-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 25 11:50:36.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-6537 --namespace=crd-publish-openapi-6537 delete e2e-test-crd-publish-openapi-3237-crds test-cr'
May 25 11:50:36.642: INFO: stderr: ""
May 25 11:50:36.642: INFO: stdout: "e2e-test-crd-publish-openapi-3237-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 25 11:50:36.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=crd-publish-openapi-6537 explain e2e-test-crd-publish-openapi-3237-crds'
May 25 11:50:36.934: INFO: stderr: ""
May 25 11:50:36.934: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3237-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:50:40.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6537" for this suite.

• [SLOW TEST:9.023 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":301,"skipped":5190,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:50:40.566: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1372
STEP: creating service affinity-clusterip in namespace services-1372
STEP: creating replication controller affinity-clusterip in namespace services-1372
I0525 11:50:40.731932      19 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1372, replica count: 3
I0525 11:50:43.782389      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 11:50:43.808: INFO: Creating new exec pod
May 25 11:50:46.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-1372 exec execpod-affinityv465l -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 25 11:50:47.105: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 25 11:50:47.105: INFO: stdout: ""
May 25 11:50:47.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-1372 exec execpod-affinityv465l -- /bin/sh -x -c nc -zv -t -w 2 10.68.7.194 80'
May 25 11:50:47.316: INFO: stderr: "+ nc -zv -t -w 2 10.68.7.194 80\nConnection to 10.68.7.194 80 port [tcp/http] succeeded!\n"
May 25 11:50:47.316: INFO: stdout: ""
May 25 11:50:47.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=services-1372 exec execpod-affinityv465l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.7.194:80/ ; done'
May 25 11:50:47.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.7.194:80/\n"
May 25 11:50:47.629: INFO: stdout: "\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t\naffinity-clusterip-52f4t"
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Received response from host: affinity-clusterip-52f4t
May 25 11:50:47.629: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1372, will wait for the garbage collector to delete the pods
May 25 11:50:47.714: INFO: Deleting ReplicationController affinity-clusterip took: 10.55802ms
May 25 11:50:48.515: INFO: Terminating ReplicationController affinity-clusterip pods took: 800.248925ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:02.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1372" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:21.845 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":302,"skipped":5207,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:02.413: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:51:02.468: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:04.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3521" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5217,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:04.620: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-c4839c1f-9597-4aba-924d-4ec612d716e3
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:04.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1349" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":304,"skipped":5218,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:04.700: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:04.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8487" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":305,"skipped":5233,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:04.774: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 25 11:51:08.931: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 25 11:51:08.939: INFO: Pod pod-with-prestop-exec-hook still exists
May 25 11:51:10.939: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 25 11:51:10.955: INFO: Pod pod-with-prestop-exec-hook still exists
May 25 11:51:12.939: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 25 11:51:12.949: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:12.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1595" for this suite.

• [SLOW TEST:8.211 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5237,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:12.986: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3074" for this suite.

• [SLOW TEST:11.188 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":307,"skipped":5237,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:24.174: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 11:51:24.838: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 11:51:26.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757540284, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757540284, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757540284, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757540284, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 11:51:29.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 25 11:51:29.943: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9569-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:31.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7511" for this suite.
STEP: Destroying namespace "webhook-7511-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.532 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":308,"skipped":5252,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:31.707: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 25 11:51:31.833: INFO: Waiting up to 5m0s for pod "pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2" in namespace "emptydir-781" to be "Succeeded or Failed"
May 25 11:51:31.852: INFO: Pod "pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.405987ms
May 25 11:51:33.868: INFO: Pod "pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034200417s
STEP: Saw pod success
May 25 11:51:33.868: INFO: Pod "pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2" satisfied condition "Succeeded or Failed"
May 25 11:51:33.874: INFO: Trying to get logs from node gke-cluster-1-default-pool-8f0bb8bb-wpkp pod pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2 container test-container: <nil>
STEP: delete the pod
May 25 11:51:33.958: INFO: Waiting for pod pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2 to disappear
May 25 11:51:33.967: INFO: Pod pod-536e3297-78b6-4d2f-9393-2f2f6a1e1ae2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:33.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-781" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:33.988: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1107.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1107.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 11:51:36.150: INFO: DNS probes using dns-1107/dns-test-5cebb39b-d5e0-4feb-803a-155fcce80b64 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:36.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1107" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":310,"skipped":5327,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 25 11:51:36.202: INFO: >>> kubeConfig: /tmp/kubeconfig-325827021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 25 11:51:36.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-9881 create -f -'
May 25 11:51:36.678: INFO: stderr: ""
May 25 11:51:36.678: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 25 11:51:37.691: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:51:37.691: INFO: Found 0 / 1
May 25 11:51:38.698: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:51:38.698: INFO: Found 1 / 1
May 25 11:51:38.698: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 25 11:51:38.705: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:51:38.705: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 25 11:51:38.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-325827021 --namespace=kubectl-9881 patch pod agnhost-primary-ztzzl -p {"metadata":{"annotations":{"x":"y"}}}'
May 25 11:51:38.825: INFO: stderr: ""
May 25 11:51:38.825: INFO: stdout: "pod/agnhost-primary-ztzzl patched\n"
STEP: checking annotations
May 25 11:51:38.831: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 11:51:38.831: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 25 11:51:38.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9881" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":311,"skipped":5332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSMay 25 11:51:38.851: INFO: Running AfterSuite actions on all nodes
May 25 11:51:38.852: INFO: Running AfterSuite actions on node 1
May 25 11:51:38.852: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5032.442 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h23m54.17821235s
Test Suite Passed
