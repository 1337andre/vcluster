<?xml version="1.0" encoding="UTF-8"?>
  <testsuite name="Kubernetes e2e suite" tests="311" failures="17" errors="0" time="8486.984">
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should not be ready with a docker exec readiness probe timeout " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should be able to schedule after more than 100 missed schedule" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl replace should update a single-container pod&#39;s image  [Conformance]" classname="Kubernetes e2e suite" time="14.929827548"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="21.03538567"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kube-controller-manager restarts should delete a bound PVC from a clientPod, restart the kube-control-manager, and ensure the kube-controller-manager does not crash" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should be able to mount directory &#39;adir&#39; successfully when HostPathType is HostPathUnset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should check kube-proxy urls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Metadata Concealment should run a check-metadata-concealment job to completion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] client-go should negotiate watch and report errors with accept &#34;application/vnd.kubernetes.protobuf,application/json&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when CSIDriver does not exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Upgrade [Feature:Upgrade] cluster upgrade should maintain a functioning cluster [Feature:ClusterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI attach test using mock driver should require VolumeAttach for drivers with attachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for old resource model [Feature:StackdriverCustomMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI NodeStage error cases [Slow] should retry NodeStage after NodeStage final error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with incompatible zone along with compatible storagePolicy and datastore combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] regular resource usage tracking [Feature:RegularResourceUsageTracking] resource tracking for 0 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should delete a collection of pods [Conformance]" classname="Kubernetes e2e suite" time="0.578730775"></testcase>
      <testcase name="[sig-api-machinery] API priority and fairness should ensure that requests can&#39;t be drowned out (fairness)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIServiceAccountToken [Feature:CSIServiceAccountToken] token should not be plumbed down when csiServiceAccountTokenEnabled=false" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should create and update a lease in the kube-node-lease namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should fail on mounting file &#39;afile&#39; when HostPathType is HostPathSocket" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should fail on mounting socket &#39;asocket&#39; when HostPathType is HostPathCharDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]" classname="Kubernetes e2e suite" time="155.112987324"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]" classname="Kubernetes e2e suite" time="1.5122915749999999"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="28.814938446"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] KubeProxy should resolve connection reset issue #74839 [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]" classname="Kubernetes e2e suite" time="192.188860406"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.608157264"></testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s multiple priority class scope (quota set to pod count: 2) against 2 pods with same priority classes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support volume SELinux relabeling when using hostIPC [Flaky] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]" classname="Kubernetes e2e suite" time="0.600473409"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Reboot [Disruptive] [Feature:Reboot] each node by switching off the network interface and ensure they function upon switch on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should support multiple inline ephemeral volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeFeature:RuntimeHandler]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:ClusterSizeAutoscalingScaleUp] [Slow] Autoscaling [sig-autoscaling] Autoscaling a service from 1 pod and 3 nodes to 8 pods and &gt;=4 nodes takes less than 15 minutes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Downgrade [Feature:Downgrade] cluster downgrade should maintain a functioning cluster [Feature:ClusterDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets Should fail non-optional pod creation due to secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]" classname="Kubernetes e2e suite" time="14.514481579"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="68.886795151"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should allow ingress access from updated pod [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] SCTP [Feature:SCTP] [LinuxOnly] should allow creating a basic SCTP service with pod and endpoints" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="244.298311535"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should fail to exceed backoffLimit" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]" classname="Kubernetes e2e suite" time="4.675003973"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies with delayed binding [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to the readonly kubelet port 10255 using proxy subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should be able to reach pod on ipv4 and ipv6 ip [Feature:IPv6DualStackAlphaFeature:Phase2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s cpu limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.621856774"></testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]" classname="Kubernetes e2e suite" time="6.944446021"></testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should be able to mount character device &#39;achardev&#39; successfully when HostPathType is HostPathCharDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]" classname="Kubernetes e2e suite" time="2.574350759"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Feature:Flexvolumes] Detaching volumes should not work when mount is in progress [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIStorageCapacity [Feature:CSIStorageCapacity] CSIStorageCapacity disabled" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should delete successful finished jobs with limit of one successful job" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] DNS horizontal autoscaling [DisabledForLargeClusters] kube-dns-autoscaler should scale kube-dns pods in both nonfaulty and faulty scenarios" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Shouldn&#39;t perform scale up operation and should list unhealthy status if most of the cluster is broken[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should have ipv4 and ipv6 node podCIDRs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working redis cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="125.805315886">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 13:56:37.666: Did not get expected responses within the timeout period of 120.00 seconds.&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/replica_set.go:95</failure>
          <system-out>[BeforeEach] [sig-apps] ReplicaSet&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 13:54:32.085: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename replicaset&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should serve a basic image on each replica with a public image  [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 13:54:32.566: INFO: Creating ReplicaSet my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#xA;May 18 13:54:32.589: INFO: Pod name my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Found 0 pods out of 1&#xA;May 18 13:54:37.601: INFO: Pod name my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Found 1 pods out of 1&#xA;May 18 13:54:37.601: INFO: Ensuring a pod for ReplicaSet &#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34; is running&#xA;May 18 13:54:37.608: INFO: Pod &#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j&#34; is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 13:54:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 13:54:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 13:54:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 13:54:32 +0000 UTC Reason: Message:}])&#xA;May 18 13:54:37.609: INFO: Trying to dial the pod&#xA;May 18 13:54:42.659: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:54:47.641: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:54:52.639: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:54:57.642: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:02.645: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:07.635: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:12.651: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:17.639: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:22.656: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:27.641: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:32.676: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:37.643: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:42.641: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:47.639: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:52.642: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:55:57.647: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:02.653: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:07.638: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:12.652: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:17.647: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:22.637: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:27.640: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:32.648: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:37.641: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:37.665: INFO: Controller my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: Failed to GET from replica 1 [my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j]: the server could not find the requested resource (get pods my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j)&#xA;pod status: v1.PodStatus{Phase:&#34;Running&#34;, Conditions:[]v1.PodCondition{v1.PodCondition{Type:&#34;Initialized&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;Ready&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;ContainersReady&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942873, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}, v1.PodCondition{Type:&#34;PodScheduled&#34;, Status:&#34;True&#34;, LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756942872, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;&#34;, Message:&#34;&#34;}}, Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;10.156.0.12&#34;, PodIP:&#34;10.104.1.26&#34;, PodIPs:[]v1.PodIP{v1.PodIP{IP:&#34;10.104.1.26&#34;}}, StartTime:(*v1.Time)(0xc002903180), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:&#34;my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#34;, State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029031a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:&#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34;, ImageID:&#34;k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a&#34;, ContainerID:&#34;containerd://7b03d5a4a76e71f1285921e758d80d013032dbffb034d694bc32d4ac9d1b37a5&#34;, Started:(*bool)(0xc002fb372b)}}, QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 13:56:37.666: FAIL: Did not get expected responses within the timeout period of 120.00 seconds.&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/apps.glob..func9.1()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/replica_set.go:95 +0x57&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] [sig-apps] ReplicaSet&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;replicaset-3209&#34;.&#xA;STEP: Found 5 events.&#xA;May 18 13:56:37.674: INFO: At 2021-05-18 13:54:32 +0000 UTC - event for my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad: {replicaset-controller } SuccessfulCreate: Created pod: my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j&#xA;May 18 13:56:37.674: INFO: At 2021-05-18 13:54:32 +0000 UTC - event for my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j: {default-scheduler } Scheduled: Successfully assigned replicaset-3209/my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 13:56:37.675: INFO: At 2021-05-18 13:54:33 +0000 UTC - event for my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 13:56:37.675: INFO: At 2021-05-18 13:54:33 +0000 UTC - event for my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#xA;May 18 13:56:37.675: INFO: At 2021-05-18 13:54:33 +0000 UTC - event for my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad&#xA;May 18 13:56:37.682: INFO: POD                                                           NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 13:56:37.682: INFO: my-hostname-basic-7dfa35aa-ce8e-4864-bfca-b64b67364dad-jv68j  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 13:54:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 13:54:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 13:54:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 13:54:32 +0000 UTC  }]&#xA;May 18 13:56:37.683: INFO: &#xA;May 18 13:56:37.691: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 13:56:37.697: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 1776 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 13:55:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 13:52:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 13:52:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 13:52:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 13:52:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 13:56:37.698: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 13:56:37.706: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 13:56:37.765: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 13:56:37.765: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 13:56:37.773: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 1777 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 13:54:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 13:54:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 13:54:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 13:54:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},ContainerImage{Names:[docker.io/library/alpine@sha256:08d6ca16c60fe7490c03d10dc339d9fd8ea67c6466dea8d558526b1330a85930 docker.io/library/alpine:3.13.1],SizeBytes:2814958,},ContainerImage{Names:[eu.gcr.io/google-containers/pause-amd64:3.0 gcr.io/google-containers/pause-amd64:3.0 asia.gcr.io/google-containers/pause-amd64:3.0],SizeBytes:312520,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 13:56:37.774: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 13:56:37.782: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 13:56:37.811: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 13:56:37.811: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 13:56:37.820: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 1778 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 13:55:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 13:52:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 13:52:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 13:52:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 13:52:03 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[asia.gcr.io/google-containers/pause-amd64:3.0 eu.gcr.io/google-containers/pause-amd64:3.0 gcr.io/google-containers/pause-amd64:3.0],SizeBytes:312520,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:299513,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 13:56:37.822: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 13:56:37.830: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 13:56:37.870: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 13:56:37.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;replicaset-3209&#34; for this suite.&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="60.504800643"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should add node to the particular mig [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="8.112720115"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname as non-root with fsgroup [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on allowed zones specified in storage class " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on localhost that expects a client request should support a client that connects, sends NO DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:vsphere][Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere [Feature:vsphere] should not detach and unmount PV when associated pvc with delete as reclaimPolicy is deleted when it is in use by the pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don&#39;t match the PodAntiAffinity terms" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] PodTopologySpread Filtering validates 4 pods with MaxSkew=1 are evenly distributed into 2 nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning errors [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage with delayed binding [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for new resource model [Feature:StackdriverCustomMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be immutable if `immutable` field is set" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s memory limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.573835247"></testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]" classname="Kubernetes e2e suite" time="6.450356267"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should support two pods which share the same volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should create read-only inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.643988837"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicaSet Should scale from 1 pod to 3 pods and from 3 to 5" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce multiple, stacked policies with overlapping podSelectors [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment should not disrupt a cloud load-balancer&#39;s connectivity during rollout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]" classname="Kubernetes e2e suite" time="30.79488145"></testcase>
      <testcase name="[k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.633517671"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap Should fail non-optional pod creation due to the key in the configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should release NodePorts on delete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]" classname="Kubernetes e2e suite" time="0.559558354"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy [Feature:PodSecurityPolicy] should allow pods under the privileged policy.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]" classname="Kubernetes e2e suite" time="0.552893323"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsNonRoot should run with an explicit non-root user ID [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.5485867"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should find a service from listing all namespaces [Conformance]" classname="Kubernetes e2e suite" time="0.484087828"></testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a container&#39;s command [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.6042547"></testcase>
      <testcase name="[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="153.578960952">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:01:04.755: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0008b6840&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31661 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31661 over TCP protocol&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3365</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 13:58:46.373: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename services&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745&#xA;[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating service in namespace services-1853&#xA;May 18 13:58:48.883: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode&#39;&#xA;May 18 13:58:49.132: INFO: stderr: &#34;+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n&#34;&#xA;May 18 13:58:49.132: INFO: stdout: &#34;iptables&#34;&#xA;May 18 13:58:49.132: INFO: proxyMode: iptables&#xA;May 18 13:58:49.150: INFO: Waiting for pod kube-proxy-mode-detector to disappear&#xA;May 18 13:58:49.158: INFO: Pod kube-proxy-mode-detector no longer exists&#xA;STEP: creating service affinity-nodeport-timeout in namespace services-1853&#xA;STEP: creating replication controller affinity-nodeport-timeout in namespace services-1853&#xA;May 18 13:58:52.360: INFO: Creating new exec pod&#xA;May 18 13:58:55.405: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80&#39;&#xA;May 18 13:58:56.658: INFO: rc: 1&#xA;May 18 13:58:56.658: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 affinity-nodeport-timeout 80&#xA;nc: connect to affinity-nodeport-timeout port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:58:57.658: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80&#39;&#xA;May 18 13:58:58.900: INFO: rc: 1&#xA;May 18 13:58:58.901: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 affinity-nodeport-timeout 80&#xA;nc: connect to affinity-nodeport-timeout port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:58:59.658: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80&#39;&#xA;May 18 13:58:59.885: INFO: stderr: &#34;+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 13:58:59.885: INFO: stdout: &#34;&#34;&#xA;May 18 13:58:59.886: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.8.167 80&#39;&#xA;May 18 13:59:00.109: INFO: stderr: &#34;+ nc -zv -t -w 2 10.108.8.167 80\nConnection to 10.108.8.167 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 13:59:00.109: INFO: stdout: &#34;&#34;&#xA;May 18 13:59:00.109: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:02.304: INFO: rc: 1&#xA;May 18 13:59:02.304: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:03.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:05.528: INFO: rc: 1&#xA;May 18 13:59:05.528: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:06.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:08.527: INFO: rc: 1&#xA;May 18 13:59:08.528: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:09.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:11.663: INFO: rc: 1&#xA;May 18 13:59:11.663: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:12.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:14.544: INFO: rc: 1&#xA;May 18 13:59:14.544: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:15.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:17.566: INFO: rc: 1&#xA;May 18 13:59:17.566: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:18.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:20.513: INFO: rc: 1&#xA;May 18 13:59:20.513: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:21.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:23.521: INFO: rc: 1&#xA;May 18 13:59:23.521: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:24.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:26.519: INFO: rc: 1&#xA;May 18 13:59:26.519: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:27.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:29.518: INFO: rc: 1&#xA;May 18 13:59:29.518: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:30.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:32.552: INFO: rc: 1&#xA;May 18 13:59:32.553: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:33.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:35.507: INFO: rc: 1&#xA;May 18 13:59:35.507: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:36.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:38.538: INFO: rc: 1&#xA;May 18 13:59:38.538: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:39.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:41.531: INFO: rc: 1&#xA;May 18 13:59:41.531: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:42.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:44.538: INFO: rc: 1&#xA;May 18 13:59:44.538: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:45.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:47.536: INFO: rc: 1&#xA;May 18 13:59:47.536: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:48.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:50.547: INFO: rc: 1&#xA;May 18 13:59:50.547: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:51.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:53.520: INFO: rc: 1&#xA;May 18 13:59:53.520: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:54.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:56.534: INFO: rc: 1&#xA;May 18 13:59:56.534: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 13:59:57.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 13:59:59.570: INFO: rc: 1&#xA;May 18 13:59:59.570: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:00.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:02.524: INFO: rc: 1&#xA;May 18 14:00:02.524: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:03.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:05.534: INFO: rc: 1&#xA;May 18 14:00:05.534: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:06.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:08.512: INFO: rc: 1&#xA;May 18 14:00:08.512: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:09.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:11.564: INFO: rc: 1&#xA;May 18 14:00:11.564: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:12.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:14.511: INFO: rc: 1&#xA;May 18 14:00:14.511: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:15.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:17.531: INFO: rc: 1&#xA;May 18 14:00:17.531: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:18.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:20.517: INFO: rc: 1&#xA;May 18 14:00:20.517: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:21.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:23.723: INFO: rc: 1&#xA;May 18 14:00:23.723: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:24.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:26.526: INFO: rc: 1&#xA;May 18 14:00:26.526: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:27.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:29.515: INFO: rc: 1&#xA;May 18 14:00:29.515: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:30.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:32.573: INFO: rc: 1&#xA;May 18 14:00:32.573: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:33.305: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:35.554: INFO: rc: 1&#xA;May 18 14:00:35.554: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:36.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:38.510: INFO: rc: 1&#xA;May 18 14:00:38.510: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:39.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:41.523: INFO: rc: 1&#xA;May 18 14:00:41.523: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:42.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:44.524: INFO: rc: 1&#xA;May 18 14:00:44.524: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:45.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:47.552: INFO: rc: 1&#xA;May 18 14:00:47.553: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:48.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:50.513: INFO: rc: 1&#xA;May 18 14:00:50.513: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:51.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:53.524: INFO: rc: 1&#xA;May 18 14:00:53.525: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:54.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:56.514: INFO: rc: 1&#xA;May 18 14:00:56.514: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:00:57.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:00:59.510: INFO: rc: 1&#xA;May 18 14:00:59.510: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:01:00.304: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:01:02.532: INFO: rc: 1&#xA;May 18 14:01:02.532: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:01:02.532: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661&#39;&#xA;May 18 14:01:04.755: INFO: rc: 1&#xA;May 18 14:01:04.755: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-1853 exec execpod-affinity4nj5c -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31661:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31661&#xA;nc: connect to 10.108.1.212 port 31661 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:01:04.755: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0008b6840&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31661 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31661 over TCP protocol&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.execAffinityTestForSessionAffinityTimeout(0xc000267a20, 0x5607080, 0xc001846c60, 0xc0011b1680)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3365 +0x751&#xA;k8s.io/kubernetes/test/e2e/network.glob..func24.29()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2469 +0x9c&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;May 18 14:01:04.756: INFO: Cleaning up the exec pod&#xA;STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1853, will wait for the garbage collector to delete the pods&#xA;May 18 14:01:04.859: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.729453ms&#xA;May 18 14:01:05.659: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 800.341468ms&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;services-1853&#34;.&#xA;STEP: Found 27 events.&#xA;May 18 14:01:19.740: INFO: At 2021-05-18 13:58:46 +0000 UTC - event for kube-proxy-mode-detector: {default-scheduler } Scheduled: Successfully assigned services-1853/kube-proxy-mode-detector to gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:01:19.741: INFO: At 2021-05-18 13:58:47 +0000 UTC - event for kube-proxy-mode-detector: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:01:19.741: INFO: At 2021-05-18 13:58:47 +0000 UTC - event for kube-proxy-mode-detector: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Created: Created container agnhost-container&#xA;May 18 14:01:19.741: INFO: At 2021-05-18 13:58:47 +0000 UTC - event for kube-proxy-mode-detector: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Started: Started container agnhost-container&#xA;May 18 14:01:19.741: INFO: At 2021-05-18 13:58:49 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-nqfmx&#xA;May 18 14:01:19.741: INFO: At 2021-05-18 13:58:49 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-9rx79&#xA;May 18 14:01:19.742: INFO: At 2021-05-18 13:58:49 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-85pkf&#xA;May 18 14:01:19.742: INFO: At 2021-05-18 13:58:49 +0000 UTC - event for affinity-nodeport-timeout-85pkf: {default-scheduler } Scheduled: Successfully assigned services-1853/affinity-nodeport-timeout-85pkf to gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:01:19.742: INFO: At 2021-05-18 13:58:49 +0000 UTC - event for affinity-nodeport-timeout-9rx79: {default-scheduler } Scheduled: Successfully assigned services-1853/affinity-nodeport-timeout-9rx79 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:01:19.742: INFO: At 2021-05-18 13:58:49 +0000 UTC - event for affinity-nodeport-timeout-nqfmx: {default-scheduler } Scheduled: Successfully assigned services-1853/affinity-nodeport-timeout-nqfmx to gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:01:19.742: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-85pkf: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Created: Created container affinity-nodeport-timeout&#xA;May 18 14:01:19.742: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-85pkf: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:01:19.743: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-85pkf: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Started: Started container affinity-nodeport-timeout&#xA;May 18 14:01:19.743: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-9rx79: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport-timeout&#xA;May 18 14:01:19.743: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-9rx79: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport-timeout&#xA;May 18 14:01:19.743: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-9rx79: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:01:19.743: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-nqfmx: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Created: Created container affinity-nodeport-timeout&#xA;May 18 14:01:19.744: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-nqfmx: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Started: Started container affinity-nodeport-timeout&#xA;May 18 14:01:19.744: INFO: At 2021-05-18 13:58:50 +0000 UTC - event for affinity-nodeport-timeout-nqfmx: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:01:19.744: INFO: At 2021-05-18 13:58:52 +0000 UTC - event for execpod-affinity4nj5c: {default-scheduler } Scheduled: Successfully assigned services-1853/execpod-affinity4nj5c to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:01:19.744: INFO: At 2021-05-18 13:58:53 +0000 UTC - event for execpod-affinity4nj5c: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:01:19.744: INFO: At 2021-05-18 13:58:53 +0000 UTC - event for execpod-affinity4nj5c: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost-container&#xA;May 18 14:01:19.744: INFO: At 2021-05-18 13:58:53 +0000 UTC - event for execpod-affinity4nj5c: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost-container&#xA;May 18 14:01:19.745: INFO: At 2021-05-18 14:01:05 +0000 UTC - event for affinity-nodeport-timeout: {endpoint-controller } FailedToUpdateEndpoint: Failed to update endpoint services-1853/affinity-nodeport-timeout: Operation cannot be fulfilled on endpoints &#34;affinity-nodeport-timeout&#34;: the object has been modified; please apply your changes to the latest version and try again&#xA;May 18 14:01:19.745: INFO: At 2021-05-18 14:01:05 +0000 UTC - event for affinity-nodeport-timeout-85pkf: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Killing: Stopping container affinity-nodeport-timeout&#xA;May 18 14:01:19.745: INFO: At 2021-05-18 14:01:05 +0000 UTC - event for affinity-nodeport-timeout-9rx79: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport-timeout&#xA;May 18 14:01:19.745: INFO: At 2021-05-18 14:01:05 +0000 UTC - event for affinity-nodeport-timeout-nqfmx: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Killing: Stopping container affinity-nodeport-timeout&#xA;May 18 14:01:19.753: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 14:01:19.753: INFO: &#xA;May 18 14:01:19.761: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:01:19.768: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 2364 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:00:01 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 13:57:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 13:57:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 13:57:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 13:57:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:01:19.769: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:01:19.779: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:01:19.815: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:01:19.815: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:01:19.823: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 2365 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:00:02 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 13:59:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 13:59:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 13:59:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 13:59:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},ContainerImage{Names:[docker.io/library/alpine@sha256:08d6ca16c60fe7490c03d10dc339d9fd8ea67c6466dea8d558526b1330a85930 docker.io/library/alpine:3.13.1],SizeBytes:2814958,},ContainerImage{Names:[docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/library/busybox:1.29],SizeBytes:732685,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:01:19.824: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:01:19.832: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:01:19.881: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:01:19.881: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:01:19.891: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 2366 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:00:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 13:57:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 13:57:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 13:57:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 13:57:05 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[asia.gcr.io/google-containers/pause-amd64:3.0 eu.gcr.io/google-containers/pause-amd64:3.0 gcr.io/google-containers/pause-amd64:3.0],SizeBytes:312520,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:299513,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:01:19.891: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:01:19.898: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:01:19.932: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:01:19.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;services-1853&#34; for this suite.&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should not provision a volume in an unmanaged GCE zone." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should fail on mounting character device &#39;achardev&#39; when HostPathType is HostPathFile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the signed bootstrap tokens from clusterInfo ConfigMap when bootstrap token is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide container&#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.554403548"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] host cleanup with volume mounts [sig-storage][HostCleanup][Flaky] Host cleanup after disrupting NFS volume [NFS] after stopping the nfs-server and deleting the (sleeping) client pod, the NFS mount and the pod&#39;s UID directory should be removed." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should create and delete default persistent volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="32.051147975"></testcase>
      <testcase name="[sig-storage] Secrets should be immutable if `immutable` field is set" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] crictl should be able to run crictl on the node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl apply apply set/view last-applied" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] Deployment Should scale from 1 pod to 3 pods and from 3 to 5" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Reboot [Disruptive] [Feature:Reboot] each node by triggering kernel panic and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should work after restarting apiserver [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should not delete the token secret when the secret is not expired" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Verify Volume Attach Through vpxd Restart [Feature:vsphere][Serial][Disruptive] verify volume remains attached through vpxd restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion should not expand volume if resizingOnDriver=off, resizingOnSC=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for node-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in storage class " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NoSNAT [Feature:NoSNAT] [Slow] Should be able to send traffic between Pods without SNAT" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a SPBM policy is not honored on a non-compatible datastore for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="6.590999633"></testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]" classname="Kubernetes e2e suite" time="101.251788446"></testcase>
      <testcase name="[sig-storage] CSI mock volume CSI attach test using mock driver should not require VolumeAttach for drivers without attachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned Snapshot (delete policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify dynamically created pv with multiple zones specified in the storage class, shows both the zones on its labels" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic Snapshot (delete policy)] snapshottable-stress[Feature:VolumeSnapshotDataSource] should support snapshotting of many volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.694636023"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should fail on mounting directory &#39;adir&#39; when HostPathType is HostPathBlockDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Upgrade [Feature:Upgrade] node upgrade should maintain a functioning cluster [Feature:NodeUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl client-side validation should create/apply a valid CR for CRD with validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should stop enforcing policies after they are deleted [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]" classname="Kubernetes e2e suite" time="4.02431005"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 should support forwarding over websockets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Density [Serial] [Slow] create a batch of pods latency/resource should be within limit when create 10 pods with 0s interval" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider] [Feature:CloudProvider][Disruptive] Nodes should be deleted on API server if it doesn&#39;t exist in the cloud provider" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should handle load balancer cleanup finalizer for service [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints for both Ingress-referenced NEG and standalone NEG" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should support basic nodePort: udp functionality" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion should expand volume by restarting pod if attach=off, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.556673206"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Etcd failure [Disruptive] should recover from SIGKILL" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should support two pods which share the same volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="12.64422125"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.612433363"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.6505599650000002"></testcase>
      <testcase name="[k8s.io] [sig-node] Pod garbage collector [Feature:PodGarbageCollector] [Slow] should handle the creation of 1000 pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [StatefulSet] should come back up if node goes down [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret Should fail non-optional pod creation due to secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for external metrics [Feature:StackdriverExternalMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeFeature:RuntimeHandler]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Services should be able to create a functioning NodePort service for Windows" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Downward API [Serial] [Disruptive] [NodeFeature:EphemeralStorage] Downward API tests for local ephemeral storage should provide default limits.ephemeral-storage from node allocatable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: sctp [LinuxOnly][Feature:SCTPConnectivity][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume limits should verify that all nodes have volume limits" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against 2 pods with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.590975535"></testcase>
      <testcase name="[sig-apps] Job should fail when exceeds active deadline" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created on a non-Workspace zone and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.590590561"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should support a &#39;default-deny-ingress&#39; policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should create service with ipv6,v4 cluster ip [Feature:IPv6DualStackAlphaFeature:Phase2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should fail on mounting block device &#39;ablkdev&#39; when HostPathType is HostPathSocket" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.5189178519999995"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should fail on mounting directory &#39;adir&#39; when HostPathType is HostPathFile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]" classname="Kubernetes e2e suite" time="16.728917024"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 4 PVs and 2 PVCs: test write access [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] A node shouldn&#39;t be able to delete another node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] HA-master [Feature:HAMaster] survive addition/removal replicas same zone [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver Forward PTR lookup should forward PTR records lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should fail on mounting non-existent file &#39;does-not-exist-file&#39; when HostPathType is HostPathFile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] API priority and fairness should ensure that requests can be classified by testing flow-schemas/priority-levels" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should have their auto-restart back-off timer reset on image update [Slow][NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.543816428"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting EmptyDir volume is pending [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.600496646"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]" classname="Kubernetes e2e suite" time="6.706468872"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Secret should create a pod that reads a secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should check NodePort out-of-range" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Reboot [Disruptive] [Feature:Reboot] each node by dropping all outbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should handle in-cluster config" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]" classname="Kubernetes e2e suite" time="0.541537392"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment iterative rollouts should eventually progress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.633169063"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to pod anti-affinity [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]" classname="Kubernetes e2e suite" time="30.209252542"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]" classname="Kubernetes e2e suite" time="6.419815544"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation fails if only storage policy is specified in the storage class (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion should expand volume by restarting pod if attach=on, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should fail on mounting character device &#39;achardev&#39; when HostPathType is HostPathBlockDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]" classname="Kubernetes e2e suite" time="0.601247196"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] PreStop graceful pod terminated should wait until preStop hook completes the process" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]" classname="Kubernetes e2e suite" time="3.617016374"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid hostFailuresToTolerate and cacheReservation values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should schedule multiple jobs concurrently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] health handlers should contain necessary checks" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.6026254939999998"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should create read-only inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should report node status infrequently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Cpu Resources [Serial] Container limits should not be exceeded after waiting 2 minutes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] Pods should be evicted from unready Node [Feature:TaintEviction] All pods on the unreachable node should be marked as NotReady upon the node turn NotReady AND all pods should be evicted after eviction timeout passes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]" classname="Kubernetes e2e suite" time="0.740450283"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a PVC creation fails when multiple zones are specified in the storage class without shared datastores among the zones in waitForFirstConsumer binding mode" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t trigger additional scale-ups during processing scale-up [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret Should fail non-optional pod creation due to the key in the secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.722703993"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an existing and compatible SPBM policy is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI volume limit information using mock driver should report attach limit when limit is bigger than 0 [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]" classname="Kubernetes e2e suite" time="11.599441272"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for pod-Service: sctp [Feature:SCTPConnectivity][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv6][Experimental][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with privileged should run the container as privileged when true [LinuxOnly] [NodeFeature:HostAccess]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl create quota should create a quota without scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should not be ready until startupProbe succeeds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] AppArmor load AppArmor profiles can disable an AppArmor profile, using unconfined" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] single and multi-cluster ingresses should be able to exist together" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=false" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-ui] Kubernetes Dashboard [Feature:Dashboard] should check that the kubernetes-dashboard instance is alive" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="7.893007439"></testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s cpu limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.570746166"></testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s memory limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.571403222"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]" classname="Kubernetes e2e suite" time="92.264692156"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against 2 pods with different priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]" classname="Kubernetes e2e suite" time="6.8778363670000005"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]" classname="Kubernetes e2e suite" time="73.274068125"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should create service with ipv6 cluster ip [Feature:IPv6DualStackAlphaFeature:Phase2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]" classname="Kubernetes e2e suite" time="2.830764209"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a custom resource." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for pod-Service: sctp [Feature:SCTPConnectivity][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]" classname="Kubernetes e2e suite" time="5.5904011780000005"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with non-default reclaim policy Retain" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]" classname="Kubernetes e2e suite" time="0.488252885"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with spbm policy on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.580605313"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod&#39;s predecessor fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should recreate pods scheduled on the unreachable node AND allow scheduling of pods on a node after it rejoins the cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s memory request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.609517494"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to create a ClusterIP service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]" classname="Kubernetes e2e suite" time="10.459152857"></testcase>
      <testcase name="[sig-network] DNS should support configurable pod resolv.conf" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] gpu Upgrade [Feature:GPUUpgrade] cluster upgrade should be able to run gpu pod after upgrade [Feature:GPUClusterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]" classname="Kubernetes e2e suite" time="17.209689813"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Discovery Custom resource should have storage version hash" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify dynamically created pv with allowed zones specified in storage class, shows the right zone information on its labels" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] experimental resource usage tracking [Feature:ExperimentalResourceUsageTracking] resource tracking for 100 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="48.128653985"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.617980675"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with pre-shared certificate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with default parameter on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv4]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for client IP based session affinity: udp [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Pod Container lifecycle should not create extra sandbox if all containers are done" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation fails if the availability zone specified in the storage class have no shared datastores under it." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a non-existing SPBM policy is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Feature:Flexvolumes] Mounted flexvolume expand[Slow] Should verify mounted flex volumes can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.566324276"></testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Downward API should create a pod that prints his name and namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Kube-proxy should recover after being killed accidentally" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]" classname="Kubernetes e2e suite" time="7.592339293"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement [Feature:vsphere] should create and delete pod with multiple volumes from different datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.555636129"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="3.085292367"></testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should update endpoints: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should delete failed finished jobs with limit of one job" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy API should support creating NetworkPolicy API operations" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="22.802959217"></testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale down empty nodes [Feature:ClusterAutoscalerScalability3]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] doesn&#39;t evict pod with tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod fails to get scheduled when conflicting volume topology (allowedTopologies) and pod scheduling constraints(nodeSelector) are specified" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]" classname="Kubernetes e2e suite" time="0.643910305"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.7223653070000005"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:vsphere][Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere [Feature:vsphere] should retain persistent volume when reclaimPolicy set to retain when associated claim is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify invalid fstype" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Lease lease API should be available [Conformance]" classname="Kubernetes e2e suite" time="0.596429999"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIStorageCapacity [Feature:CSIStorageCapacity] CSIStorageCapacity used, no capacity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="12.612670519"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to update service type to NodePort listening on same port number but different protocols" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.630525974"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should remove pods when job is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]" classname="Kubernetes e2e suite" time="5.076490576"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should create read-only inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage =&gt; should not allow an eviction [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default command and arguments [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.718696685"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="151.692028664"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for services  [Conformance]" classname="Kubernetes e2e suite" time="10.397945912"></testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Nodes [Disruptive] Resize [Slow] should be able to delete nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [Feature:SCTPConnectivity][LinuxOnly][Disruptive] NetworkPolicy between server and client using SCTP should enforce policy based on Ports [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]" classname="Kubernetes e2e suite" time="4.751917091"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Kubelet-Stats [Serial] Kubelet stats collection for Windows nodes when running 10 pods should return within 10 seconds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="5.372473164"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy [Feature:PodSecurityPolicy] should forbid pod creation when no PSP is available" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]" classname="Kubernetes e2e suite" time="15.72758371"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 4 containers and 1 PDs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] API priority and fairness should ensure that requests can&#39;t be drowned out (priority)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.600843257"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]" classname="Kubernetes e2e suite" time="14.539066404"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.579766699"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group up from 0[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] stateful Upgrade [Feature:StatefulUpgrade] [k8s.io] stateful upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:vsphere][Feature:LabelSelector] [sig-storage] Selector-Label Volume Binding:vsphere [Feature:vsphere] should bind volume with claim for given label" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted by liveness probe after startup probe enables it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pods sharing a single local PV [Serial] all pods should be running" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support volume SELinux relabeling when using hostPID [Flaky] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should resign the bootstrap tokens when the clusterInfo ConfigMap updated [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for node-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Ports Security Check [Feature:KubeletSecurity] should not have port 10255 open on its all public IP addresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should get a host IP [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.58926274"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks should be able to delete a non-existent PD without error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner External should let an external dynamic provisioner create and delete persistent volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] shouldn&#39;t scale down with underutilized nodes due to host port conflicts [Feature:ClusterAutoscalerScalability5]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]" classname="Kubernetes e2e suite" time="10.159068783"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for pod-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.787429"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]" classname="Kubernetes e2e suite" time="301.119551084">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:20:27.209: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:404</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:15:26.460: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename sched-pred&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92&#xA;May 18 14:15:26.921: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready&#xA;May 18 14:15:26.940: INFO: Waiting for terminating namespaces to be deleted...&#xA;May 18 14:15:26.947: INFO: &#xA;Logging pods the apiserver thinks is on node gke-cluster-2-default-pool-426afc83-4s72 before test&#xA;May 18 14:15:26.956: INFO: sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-46lgd from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 14:15:26.956: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;May 18 14:15:26.956: INFO: &#x9;Container systemd-logs ready: true, restart count 0&#xA;May 18 14:15:26.956: INFO: sonobuoy from sonobuoy started at 2021-05-18 13:41:17 +0000 UTC (1 container statuses recorded)&#xA;May 18 14:15:26.956: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;May 18 14:15:26.956: INFO: &#xA;Logging pods the apiserver thinks is on node gke-cluster-2-default-pool-426afc83-0shb before test&#xA;May 18 14:15:26.965: INFO: coredns-854c77959c-62dp5 from kube-system started at 2021-05-18 13:39:44 +0000 UTC (1 container statuses recorded)&#xA;May 18 14:15:26.965: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 18 14:15:26.965: INFO: sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-rxm4c from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 14:15:26.965: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;May 18 14:15:26.965: INFO: &#x9;Container systemd-logs ready: true, restart count 0&#xA;May 18 14:15:26.965: INFO: sonobuoy-e2e-job-495605ea5c7c42c0 from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 14:15:26.965: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;May 18 14:15:26.965: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;May 18 14:15:26.965: INFO: &#xA;Logging pods the apiserver thinks is on node gke-cluster-2-default-pool-426afc83-rll8 before test&#xA;May 18 14:15:26.974: INFO: sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-gcqgt from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 14:15:26.974: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;May 18 14:15:26.974: INFO: &#x9;Container systemd-logs ready: true, restart count 0&#xA;May 18 14:15:26.974: INFO: pod-configmaps-c7b91639-eee8-4a50-992e-457a41b7dbd5 from configmap-5138 started at 2021-05-18 14:15:22 +0000 UTC (3 container statuses recorded)&#xA;May 18 14:15:26.974: INFO: &#x9;Container createcm-volume-test ready: true, restart count 0&#xA;May 18 14:15:26.974: INFO: &#x9;Container delcm-volume-test ready: true, restart count 0&#xA;May 18 14:15:26.974: INFO: &#x9;Container updcm-volume-test ready: true, restart count 0&#xA;[It] validates resource limits of pods that are allowed to run  [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: verifying the node has the label node gke-cluster-2-default-pool-426afc83-4s72&#xA;STEP: verifying the node has the label node gke-cluster-2-default-pool-426afc83-0shb&#xA;STEP: verifying the node has the label node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:15:27.068: INFO: Pod coredns-854c77959c-62dp5 requesting resource cpu=100m on Node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:15:27.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-46lgd requesting resource cpu=0m on Node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:15:27.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-gcqgt requesting resource cpu=0m on Node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:15:27.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-rxm4c requesting resource cpu=0m on Node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:15:27.068: INFO: Pod sonobuoy-e2e-job-495605ea5c7c42c0 requesting resource cpu=0m on Node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:15:27.068: INFO: Pod sonobuoy requesting resource cpu=0m on Node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:15:27.068: INFO: Pod pod-configmaps-c7b91639-eee8-4a50-992e-457a41b7dbd5 requesting resource cpu=0m on Node gke-cluster-2-default-pool-426afc83-rll8&#xA;STEP: Starting Pods to consume most of the cluster CPU.&#xA;May 18 14:15:27.068: INFO: Creating a pod which consumes cpu=588m on Node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:15:27.105: INFO: Creating a pod which consumes cpu=658m on Node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:15:27.130: INFO: Creating a pod which consumes cpu=658m on Node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:20:27.209: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/scheduling.glob..func4.5()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:404 +0xd07&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;STEP: removing the label node off the node gke-cluster-2-default-pool-426afc83-4s72&#xA;STEP: verifying the node doesn&#39;t have the label node&#xA;STEP: removing the label node off the node gke-cluster-2-default-pool-426afc83-0shb&#xA;STEP: verifying the node doesn&#39;t have the label node&#xA;STEP: removing the label node off the node gke-cluster-2-default-pool-426afc83-rll8&#xA;STEP: verifying the node doesn&#39;t have the label node&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;sched-pred-4803&#34;.&#xA;STEP: Found 6 events.&#xA;May 18 14:20:27.348: INFO: At 2021-05-18 14:15:27 +0000 UTC - event for filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709: {default-scheduler } Scheduled: Successfully assigned sched-pred-4803/filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:20:27.348: INFO: At 2021-05-18 14:15:27 +0000 UTC - event for filler-pod-3415bcd5-d064-4abd-89fe-9105ff35cb49: {default-scheduler } FailedScheduling: 0/3 nodes are available: 1 Insufficient cpu, 2 node(s) didn&#39;t match Pod&#39;s node affinity.&#xA;May 18 14:20:27.348: INFO: At 2021-05-18 14:15:27 +0000 UTC - event for filler-pod-62f0f380-460a-45bd-ac67-b32a48ff78f7: {default-scheduler } FailedScheduling: 0/3 nodes are available: 1 Insufficient cpu, 2 node(s) didn&#39;t match Pod&#39;s node affinity.&#xA;May 18 14:20:27.348: INFO: At 2021-05-18 14:15:28 +0000 UTC - event for filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/pause:3.2&#34; already present on machine&#xA;May 18 14:20:27.348: INFO: At 2021-05-18 14:15:28 +0000 UTC - event for filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709&#xA;May 18 14:20:27.348: INFO: At 2021-05-18 14:15:28 +0000 UTC - event for filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709&#xA;May 18 14:20:27.357: INFO: POD                                              NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 14:20:27.357: INFO: filler-pod-62f0f380-460a-45bd-ac67-b32a48ff78f7                                            Pending         [{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:15:27 +0000 UTC Unschedulable 0/3 nodes are available: 1 Insufficient cpu, 2 node(s) didn&#39;t match Pod&#39;s node affinity.}]&#xA;May 18 14:20:27.357: INFO: filler-pod-1da44731-622e-46f7-aba2-dd3124ee7709  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:15:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:15:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:15:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:15:27 +0000 UTC  }]&#xA;May 18 14:20:27.357: INFO: filler-pod-3415bcd5-d064-4abd-89fe-9105ff35cb49                                            Pending         [{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:15:27 +0000 UTC Unschedulable 0/3 nodes are available: 3 node(s) didn&#39;t match Pod&#39;s node affinity.}]&#xA;May 18 14:20:27.357: INFO: &#xA;May 18 14:20:27.368: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:20:27.377: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 5703 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:20:27.377: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:20:27.386: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:20:27.445: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:20:27.445: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:20:27.454: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 5704 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:17:40 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:20:27.455: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:20:27.463: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:20:27.504: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:20:27.504: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:20:27.513: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 5705 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:20:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:20:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:20:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:20:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:20:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:20:27.514: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:20:27.523: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:20:27.560: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:20:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;sched-pred-4803&#34; for this suite.&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Container restart should verify that container can restart successfully after configmaps modified" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.76980089"></testcase>
      <testcase name="[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp unconfined on the pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment reaping should cascade to its replica sets and pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]" classname="Kubernetes e2e suite" time="19.911127969"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should support InClusterConfig with token rotation [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 that expects a client request should support a client that connects, sends NO DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.554140916"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should allow ingress access from namespace on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should support configurable pod DNS nameservers [Conformance]" classname="Kubernetes e2e suite" time="2.831854046"></testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]" classname="Kubernetes e2e suite" time="2.603908795"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.699566746"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes pod should support memory backed volumes of specified size" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should be able to mount directory &#39;adir&#39; successfully when HostPathType is HostPathDirectory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for endpoint-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [Feature:SCTPConnectivity][LinuxOnly][Disruptive] NetworkPolicy between server and client using SCTP should enforce policy to allow traffic only from a pod in a different namespace based on PodSelector and NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] SSH should SSH to all nodes and run commands" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify &#34;immediate&#34; deletion of a PVC that is not in active use by a pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should allow pods to hairpin back to themselves through services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should support exec through kubectl proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing secret should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should delete old replica sets [Conformance]" classname="Kubernetes e2e suite" time="5.065428136"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]" classname="Kubernetes e2e suite" time="14.976729868"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic Snapshot (retain policy)] snapshottable-stress[Feature:VolumeSnapshotDataSource] should support snapshotting of many volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]" classname="Kubernetes e2e suite" time="1.566548182"></testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should support multiple inline ephemeral volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vsphere cloud provider stress [Feature:vsphere] vsphere stress tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]" classname="Kubernetes e2e suite" time="0.551351979"></testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] Pods should return to running and ready state after network partition is healed All pods on the unreachable node should be marked as NotReady upon the node turn NotReady AND all pods should be mark back to Ready when the node get back to Ready before pod eviction timeout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should replace jobs when ReplaceConcurrent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should fail on mounting file &#39;afile&#39; when HostPathType is HostPathCharDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t increase cluster size if pending pod is too large [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]" classname="Kubernetes e2e suite" time="1.041562241"></testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should fail on mounting non-existent character device &#39;does-not-exist-char-dev&#39; when HostPathType is HostPathCharDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.730299655"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should disable node pool autoscaling [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]" classname="Kubernetes e2e suite" time="611.970076292">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:31:59.190: Frontend service did not start serving content in 600 seconds.&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:378</failure>
          <system-out>[BeforeEach] [sig-cli] Kubectl client&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:21:48.938: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename kubectl&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-cli] Kubectl client&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247&#xA;[It] should create and stop a working application  [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating all guestbook components&#xA;May 18 14:21:49.395: INFO: apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: agnhost-replica&#xA;  labels:&#xA;    app: agnhost&#xA;    role: replica&#xA;    tier: backend&#xA;spec:&#xA;  ports:&#xA;  - port: 6379&#xA;  selector:&#xA;    app: agnhost&#xA;    role: replica&#xA;    tier: backend&#xA;&#xA;May 18 14:21:49.395: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 create -f -&#39;&#xA;May 18 14:21:49.844: INFO: stderr: &#34;&#34;&#xA;May 18 14:21:49.844: INFO: stdout: &#34;service/agnhost-replica created\n&#34;&#xA;May 18 14:21:49.844: INFO: apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: agnhost-primary&#xA;  labels:&#xA;    app: agnhost&#xA;    role: primary&#xA;    tier: backend&#xA;spec:&#xA;  ports:&#xA;  - port: 6379&#xA;    targetPort: 6379&#xA;  selector:&#xA;    app: agnhost&#xA;    role: primary&#xA;    tier: backend&#xA;&#xA;May 18 14:21:49.844: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 create -f -&#39;&#xA;May 18 14:21:50.260: INFO: stderr: &#34;&#34;&#xA;May 18 14:21:50.260: INFO: stdout: &#34;service/agnhost-primary created\n&#34;&#xA;May 18 14:21:50.260: INFO: apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: frontend&#xA;  labels:&#xA;    app: guestbook&#xA;    tier: frontend&#xA;spec:&#xA;  # if your cluster supports it, uncomment the following to automatically create&#xA;  # an external load-balanced IP for the frontend service.&#xA;  # type: LoadBalancer&#xA;  ports:&#xA;  - port: 80&#xA;  selector:&#xA;    app: guestbook&#xA;    tier: frontend&#xA;&#xA;May 18 14:21:50.260: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 create -f -&#39;&#xA;May 18 14:21:50.570: INFO: stderr: &#34;&#34;&#xA;May 18 14:21:50.570: INFO: stdout: &#34;service/frontend created\n&#34;&#xA;May 18 14:21:50.570: INFO: apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: frontend&#xA;spec:&#xA;  replicas: 3&#xA;  selector:&#xA;    matchLabels:&#xA;      app: guestbook&#xA;      tier: frontend&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: guestbook&#xA;        tier: frontend&#xA;    spec:&#xA;      containers:&#xA;      - name: guestbook-frontend&#xA;        image: k8s.gcr.io/e2e-test-images/agnhost:2.21&#xA;        args: [ &#34;guestbook&#34;, &#34;--backend-port&#34;, &#34;6379&#34; ]&#xA;        resources:&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        ports:&#xA;        - containerPort: 80&#xA;&#xA;May 18 14:21:50.571: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 create -f -&#39;&#xA;May 18 14:21:50.852: INFO: stderr: &#34;&#34;&#xA;May 18 14:21:50.852: INFO: stdout: &#34;deployment.apps/frontend created\n&#34;&#xA;May 18 14:21:50.852: INFO: apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: agnhost-primary&#xA;spec:&#xA;  replicas: 1&#xA;  selector:&#xA;    matchLabels:&#xA;      app: agnhost&#xA;      role: primary&#xA;      tier: backend&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: agnhost&#xA;        role: primary&#xA;        tier: backend&#xA;    spec:&#xA;      containers:&#xA;      - name: primary&#xA;        image: k8s.gcr.io/e2e-test-images/agnhost:2.21&#xA;        args: [ &#34;guestbook&#34;, &#34;--http-port&#34;, &#34;6379&#34; ]&#xA;        resources:&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        ports:&#xA;        - containerPort: 6379&#xA;&#xA;May 18 14:21:50.852: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 create -f -&#39;&#xA;May 18 14:21:51.294: INFO: stderr: &#34;&#34;&#xA;May 18 14:21:51.294: INFO: stdout: &#34;deployment.apps/agnhost-primary created\n&#34;&#xA;May 18 14:21:51.294: INFO: apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: agnhost-replica&#xA;spec:&#xA;  replicas: 2&#xA;  selector:&#xA;    matchLabels:&#xA;      app: agnhost&#xA;      role: replica&#xA;      tier: backend&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: agnhost&#xA;        role: replica&#xA;        tier: backend&#xA;    spec:&#xA;      containers:&#xA;      - name: replica&#xA;        image: k8s.gcr.io/e2e-test-images/agnhost:2.21&#xA;        args: [ &#34;guestbook&#34;, &#34;--replicaof&#34;, &#34;agnhost-primary&#34;, &#34;--http-port&#34;, &#34;6379&#34; ]&#xA;        resources:&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        ports:&#xA;        - containerPort: 6379&#xA;&#xA;May 18 14:21:51.294: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 create -f -&#39;&#xA;May 18 14:21:51.774: INFO: stderr: &#34;&#34;&#xA;May 18 14:21:51.774: INFO: stdout: &#34;deployment.apps/agnhost-replica created\n&#34;&#xA;STEP: validating guestbook app&#xA;May 18 14:21:51.774: INFO: Waiting for all frontend pods to be Running.&#xA;May 18 14:21:56.825: INFO: Waiting for frontend to serve content.&#xA;May 18 14:21:56.906: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:01.923: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:06.942: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:11.959: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:16.977: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:21.993: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:27.013: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:32.028: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:37.049: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:42.072: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:47.108: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:52.128: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:22:57.153: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:02.172: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:07.190: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:12.207: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:17.224: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:22.241: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:27.265: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:32.281: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:37.301: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:42.317: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:47.336: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:52.358: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:23:57.376: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:02.393: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:07.408: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:12.425: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:17.440: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:22.459: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:27.491: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:32.514: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:37.531: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:42.550: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:47.567: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:52.596: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:24:57.616: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:02.631: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:07.644: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:12.671: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:17.690: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:22.709: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:27.729: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:32.748: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:37.768: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:42.788: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:47.807: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:52.826: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:25:57.848: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:02.867: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:07.889: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:12.908: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:17.924: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:22.944: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:27.962: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:32.981: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:38.003: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:43.024: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:48.046: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:53.064: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:26:58.085: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:03.101: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:08.120: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:13.135: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:18.161: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:23.177: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:28.192: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:33.210: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:38.228: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:43.281: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:48.296: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:53.314: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:27:58.330: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:03.348: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:08.362: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:13.376: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:18.391: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:23.408: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:28.428: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:33.446: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:38.465: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:43.482: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:48.500: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:53.529: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:28:58.545: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:03.560: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:08.580: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:13.599: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:18.620: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:23.637: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:28.656: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:33.672: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:38.692: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:43.710: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:48.725: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:53.743: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:29:58.764: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:03.781: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:08.796: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:13.830: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:18.851: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:23.866: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:28.881: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:33.898: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:38.920: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:43.937: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:48.952: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:53.982: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:30:58.997: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:04.014: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:09.029: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:14.047: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:19.075: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:24.094: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:29.109: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:34.125: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:39.140: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:44.159: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:49.174: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:54.189: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s�&#xA;�&#xA;�v1��Status�q&#xA;�&#xA;�������Failure�Gno endpoints available for service &#34;frontend-x-kubectl-1653-x-vcluster&#34;&#34;�ServiceUnavailable0����&#34;�&#xA;May 18 14:31:59.190: FAIL: Frontend service did not start serving content in 600 seconds.&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/kubectl.glob..func1.7.2()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:378 +0x159&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;STEP: using delete to clean up resources&#xA;May 18 14:31:59.191: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 delete --grace-period=0 --force -f -&#39;&#xA;May 18 14:31:59.797: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 18 14:31:59.797: INFO: stdout: &#34;service \&#34;agnhost-replica\&#34; force deleted\n&#34;&#xA;STEP: using delete to clean up resources&#xA;May 18 14:31:59.797: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 delete --grace-period=0 --force -f -&#39;&#xA;May 18 14:32:00.013: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 18 14:32:00.013: INFO: stdout: &#34;service \&#34;agnhost-primary\&#34; force deleted\n&#34;&#xA;STEP: using delete to clean up resources&#xA;May 18 14:32:00.013: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 delete --grace-period=0 --force -f -&#39;&#xA;May 18 14:32:00.145: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 18 14:32:00.145: INFO: stdout: &#34;service \&#34;frontend\&#34; force deleted\n&#34;&#xA;STEP: using delete to clean up resources&#xA;May 18 14:32:00.145: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 delete --grace-period=0 --force -f -&#39;&#xA;May 18 14:32:00.289: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 18 14:32:00.289: INFO: stdout: &#34;deployment.apps \&#34;frontend\&#34; force deleted\n&#34;&#xA;STEP: using delete to clean up resources&#xA;May 18 14:32:00.290: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 delete --grace-period=0 --force -f -&#39;&#xA;May 18 14:32:00.445: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 18 14:32:00.445: INFO: stdout: &#34;deployment.apps \&#34;agnhost-primary\&#34; force deleted\n&#34;&#xA;STEP: using delete to clean up resources&#xA;May 18 14:32:00.445: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=kubectl-1653 delete --grace-period=0 --force -f -&#39;&#xA;May 18 14:32:00.579: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 18 14:32:00.579: INFO: stdout: &#34;deployment.apps \&#34;agnhost-replica\&#34; force deleted\n&#34;&#xA;[AfterEach] [sig-cli] Kubectl client&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;kubectl-1653&#34;.&#xA;STEP: Found 39 events.&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:50 +0000 UTC - event for frontend: {deployment-controller } ScalingReplicaSet: Scaled up replica set frontend-7659f66489 to 3&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:50 +0000 UTC - event for frontend-7659f66489: {replicaset-controller } SuccessfulCreate: Created pod: frontend-7659f66489-qgvzh&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:50 +0000 UTC - event for frontend-7659f66489: {replicaset-controller } SuccessfulCreate: Created pod: frontend-7659f66489-nck46&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:50 +0000 UTC - event for frontend-7659f66489: {replicaset-controller } SuccessfulCreate: Created pod: frontend-7659f66489-5778n&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:50 +0000 UTC - event for frontend-7659f66489-5778n: {default-scheduler } Scheduled: Successfully assigned kubectl-1653/frontend-7659f66489-5778n to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:50 +0000 UTC - event for frontend-7659f66489-qgvzh: {default-scheduler } Scheduled: Successfully assigned kubectl-1653/frontend-7659f66489-qgvzh to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-primary: {deployment-controller } ScalingReplicaSet: Scaled up replica set agnhost-primary-56857545d9 to 1&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-primary-56857545d9: {replicaset-controller } SuccessfulCreate: Created pod: agnhost-primary-56857545d9-9pgtc&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-primary-56857545d9-9pgtc: {pod-syncer } SyncError: Error syncing to physical cluster: Operation cannot be fulfilled on pods &#34;agnhost-primary-56857545d9-9pgtc-x-kubectl-1653-x-vcluster&#34;: the object has been modified; please apply your changes to the latest version and try again&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-primary-56857545d9-9pgtc: {default-scheduler } Scheduled: Successfully assigned kubectl-1653/agnhost-primary-56857545d9-9pgtc to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-replica: {deployment-controller } ScalingReplicaSet: Scaled up replica set agnhost-replica-55fd9c5577 to 2&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-replica-55fd9c5577: {replicaset-controller } SuccessfulCreate: Created pod: agnhost-replica-55fd9c5577-2pbkj&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-replica-55fd9c5577: {replicaset-controller } SuccessfulCreate: Created pod: agnhost-replica-55fd9c5577-vm58f&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-replica-55fd9c5577-2pbkj: {default-scheduler } Scheduled: Successfully assigned kubectl-1653/agnhost-replica-55fd9c5577-2pbkj to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for agnhost-replica-55fd9c5577-vm58f: {default-scheduler } Scheduled: Successfully assigned kubectl-1653/agnhost-replica-55fd9c5577-vm58f to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for frontend-7659f66489-nck46: {default-scheduler } Scheduled: Successfully assigned kubectl-1653/frontend-7659f66489-nck46 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:51 +0000 UTC - event for frontend-7659f66489-nck46: {pod-syncer } SyncError: Error syncing to physical cluster: Operation cannot be fulfilled on pods &#34;frontend-7659f66489-nck46-x-kubectl-1653-x-vcluster&#34;: the object has been modified; please apply your changes to the latest version and try again&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:52 +0000 UTC - event for frontend-7659f66489-5778n: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:52 +0000 UTC - event for frontend-7659f66489-5778n: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:52 +0000 UTC - event for frontend-7659f66489-5778n: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:52 +0000 UTC - event for frontend-7659f66489-nck46: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:52 +0000 UTC - event for frontend-7659f66489-nck46: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:52 +0000 UTC - event for frontend-7659f66489-nck46: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-primary-56857545d9-9pgtc: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container primary&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-primary-56857545d9-9pgtc: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-primary-56857545d9-9pgtc: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container primary&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-replica-55fd9c5577-2pbkj: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container replica&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-replica-55fd9c5577-2pbkj: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container replica&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-replica-55fd9c5577-2pbkj: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-replica-55fd9c5577-vm58f: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container replica&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-replica-55fd9c5577-vm58f: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container replica&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for agnhost-replica-55fd9c5577-vm58f: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for frontend-7659f66489-qgvzh: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for frontend-7659f66489-qgvzh: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:21:53 +0000 UTC - event for frontend-7659f66489-qgvzh: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:32:00 +0000 UTC - event for agnhost-primary-56857545d9-9pgtc: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container primary&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:32:00 +0000 UTC - event for frontend-7659f66489-5778n: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:32:00 +0000 UTC - event for frontend-7659f66489-nck46: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container guestbook-frontend&#xA;May 18 14:32:00.598: INFO: At 2021-05-18 14:32:00 +0000 UTC - event for frontend-7659f66489-qgvzh: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container guestbook-frontend&#xA;May 18 14:32:00.606: INFO: POD                               NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 14:32:00.607: INFO: frontend-7659f66489-5778n         gke-cluster-2-default-pool-426afc83-rll8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:50 +0000 UTC  }]&#xA;May 18 14:32:00.607: INFO: frontend-7659f66489-nck46         gke-cluster-2-default-pool-426afc83-rll8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  }]&#xA;May 18 14:32:00.607: INFO: frontend-7659f66489-qgvzh         gke-cluster-2-default-pool-426afc83-rll8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:50 +0000 UTC  }]&#xA;May 18 14:32:00.607: INFO: agnhost-primary-56857545d9-9pgtc  gke-cluster-2-default-pool-426afc83-rll8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  }]&#xA;May 18 14:32:00.608: INFO: agnhost-replica-55fd9c5577-2pbkj  gke-cluster-2-default-pool-426afc83-rll8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  }]&#xA;May 18 14:32:00.608: INFO: agnhost-replica-55fd9c5577-vm58f  gke-cluster-2-default-pool-426afc83-rll8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:21:51 +0000 UTC  }]&#xA;May 18 14:32:00.608: INFO: &#xA;May 18 14:32:00.619: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:32:00.627: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 6408 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:30:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:32:00.628: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:32:00.636: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:32:00.706: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:32:00.706: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:32:00.714: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 6409 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:30:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:27:44 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:32:00.715: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:32:00.724: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:32:00.819: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:32:00.819: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.828: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 6411 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:30:03 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:32:00.829: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.844: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:32:00.888: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:32:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;kubectl-1653&#34; for this suite.&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should be able to mount socket &#39;asocket&#39; successfully when HostPathType is HostPathSocket" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]" classname="Kubernetes e2e suite" time="1.661124961"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]" classname="Kubernetes e2e suite" time="14.338861953"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should create read-only inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support proportional scaling [Conformance]" classname="Kubernetes e2e suite" time="9.549570307"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] IngressClass [Feature:Ingress] should not set default value if no default IngressClass [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Firewall rule [Slow] [Serial] should create valid firewall rules for LoadBalancer type service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic Snapshot (delete policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] multicluster ingress should get instance group annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and one node is broken [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up when expendable pod is preempted [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should set ownership and permission when RunAsUser or FsGroup is present [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="98.819970663"></testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with different parameters" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not emit unexpected warnings" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] gpu Upgrade [Feature:GPUUpgrade] cluster downgrade should be able to run gpu pod after downgrade [Feature:GPUClusterDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide host IP and pod IP as an env var if pod uses host network [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an if a SPBM policy and VSAN capabilities cannot be honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]" classname="Kubernetes e2e suite" time="1.7945468180000002"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that&#39;s waiting for dependents to be deleted [Conformance]" classname="Kubernetes e2e suite" time="10.746980417"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]" classname="Kubernetes e2e suite" time="10.592479755"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create an internal type load balancer [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume storage capacity exhausted, late binding, with topology" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.607383404"></testcase>
      <testcase name="[sig-network] DNS should provide DNS for ExternalName services [Conformance]" classname="Kubernetes e2e suite" time="618.419047359">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:44:49.122: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/dns_common.go:464</failure>
          <system-out>[BeforeEach] [sig-network] DNS&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:34:31.071: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename dns&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should provide DNS for ExternalName services [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: Creating a test externalName service&#xA;STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3437.svc.cluster.local CNAME &gt; /results/wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local; sleep 1; done&#xA;&#xA;STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3437.svc.cluster.local CNAME &gt; /results/jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local; sleep 1; done&#xA;&#xA;STEP: creating a pod to probe DNS&#xA;STEP: submitting the pod to kubernetes&#xA;STEP: retrieving the pod&#xA;STEP: looking for the results for each expected name from probers&#xA;May 18 14:34:35.697: INFO: DNS probes using dns-test-944d309c-62ca-4bda-8282-425529a97edd succeeded&#xA;&#xA;STEP: deleting the pod&#xA;STEP: changing the externalName to bar.example.com&#xA;STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3437.svc.cluster.local CNAME &gt; /results/wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local; sleep 1; done&#xA;&#xA;STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3437.svc.cluster.local CNAME &gt; /results/jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local; sleep 1; done&#xA;&#xA;STEP: creating a second pod to probe DNS&#xA;STEP: submitting the pod to kubernetes&#xA;STEP: retrieving the pod&#xA;STEP: looking for the results for each expected name from probers&#xA;May 18 14:34:39.839: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2 contains &#39;foo.example.com.&#xA;&#39; instead of &#39;bar.example.com.&#39;&#xA;May 18 14:34:39.855: INFO: Lookups using dns-3437/dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:34:44.939: INFO: DNS probes using dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2 succeeded&#xA;&#xA;STEP: deleting the pod&#xA;STEP: changing the service to type=ClusterIP&#xA;STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3437.svc.cluster.local A &gt; /results/wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local; sleep 1; done&#xA;&#xA;STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3437.svc.cluster.local A &gt; /results/jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local; sleep 1; done&#xA;&#xA;STEP: creating a third pod to probe DNS&#xA;STEP: submitting the pod to kubernetes&#xA;STEP: retrieving the pod&#xA;STEP: looking for the results for each expected name from probers&#xA;May 18 14:34:49.048: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:34:49.063: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:34:49.063: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:34:54.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:34:54.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:34:54.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:34:59.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:34:59.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:34:59.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:04.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:04.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:04.091: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:09.093: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:09.108: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:09.108: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:14.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:14.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:14.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:19.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:19.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:19.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:24.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:24.098: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:24.098: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:29.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:29.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:29.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:34.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:34.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:34.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:39.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:39.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:39.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:44.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:44.099: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:44.099: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:49.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:49.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:49.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:54.087: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:54.112: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:54.112: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:35:59.093: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:59.114: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:35:59.114: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:04.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:04.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:04.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:09.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:09.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:09.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:14.111: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:14.148: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:14.148: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:19.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:19.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:19.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:24.083: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:24.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:24.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:29.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:29.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:29.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:34.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:34.090: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:34.090: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:39.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:39.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:39.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:44.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:44.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:44.091: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:49.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:49.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:49.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:54.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:54.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:54.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:36:59.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:59.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:36:59.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:04.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:04.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:04.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:09.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:09.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:09.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:14.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:14.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:14.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:19.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:19.098: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:19.098: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:24.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:24.102: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:24.102: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:29.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:29.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:29.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:34.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:34.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:34.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:39.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:39.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:39.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:44.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:44.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:44.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:49.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:49.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:49.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:54.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:54.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:54.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:37:59.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:59.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:37:59.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:04.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:04.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:04.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:09.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:09.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:09.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:14.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:14.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:14.091: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:19.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:19.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:19.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:24.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:24.089: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:24.089: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:29.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:29.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:29.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:34.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:34.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:34.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:39.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:39.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:39.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:44.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:44.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:44.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:49.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:49.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:49.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:54.099: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:54.117: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:54.118: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:38:59.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:59.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:38:59.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:04.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:04.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:04.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:09.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:09.090: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:09.090: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:14.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:14.089: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:14.089: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:19.091: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:19.104: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:19.104: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:24.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:24.089: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:24.090: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:29.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:29.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:29.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:34.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:34.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:34.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:39.085: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:39.099: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:39.099: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:44.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:44.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:44.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:49.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:49.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:49.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:54.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:54.089: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:54.089: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:39:59.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:59.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:39:59.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:04.083: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:04.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:04.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:09.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:09.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:09.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:14.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:14.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:14.091: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:19.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:19.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:19.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:24.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:24.090: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:24.090: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:29.083: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:29.105: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:29.106: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:34.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:34.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:34.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:39.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:39.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:39.091: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:44.083: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:44.097: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:44.097: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:49.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:49.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:49.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:54.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:54.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:54.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:40:59.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:59.097: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:40:59.097: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:04.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:04.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:04.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:09.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:09.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:09.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:14.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:14.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:14.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:19.086: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:19.113: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:19.114: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:24.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:24.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:24.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:29.111: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:29.132: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:29.133: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:34.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:34.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:34.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:39.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:39.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:39.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:44.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:44.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:44.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:49.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:49.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:49.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:54.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:54.090: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:54.090: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:41:59.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:59.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:41:59.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:04.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:04.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:04.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:09.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:09.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:09.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:14.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:14.121: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:14.121: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:19.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:19.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:19.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:24.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:24.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:24.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:29.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:29.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:29.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:34.101: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:34.112: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:34.112: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:39.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:39.092: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:39.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:44.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:44.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:44.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:49.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:49.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:49.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:54.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:54.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:54.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:42:59.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:59.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:42:59.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:04.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:04.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:04.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:09.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:09.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:09.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:14.079: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:14.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:14.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:19.080: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:19.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:19.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:24.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:24.090: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:24.090: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:29.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:29.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:29.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:34.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:34.097: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:34.097: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:39.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:39.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:39.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:44.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:44.096: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:44.096: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:49.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:49.094: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:49.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:54.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:54.091: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:54.092: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:43:59.078: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:59.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:43:59.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:04.085: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:04.099: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:04.099: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:09.100: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:09.114: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:09.114: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:14.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:14.097: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:14.097: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:19.168: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:19.183: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:19.183: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:24.076: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:24.087: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:24.087: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:29.082: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:29.095: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:29.095: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:34.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:34.089: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:34.089: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:39.081: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:39.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:39.094: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:44.088: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:44.105: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:44.105: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:49.077: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:49.093: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:49.093: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:49.107: INFO: File wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:49.121: INFO: File jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local from pod  dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 contains &#39;10.108.4.3&#xA;&#39; instead of &#39;10.108.11.33&#39;&#xA;May 18 14:44:49.121: INFO: Lookups using dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 failed for: [wheezy_udp@dns-test-service-3.dns-3437.svc.cluster.local jessie_udp@dns-test-service-3.dns-3437.svc.cluster.local]&#xA;&#xA;May 18 14:44:49.122: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.assertFilesContain(0xc0024ac918, 0x2, 0x2, 0x4dc68f4, 0x7, 0xc001e63000, 0x5607080, 0xc001ba6b00, 0x1, 0xc00307bce0, ...)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/dns_common.go:464 +0x18a&#xA;k8s.io/kubernetes/test/e2e/network.validateTargetedProbeOutput(0xc0009fc160, 0xc001e63000, 0xc0024ac918, 0x2, 0x2, 0xc00307bce0, 0xc)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/dns_common.go:549 +0x365&#xA;k8s.io/kubernetes/test/e2e/network.glob..func2.9()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/dns.go:399 +0x11ea&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;STEP: deleting the pod&#xA;STEP: deleting the test externalName service&#xA;[AfterEach] [sig-network] DNS&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;dns-3437&#34;.&#xA;STEP: Found 41 events.&#xA;May 18 14:44:49.201: INFO: At 2021-05-18 14:34:31 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {default-scheduler } Scheduled: Successfully assigned dns-3437/dns-test-944d309c-62ca-4bda-8282-425529a97edd to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:44:49.201: INFO: At 2021-05-18 14:34:32 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;alpine:3.13.1&#34; already present on machine&#xA;May 18 14:44:49.202: INFO: At 2021-05-18 14:34:32 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container vcluster-rewrite-hosts&#xA;May 18 14:44:49.204: INFO: At 2021-05-18 14:34:32 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container vcluster-rewrite-hosts&#xA;May 18 14:44:49.205: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container webserver&#xA;May 18 14:44:49.207: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:44:49.208: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container webserver&#xA;May 18 14:44:49.210: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:44:49.211: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container querier&#xA;May 18 14:44:49.213: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container querier&#xA;May 18 14:44:49.214: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&#34; already present on machine&#xA;May 18 14:44:49.216: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container jessie-querier&#xA;May 18 14:44:49.220: INFO: At 2021-05-18 14:34:33 +0000 UTC - event for dns-test-944d309c-62ca-4bda-8282-425529a97edd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container jessie-querier&#xA;May 18 14:44:49.220: INFO: At 2021-05-18 14:34:35 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {default-scheduler } Scheduled: Successfully assigned dns-3437/dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:44:49.222: INFO: At 2021-05-18 14:34:36 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;alpine:3.13.1&#34; already present on machine&#xA;May 18 14:44:49.222: INFO: At 2021-05-18 14:34:36 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container vcluster-rewrite-hosts&#xA;May 18 14:44:49.223: INFO: At 2021-05-18 14:34:36 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container vcluster-rewrite-hosts&#xA;May 18 14:44:49.223: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container jessie-querier&#xA;May 18 14:44:49.223: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container webserver&#xA;May 18 14:44:49.224: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container webserver&#xA;May 18 14:44:49.224: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:44:49.224: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container querier&#xA;May 18 14:44:49.224: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container querier&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&#34; already present on machine&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:37 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:38 +0000 UTC - event for dns-test-daebba10-e603-4cd2-bef3-f47082dad4b2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container jessie-querier&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:45 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {default-scheduler } Scheduled: Successfully assigned dns-3437/dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:45 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;alpine:3.13.1&#34; already present on machine&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:45 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container vcluster-rewrite-hosts&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&#34; already present on machine&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container querier&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container webserver&#xA;May 18 14:44:49.225: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container webserver&#xA;May 18 14:44:49.226: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container vcluster-rewrite-hosts&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container jessie-querier&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container jessie-querier&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container querier&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:34:46 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:35:48 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} BackOff: Back-off restarting failed container&#xA;May 18 14:44:49.229: INFO: At 2021-05-18 14:35:48 +0000 UTC - event for dns-test-f5edb10a-d49e-4c19-b303-ebe4458f3fb7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} BackOff: Back-off restarting failed container&#xA;May 18 14:44:49.237: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 14:44:49.238: INFO: &#xA;May 18 14:44:49.246: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:44:49.253: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 7797 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:40:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:40:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:40:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:40:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:44:49.253: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:44:49.262: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:44:49.306: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:44:49.306: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:44:49.314: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 7929 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:40:08 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:44:49.314: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:44:49.322: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:44:49.358: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:44:49.359: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:44:49.408: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 7930 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:40:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:42:49 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:44:49.410: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:44:49.416: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:44:49.470: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:44:49.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;dns-3437&#34; for this suite.&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.537051491"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should support exec using resource/name" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver Forward external name lookup should forward externalname lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]" classname="Kubernetes e2e suite" time="361.065053661">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:50:52.719: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:259</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:44:52.032: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename sched-preemption&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90&#xA;May 18 14:44:52.532: INFO: Waiting up to 1m0s for all nodes to be ready&#xA;May 18 14:45:52.590: INFO: Waiting for terminating namespaces to be deleted...&#xA;[It] validates lower priority pod preemption by critical pod [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: Create pods that use 2/3 of node resources.&#xA;May 18 14:45:52.623: INFO: Created pod: pod0-sched-preemption-low-priority&#xA;May 18 14:45:52.649: INFO: Created pod: pod1-sched-preemption-medium-priority&#xA;May 18 14:45:52.679: INFO: Created pod: pod2-sched-preemption-medium-priority&#xA;STEP: Wait for pods to be scheduled.&#xA;May 18 14:50:52.719: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/scheduling.glob..func5.4()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:259 +0x951&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;sched-preemption-5175&#34;.&#xA;STEP: Found 3 events.&#xA;May 18 14:50:52.727: INFO: At 2021-05-18 14:45:52 +0000 UTC - event for pod0-sched-preemption-low-priority: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;pod0-sched-preemption-low-priority-x-sched-preemptio-8015a2b5eb&#34; is forbidden: no PriorityClass with name sched-preemption-low-priority was found&#xA;May 18 14:50:52.727: INFO: At 2021-05-18 14:45:52 +0000 UTC - event for pod1-sched-preemption-medium-priority: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;pod1-sched-preemption-medium-priority-x-sched-preemp-1a1b92dd85&#34; is forbidden: no PriorityClass with name sched-preemption-medium-priority was found&#xA;May 18 14:50:52.727: INFO: At 2021-05-18 14:45:52 +0000 UTC - event for pod2-sched-preemption-medium-priority: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;pod2-sched-preemption-medium-priority-x-sched-preemp-bb8a50a0db&#34; is forbidden: no PriorityClass with name sched-preemption-medium-priority was found&#xA;May 18 14:50:52.734: INFO: POD                                    NODE  PHASE    GRACE  CONDITIONS&#xA;May 18 14:50:52.735: INFO: pod0-sched-preemption-low-priority           Pending         []&#xA;May 18 14:50:52.735: INFO: pod1-sched-preemption-medium-priority        Pending         []&#xA;May 18 14:50:52.735: INFO: pod2-sched-preemption-medium-priority        Pending         []&#xA;May 18 14:50:52.735: INFO: &#xA;May 18 14:50:52.743: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:50:52.750: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 8200 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:50:52.751: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:50:52.758: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:50:52.817: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:50:52.817: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:50:52.826: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 8201 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:50:52.826: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:50:52.833: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:50:52.874: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:50:52.875: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:50:52.883: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 8204 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:50:52.884: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:50:52.891: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:50:52.969: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:50:52.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;sched-preemption-5175&#34; for this suite.&#xA;[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Controller Manager should not create/delete replicas across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create total pv count metrics for with plugin and volume mode labels after creating pv" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should deny ingress access to updated pod [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working CockroachDB cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]" classname="Kubernetes e2e suite" time="0.723477439"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t be able to scale down when rescheduling a pod is required, but pdb doesn&#39;t allow drain[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should implement service.kubernetes.io/service-proxy-name" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:TTLAfterFinished][NodeAlphaFeature:TTLAfterFinished] job should be deleted once it finishes after TTL seconds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create volume metrics in Volume Manager" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should recreate its iptables rules if they are deleted [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpNotIn)." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] GKE local SSD [Feature:GKELocalSSD] should write and read from node local SSD [Feature:GKELocalSSD]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for client IP based session affinity: udp [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale up at all [Feature:ClusterAutoscalerScalability1]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]" classname="Kubernetes e2e suite" time="6.124401307"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group down to 0[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a local redirect http liveness probe" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with a configured handler [NodeFeature:RuntimeHandler]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver with Prometheus [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should create read/write inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not schedule jobs when suspended [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from API server." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should implement service.kubernetes.io/headless" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Mount propagation should propagate mounts to the host" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should be able to mount block device &#39;ablkdev&#39; successfully when HostPathType is HostPathUnset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates local ephemeral storage resource limits of pods that are allowed to run [Feature:LocalStorageCapacityIsolation]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.5630213729999998"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]" classname="Kubernetes e2e suite" time="0.606439104"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should reconcile LB health check interval [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify clean up of stale dummy VM for dynamically provisioned pvc using SPBM policy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]" classname="Kubernetes e2e suite" time="3.558625655"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 Scalability GCE [Slow] [Serial] [Feature:IngressScale] Creating and updating ingresses should happen promptly with small/medium/large amount of ingresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Restart [Disruptive] should restart all nodes and ensure all nodes and pods recover" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] client-go should negotiate watch and report errors with accept &#34;application/json&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]" classname="Kubernetes e2e suite" time="8.506879562"></testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should allow egress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for pod-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]" classname="Kubernetes e2e suite" time="6.564778008"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] should handle updates to ExternalTrafficPolicy field [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [StatefulSet] should not reschedule stateful pods if there is a network partition [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should serve a basic endpoint from pods  [Conformance]" classname="Kubernetes e2e suite" time="4.880908196"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:PerformanceDNS][Serial] Should answer DNS query for maximum number of services per cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for git_repo [Serial] [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should fail on mounting character device &#39;achardev&#39; when HostPathType is HostPathSocket" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should work after restarting kube-proxy [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] GPUDevicePluginAcrossRecreate [Feature:Recreate] run Nvidia GPU Device Plugin tests with a recreation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.5600562780000002"></testcase>
      <testcase name="[sig-network] IngressClass [Feature:Ingress] should prevent Ingress creation if more than 1 IngressClass marked as default [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] PodTopologySpread Scoring validates pod should be preferably scheduled to node which makes the matching pods more evenly distributed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] client-go should negotiate watch and report errors with accept &#34;application/vnd.kubernetes.protobuf&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]" classname="Kubernetes e2e suite" time="21.697834525">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:51:42.249: 0 (503; 122.632008ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 122.772505ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 132.436866ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 133.141864ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 135.639443ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 138.378166ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;0: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;0: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;0: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;1 (503; 44.802861ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;1: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;1 (503; 52.488828ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;1: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;1 (503; 55.111783ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1 (503; 61.93644ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1 (503; 64.495482ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1 (503; 65.048243ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 44.360874ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 48.909571ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 59.725724ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 62.262363ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 64.526332ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 65.502701ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;2: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;2: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;2: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3 (503; 41.276033ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 41.218023ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 55.030499ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3 (503; 58.556651ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 59.716383ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 64.3124ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;4 (503; 31.19244ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 50.531142ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 52.921724ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;4: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;4: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;4: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;4 (503; 66.968861ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 67.554017ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 68.780481ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 28.546474ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;5: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;5: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;5 (503; 64.459768ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;5 (503; 76.018262ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 76.312858ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 80.268481ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 80.578959ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 33.60103ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;6: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;6 (503; 50.617169ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 50.000594ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;6: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;6 (503; 56.375891ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 55.999408ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 57.879603ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 36.18631ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;7 (503; 38.958359ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 43.144835ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 44.126687ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 47.923834ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 49.208871ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;7: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;7: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;8 (503; 26.402507ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8 (503; 37.518179ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8 (503; 39.200422ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;8: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;8 (503; 48.593496ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8 (503; 49.147133ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;8: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;8 (503; 52.519596ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 34.695832ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;9: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;9 (503; 39.70234ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 43.901314ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 48.493842ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;9: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;9 (503; 52.249322ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 53.575383ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 29.525922ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 34.406079ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;10 (503; 45.040869ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 46.10032ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 46.448136ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 46.17586ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;10: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;10: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;11 (503; 39.420859ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 42.949061ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 46.755713ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11 (503; 52.738467ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 54.637805ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 55.290781ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 36.85198ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 42.383759ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;12 (503; 45.065642ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;12: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;12: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;12 (503; 52.099283ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 52.813366ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 54.624628ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;13 (503; 33.63957ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13 (503; 37.927869ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13 (503; 42.286222ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;13 (503; 53.352479ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;13 (503; 56.694907ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13 (503; 57.992501ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14 (503; 35.587599ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14 (503; 47.411946ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14 (503; 50.881939ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14 (503; 52.080303ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;14: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14 (503; 56.584105ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14 (503; 57.640386ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 37.953952ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 40.797856ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 46.066855ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 50.260438ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 57.859097ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 57.659702ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;15: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;15: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;15: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;16 (503; 37.457825ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 61.246233ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 61.517908ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 63.469583ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 73.505788ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 72.846851ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;16: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;16: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;16: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 31.775997ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17 (503; 54.969429ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17 (503; 58.340161ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 63.300482ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 68.033331ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 69.520098ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;18 (503; 41.107331ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 47.711665ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;18: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;18 (503; 62.961912ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 65.723553ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 68.861685ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 72.083912ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19 (503; 43.353629ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19 (503; 45.426632ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19 (503; 50.434601ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19 (503; 56.434236ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19 (503; 56.222993ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;19 (503; 60.773842ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/internal/leafnodes/runner.go:113</failure>
          <system-out>[BeforeEach] version v1&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:51:29.207: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename proxy&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should proxy through a service and a pod  [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: starting an echo server on multiple ports&#xA;STEP: creating replication controller proxy-service-hcvkb in namespace proxy-1529&#xA;May 18 14:51:40.798: INFO: setup took 11.136146655s, starting test cases&#xA;STEP: running 16 cases, 20 attempts per case, 320 total attempts&#xA;May 18 14:51:40.921: INFO: (0) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 122.632008ms)&#xA;May 18 14:51:40.921: INFO: (0) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 122.772505ms)&#xA;May 18 14:51:40.927: INFO: (0) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 128.165727ms)&#xA;May 18 14:51:40.931: INFO: (0) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 132.436866ms)&#xA;May 18 14:51:40.932: INFO: (0) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 133.141864ms)&#xA;May 18 14:51:40.934: INFO: (0) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 135.639443ms)&#xA;May 18 14:51:40.937: INFO: (0) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 138.378166ms)&#xA;May 18 14:51:40.960: INFO: (0) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 162.128709ms)&#xA;May 18 14:51:40.974: INFO: (0) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 175.856686ms)&#xA;May 18 14:51:40.976: INFO: (0) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 177.432819ms)&#xA;May 18 14:51:40.980: INFO: (0) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 181.565509ms)&#xA;May 18 14:51:40.982: INFO: (0) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 183.513411ms)&#xA;May 18 14:51:40.982: INFO: (0) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 183.179776ms)&#xA;May 18 14:51:40.982: INFO: (0) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 183.951458ms)&#xA;May 18 14:51:40.988: INFO: (0) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 190.108354ms)&#xA;May 18 14:51:40.989: INFO: (0) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 190.28759ms)&#xA;May 18 14:51:41.035: INFO: (1) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 44.802861ms)&#xA;May 18 14:51:41.040: INFO: (1) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 50.050038ms)&#xA;May 18 14:51:41.040: INFO: (1) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 50.13604ms)&#xA;May 18 14:51:41.041: INFO: (1) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 50.338901ms)&#xA;May 18 14:51:41.042: INFO: (1) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 52.466201ms)&#xA;May 18 14:51:41.042: INFO: (1) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 52.488828ms)&#xA;May 18 14:51:41.042: INFO: (1) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 52.003692ms)&#xA;May 18 14:51:41.042: INFO: (1) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 52.666982ms)&#xA;May 18 14:51:41.043: INFO: (1) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 53.230317ms)&#xA;May 18 14:51:41.044: INFO: (1) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 54.909987ms)&#xA;May 18 14:51:41.044: INFO: (1) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 54.146117ms)&#xA;May 18 14:51:41.044: INFO: (1) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 54.520029ms)&#xA;May 18 14:51:41.045: INFO: (1) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 55.111783ms)&#xA;May 18 14:51:41.051: INFO: (1) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 61.93644ms)&#xA;May 18 14:51:41.054: INFO: (1) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 64.495482ms)&#xA;May 18 14:51:41.055: INFO: (1) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 65.048243ms)&#xA;May 18 14:51:41.099: INFO: (2) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 44.360874ms)&#xA;May 18 14:51:41.100: INFO: (2) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 44.446655ms)&#xA;May 18 14:51:41.104: INFO: (2) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 48.909571ms)&#xA;May 18 14:51:41.116: INFO: (2) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 59.725724ms)&#xA;May 18 14:51:41.119: INFO: (2) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 62.262363ms)&#xA;May 18 14:51:41.120: INFO: (2) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 65.020255ms)&#xA;May 18 14:51:41.121: INFO: (2) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 64.526332ms)&#xA;May 18 14:51:41.122: INFO: (2) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 65.502701ms)&#xA;May 18 14:51:41.123: INFO: (2) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 66.458749ms)&#xA;May 18 14:51:41.124: INFO: (2) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 66.803935ms)&#xA;May 18 14:51:41.125: INFO: (2) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 69.325648ms)&#xA;May 18 14:51:41.125: INFO: (2) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 69.444073ms)&#xA;May 18 14:51:41.125: INFO: (2) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 69.518073ms)&#xA;May 18 14:51:41.125: INFO: (2) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 68.49798ms)&#xA;May 18 14:51:41.125: INFO: (2) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 69.912554ms)&#xA;May 18 14:51:41.125: INFO: (2) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 69.495913ms)&#xA;May 18 14:51:41.149: INFO: (3) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 23.283173ms)&#xA;May 18 14:51:41.155: INFO: (3) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 28.550692ms)&#xA;May 18 14:51:41.168: INFO: (3) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 41.276033ms)&#xA;May 18 14:51:41.168: INFO: (3) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 41.218023ms)&#xA;May 18 14:51:41.176: INFO: (3) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 49.576207ms)&#xA;May 18 14:51:41.182: INFO: (3) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 55.030499ms)&#xA;May 18 14:51:41.184: INFO: (3) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 56.736661ms)&#xA;May 18 14:51:41.184: INFO: (3) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 57.25842ms)&#xA;May 18 14:51:41.185: INFO: (3) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 58.556651ms)&#xA;May 18 14:51:41.186: INFO: (3) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 59.716383ms)&#xA;May 18 14:51:41.188: INFO: (3) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 61.364756ms)&#xA;May 18 14:51:41.190: INFO: (3) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 63.202456ms)&#xA;May 18 14:51:41.191: INFO: (3) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 64.3124ms)&#xA;May 18 14:51:41.191: INFO: (3) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 64.799224ms)&#xA;May 18 14:51:41.192: INFO: (3) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 64.893996ms)&#xA;May 18 14:51:41.192: INFO: (3) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 65.506354ms)&#xA;May 18 14:51:41.224: INFO: (4) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 31.19244ms)&#xA;May 18 14:51:41.225: INFO: (4) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 31.78887ms)&#xA;May 18 14:51:41.229: INFO: (4) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 36.230305ms)&#xA;May 18 14:51:41.243: INFO: (4) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 50.531142ms)&#xA;May 18 14:51:41.246: INFO: (4) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 52.921724ms)&#xA;May 18 14:51:41.256: INFO: (4) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 62.561868ms)&#xA;May 18 14:51:41.257: INFO: (4) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 63.604018ms)&#xA;May 18 14:51:41.257: INFO: (4) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 63.851757ms)&#xA;May 18 14:51:41.258: INFO: (4) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 64.442257ms)&#xA;May 18 14:51:41.258: INFO: (4) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 65.451992ms)&#xA;May 18 14:51:41.258: INFO: (4) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 65.118697ms)&#xA;May 18 14:51:41.258: INFO: (4) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 64.891244ms)&#xA;May 18 14:51:41.260: INFO: (4) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 67.116103ms)&#xA;May 18 14:51:41.260: INFO: (4) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 66.968861ms)&#xA;May 18 14:51:41.261: INFO: (4) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 67.554017ms)&#xA;May 18 14:51:41.262: INFO: (4) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 68.780481ms)&#xA;May 18 14:51:41.291: INFO: (5) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 28.546474ms)&#xA;May 18 14:51:41.291: INFO: (5) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 29.017436ms)&#xA;May 18 14:51:41.327: INFO: (5) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 64.168771ms)&#xA;May 18 14:51:41.327: INFO: (5) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 64.862903ms)&#xA;May 18 14:51:41.327: INFO: (5) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 64.818668ms)&#xA;May 18 14:51:41.327: INFO: (5) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 64.65633ms)&#xA;May 18 14:51:41.327: INFO: (5) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 64.459768ms)&#xA;May 18 14:51:41.329: INFO: (5) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 66.439983ms)&#xA;May 18 14:51:41.330: INFO: (5) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 66.90575ms)&#xA;May 18 14:51:41.332: INFO: (5) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 68.887508ms)&#xA;May 18 14:51:41.339: INFO: (5) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 75.705485ms)&#xA;May 18 14:51:41.339: INFO: (5) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 75.912656ms)&#xA;May 18 14:51:41.339: INFO: (5) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 76.018262ms)&#xA;May 18 14:51:41.339: INFO: (5) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 76.312858ms)&#xA;May 18 14:51:41.343: INFO: (5) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 80.268481ms)&#xA;May 18 14:51:41.344: INFO: (5) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 80.578959ms)&#xA;May 18 14:51:41.378: INFO: (6) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 33.353012ms)&#xA;May 18 14:51:41.378: INFO: (6) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 33.60103ms)&#xA;May 18 14:51:41.378: INFO: (6) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 33.86923ms)&#xA;May 18 14:51:41.381: INFO: (6) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 36.38294ms)&#xA;May 18 14:51:41.394: INFO: (6) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 49.810875ms)&#xA;May 18 14:51:41.395: INFO: (6) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 50.000594ms)&#xA;May 18 14:51:41.395: INFO: (6) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 49.905537ms)&#xA;May 18 14:51:41.395: INFO: (6) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 50.617169ms)&#xA;May 18 14:51:41.396: INFO: (6) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 51.194245ms)&#xA;May 18 14:51:41.397: INFO: (6) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 53.04327ms)&#xA;May 18 14:51:41.398: INFO: (6) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 52.66482ms)&#xA;May 18 14:51:41.398: INFO: (6) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 53.006237ms)&#xA;May 18 14:51:41.398: INFO: (6) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 53.383628ms)&#xA;May 18 14:51:41.401: INFO: (6) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 56.375891ms)&#xA;May 18 14:51:41.401: INFO: (6) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 55.999408ms)&#xA;May 18 14:51:41.402: INFO: (6) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 57.879603ms)&#xA;May 18 14:51:41.439: INFO: (7) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 36.18631ms)&#xA;May 18 14:51:41.441: INFO: (7) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 38.779332ms)&#xA;May 18 14:51:41.442: INFO: (7) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 38.958359ms)&#xA;May 18 14:51:41.445: INFO: (7) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 43.144835ms)&#xA;May 18 14:51:41.447: INFO: (7) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 44.126687ms)&#xA;May 18 14:51:41.451: INFO: (7) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 47.923834ms)&#xA;May 18 14:51:41.452: INFO: (7) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 49.208871ms)&#xA;May 18 14:51:41.461: INFO: (7) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 57.895202ms)&#xA;May 18 14:51:41.461: INFO: (7) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 57.87929ms)&#xA;May 18 14:51:41.461: INFO: (7) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 57.997585ms)&#xA;May 18 14:51:41.461: INFO: (7) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 58.312077ms)&#xA;May 18 14:51:41.461: INFO: (7) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 58.315944ms)&#xA;May 18 14:51:41.462: INFO: (7) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 58.74912ms)&#xA;May 18 14:51:41.467: INFO: (7) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 64.552769ms)&#xA;May 18 14:51:41.467: INFO: (7) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 64.461423ms)&#xA;May 18 14:51:41.470: INFO: (7) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 66.794369ms)&#xA;May 18 14:51:41.497: INFO: (8) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 26.402507ms)&#xA;May 18 14:51:41.508: INFO: (8) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 37.518179ms)&#xA;May 18 14:51:41.512: INFO: (8) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 39.200422ms)&#xA;May 18 14:51:41.518: INFO: (8) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 47.821226ms)&#xA;May 18 14:51:41.518: INFO: (8) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 48.004231ms)&#xA;May 18 14:51:41.521: INFO: (8) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 50.763003ms)&#xA;May 18 14:51:41.522: INFO: (8) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 48.593496ms)&#xA;May 18 14:51:41.522: INFO: (8) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 49.147133ms)&#xA;May 18 14:51:41.522: INFO: (8) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 48.95455ms)&#xA;May 18 14:51:41.523: INFO: (8) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 49.651273ms)&#xA;May 18 14:51:41.524: INFO: (8) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 51.14134ms)&#xA;May 18 14:51:41.525: INFO: (8) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 54.530789ms)&#xA;May 18 14:51:41.525: INFO: (8) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 52.177504ms)&#xA;May 18 14:51:41.525: INFO: (8) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 52.05415ms)&#xA;May 18 14:51:41.525: INFO: (8) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 55.074464ms)&#xA;May 18 14:51:41.526: INFO: (8) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 52.519596ms)&#xA;May 18 14:51:41.561: INFO: (9) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 34.695832ms)&#xA;May 18 14:51:41.561: INFO: (9) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 35.405837ms)&#xA;May 18 14:51:41.562: INFO: (9) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 35.987157ms)&#xA;May 18 14:51:41.566: INFO: (9) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 39.70234ms)&#xA;May 18 14:51:41.570: INFO: (9) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 43.901314ms)&#xA;May 18 14:51:41.574: INFO: (9) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 47.573678ms)&#xA;May 18 14:51:41.575: INFO: (9) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 48.493842ms)&#xA;May 18 14:51:41.576: INFO: (9) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 49.140915ms)&#xA;May 18 14:51:41.576: INFO: (9) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 49.780229ms)&#xA;May 18 14:51:41.577: INFO: (9) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 50.145554ms)&#xA;May 18 14:51:41.577: INFO: (9) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 50.574049ms)&#xA;May 18 14:51:41.577: INFO: (9) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 50.540068ms)&#xA;May 18 14:51:41.577: INFO: (9) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 50.169259ms)&#xA;May 18 14:51:41.578: INFO: (9) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 52.249322ms)&#xA;May 18 14:51:41.579: INFO: (9) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 52.990442ms)&#xA;May 18 14:51:41.580: INFO: (9) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 53.575383ms)&#xA;May 18 14:51:41.610: INFO: (10) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 29.525922ms)&#xA;May 18 14:51:41.615: INFO: (10) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 34.406079ms)&#xA;May 18 14:51:41.618: INFO: (10) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 37.861142ms)&#xA;May 18 14:51:41.626: INFO: (10) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 44.746975ms)&#xA;May 18 14:51:41.626: INFO: (10) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 45.040869ms)&#xA;May 18 14:51:41.627: INFO: (10) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 46.10032ms)&#xA;May 18 14:51:41.627: INFO: (10) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 46.448136ms)&#xA;May 18 14:51:41.627: INFO: (10) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 46.428732ms)&#xA;May 18 14:51:41.627: INFO: (10) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 46.17586ms)&#xA;May 18 14:51:41.627: INFO: (10) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 46.073276ms)&#xA;May 18 14:51:41.627: INFO: (10) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 46.386446ms)&#xA;May 18 14:51:41.632: INFO: (10) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 50.598367ms)&#xA;May 18 14:51:41.632: INFO: (10) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 50.628295ms)&#xA;May 18 14:51:41.632: INFO: (10) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 51.364524ms)&#xA;May 18 14:51:41.633: INFO: (10) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 51.862654ms)&#xA;May 18 14:51:41.633: INFO: (10) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 51.531812ms)&#xA;May 18 14:51:41.669: INFO: (11) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 35.35905ms)&#xA;May 18 14:51:41.669: INFO: (11) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 35.383893ms)&#xA;May 18 14:51:41.669: INFO: (11) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 35.417875ms)&#xA;May 18 14:51:41.670: INFO: (11) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 36.344545ms)&#xA;May 18 14:51:41.673: INFO: (11) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 39.420859ms)&#xA;May 18 14:51:41.677: INFO: (11) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 42.949061ms)&#xA;May 18 14:51:41.679: INFO: (11) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 44.244762ms)&#xA;May 18 14:51:41.681: INFO: (11) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 46.755713ms)&#xA;May 18 14:51:41.681: INFO: (11) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 47.494826ms)&#xA;May 18 14:51:41.682: INFO: (11) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 47.798125ms)&#xA;May 18 14:51:41.684: INFO: (11) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 49.839915ms)&#xA;May 18 14:51:41.684: INFO: (11) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 50.000713ms)&#xA;May 18 14:51:41.687: INFO: (11) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 52.902901ms)&#xA;May 18 14:51:41.687: INFO: (11) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 52.738467ms)&#xA;May 18 14:51:41.689: INFO: (11) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 54.637805ms)&#xA;May 18 14:51:41.689: INFO: (11) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 55.290781ms)&#xA;May 18 14:51:41.720: INFO: (12) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 29.792904ms)&#xA;May 18 14:51:41.727: INFO: (12) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 36.85198ms)&#xA;May 18 14:51:41.732: INFO: (12) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 42.383759ms)&#xA;May 18 14:51:41.733: INFO: (12) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 43.204146ms)&#xA;May 18 14:51:41.735: INFO: (12) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 45.065642ms)&#xA;May 18 14:51:41.739: INFO: (12) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 48.700276ms)&#xA;May 18 14:51:41.740: INFO: (12) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 50.006315ms)&#xA;May 18 14:51:41.740: INFO: (12) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 50.133634ms)&#xA;May 18 14:51:41.740: INFO: (12) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 50.12362ms)&#xA;May 18 14:51:41.740: INFO: (12) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 50.019401ms)&#xA;May 18 14:51:41.742: INFO: (12) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 52.099283ms)&#xA;May 18 14:51:41.743: INFO: (12) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 52.813885ms)&#xA;May 18 14:51:41.743: INFO: (12) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 52.806535ms)&#xA;May 18 14:51:41.743: INFO: (12) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 52.900314ms)&#xA;May 18 14:51:41.743: INFO: (12) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 52.813366ms)&#xA;May 18 14:51:41.745: INFO: (12) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 54.624628ms)&#xA;May 18 14:51:41.777: INFO: (13) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 31.685671ms)&#xA;May 18 14:51:41.779: INFO: (13) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 33.63957ms)&#xA;May 18 14:51:41.783: INFO: (13) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 37.927869ms)&#xA;May 18 14:51:41.788: INFO: (13) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 42.286222ms)&#xA;May 18 14:51:41.792: INFO: (13) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 45.887298ms)&#xA;May 18 14:51:41.794: INFO: (13) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 48.235224ms)&#xA;May 18 14:51:41.798: INFO: (13) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 52.154595ms)&#xA;May 18 14:51:41.799: INFO: (13) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 53.352479ms)&#xA;May 18 14:51:41.802: INFO: (13) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 56.024335ms)&#xA;May 18 14:51:41.802: INFO: (13) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 56.37197ms)&#xA;May 18 14:51:41.802: INFO: (13) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 56.432863ms)&#xA;May 18 14:51:41.802: INFO: (13) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 56.694907ms)&#xA;May 18 14:51:41.803: INFO: (13) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 57.992501ms)&#xA;May 18 14:51:41.804: INFO: (13) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 58.048317ms)&#xA;May 18 14:51:41.804: INFO: (13) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 58.59242ms)&#xA;May 18 14:51:41.805: INFO: (13) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 59.312995ms)&#xA;May 18 14:51:41.841: INFO: (14) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 35.587599ms)&#xA;May 18 14:51:41.842: INFO: (14) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 35.888411ms)&#xA;May 18 14:51:41.846: INFO: (14) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 40.117609ms)&#xA;May 18 14:51:41.853: INFO: (14) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 47.786913ms)&#xA;May 18 14:51:41.853: INFO: (14) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 47.411946ms)&#xA;May 18 14:51:41.853: INFO: (14) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 47.365453ms)&#xA;May 18 14:51:41.853: INFO: (14) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 47.755733ms)&#xA;May 18 14:51:41.853: INFO: (14) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 48.158055ms)&#xA;May 18 14:51:41.857: INFO: (14) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 50.881939ms)&#xA;May 18 14:51:41.858: INFO: (14) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 52.080303ms)&#xA;May 18 14:51:41.859: INFO: (14) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 53.372469ms)&#xA;May 18 14:51:41.859: INFO: (14) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 53.442259ms)&#xA;May 18 14:51:41.860: INFO: (14) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 54.981214ms)&#xA;May 18 14:51:41.860: INFO: (14) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 54.679646ms)&#xA;May 18 14:51:41.862: INFO: (14) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 56.584105ms)&#xA;May 18 14:51:41.863: INFO: (14) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 57.640386ms)&#xA;May 18 14:51:41.902: INFO: (15) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 37.603205ms)&#xA;May 18 14:51:41.902: INFO: (15) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 37.953952ms)&#xA;May 18 14:51:41.905: INFO: (15) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 40.797856ms)&#xA;May 18 14:51:41.910: INFO: (15) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 46.066855ms)&#xA;May 18 14:51:41.914: INFO: (15) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 50.260438ms)&#xA;May 18 14:51:41.922: INFO: (15) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 57.859097ms)&#xA;May 18 14:51:41.922: INFO: (15) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 57.659702ms)&#xA;May 18 14:51:41.922: INFO: (15) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 58.194252ms)&#xA;May 18 14:51:41.923: INFO: (15) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 59.295837ms)&#xA;May 18 14:51:41.927: INFO: (15) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 63.197754ms)&#xA;May 18 14:51:41.928: INFO: (15) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 64.083785ms)&#xA;May 18 14:51:41.930: INFO: (15) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 67.082644ms)&#xA;May 18 14:51:41.931: INFO: (15) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 66.876394ms)&#xA;May 18 14:51:41.931: INFO: (15) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 67.553185ms)&#xA;May 18 14:51:41.933: INFO: (15) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 68.587693ms)&#xA;May 18 14:51:41.933: INFO: (15) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 68.703819ms)&#xA;May 18 14:51:41.971: INFO: (16) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 37.457825ms)&#xA;May 18 14:51:41.995: INFO: (16) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 61.246233ms)&#xA;May 18 14:51:41.995: INFO: (16) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 61.376957ms)&#xA;May 18 14:51:41.995: INFO: (16) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 61.517908ms)&#xA;May 18 14:51:41.998: INFO: (16) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 63.469583ms)&#xA;May 18 14:51:42.000: INFO: (16) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 66.594558ms)&#xA;May 18 14:51:42.007: INFO: (16) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 72.846851ms)&#xA;May 18 14:51:42.007: INFO: (16) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 73.505788ms)&#xA;May 18 14:51:42.010: INFO: (16) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 76.054828ms)&#xA;May 18 14:51:42.010: INFO: (16) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 76.664783ms)&#xA;May 18 14:51:42.012: INFO: (16) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 78.019352ms)&#xA;May 18 14:51:42.012: INFO: (16) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 78.186092ms)&#xA;May 18 14:51:42.013: INFO: (16) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 79.350906ms)&#xA;May 18 14:51:42.021: INFO: (16) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 87.075806ms)&#xA;May 18 14:51:42.021: INFO: (16) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 87.270472ms)&#xA;May 18 14:51:42.022: INFO: (16) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 87.821068ms)&#xA;May 18 14:51:42.054: INFO: (17) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 32.100277ms)&#xA;May 18 14:51:42.054: INFO: (17) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 31.775997ms)&#xA;May 18 14:51:42.077: INFO: (17) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 54.969429ms)&#xA;May 18 14:51:42.079: INFO: (17) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 55.720867ms)&#xA;May 18 14:51:42.081: INFO: (17) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 58.340161ms)&#xA;May 18 14:51:42.081: INFO: (17) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 58.972029ms)&#xA;May 18 14:51:42.085: INFO: (17) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 63.60246ms)&#xA;May 18 14:51:42.085: INFO: (17) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 62.314409ms)&#xA;May 18 14:51:42.085: INFO: (17) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 62.20904ms)&#xA;May 18 14:51:42.085: INFO: (17) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 62.630575ms)&#xA;May 18 14:51:42.085: INFO: (17) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 63.300482ms)&#xA;May 18 14:51:42.086: INFO: (17) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 63.336457ms)&#xA;May 18 14:51:42.086: INFO: (17) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 63.086348ms)&#xA;May 18 14:51:42.091: INFO: (17) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 68.033331ms)&#xA;May 18 14:51:42.091: INFO: (17) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 67.857101ms)&#xA;May 18 14:51:42.092: INFO: (17) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 69.520098ms)&#xA;May 18 14:51:42.130: INFO: (18) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 37.859413ms)&#xA;May 18 14:51:42.134: INFO: (18) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 41.107331ms)&#xA;May 18 14:51:42.140: INFO: (18) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 47.711665ms)&#xA;May 18 14:51:42.152: INFO: (18) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 59.42181ms)&#xA;May 18 14:51:42.153: INFO: (18) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 60.010776ms)&#xA;May 18 14:51:42.156: INFO: (18) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 62.961912ms)&#xA;May 18 14:51:42.158: INFO: (18) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 65.413257ms)&#xA;May 18 14:51:42.158: INFO: (18) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 65.228054ms)&#xA;May 18 14:51:42.159: INFO: (18) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 65.723553ms)&#xA;May 18 14:51:42.159: INFO: (18) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 66.235605ms)&#xA;May 18 14:51:42.161: INFO: (18) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 68.861685ms)&#xA;May 18 14:51:42.165: INFO: (18) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 72.083912ms)&#xA;May 18 14:51:42.165: INFO: (18) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 72.289506ms)&#xA;May 18 14:51:42.167: INFO: (18) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 73.513203ms)&#xA;May 18 14:51:42.167: INFO: (18) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 73.849071ms)&#xA;May 18 14:51:42.168: INFO: (18) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 74.482911ms)&#xA;May 18 14:51:42.193: INFO: (19) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:... (200; 25.344859ms)&#xA;May 18 14:51:42.211: INFO: (19) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 43.353629ms)&#xA;May 18 14:51:42.211: INFO: (19) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:460/proxy/: tls baz (200; 43.290532ms)&#xA;May 18 14:51:42.214: INFO: (19) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 45.426632ms)&#xA;May 18 14:51:42.218: INFO: (19) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy... (200; 50.205231ms)&#xA;May 18 14:51:42.219: INFO: (19) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 50.434601ms)&#xA;May 18 14:51:42.224: INFO: (19) /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�Zno endpoints available for service &#34;proxy-service-hcvkb-x... (503; 56.434236ms)&#xA;May 18 14:51:42.224: INFO: (19) /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�_no endpoints available for service &#34;http:proxy-service-hc... (503; 56.222993ms)&#xA;May 18 14:51:42.224: INFO: (19) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:462/proxy/: tls qux (200; 55.993831ms)&#xA;May 18 14:51:42.226: INFO: (19) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 54.875711ms)&#xA;May 18 14:51:42.226: INFO: (19) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:162/proxy/: bar (200; 57.739555ms)&#xA;May 18 14:51:42.226: INFO: (19) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 57.541118ms)&#xA;May 18 14:51:42.227: INFO: (19) /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:160/proxy/: foo (200; 56.093635ms)&#xA;May 18 14:51:42.228: INFO: (19) /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/... (200; 59.053071ms)&#xA;May 18 14:51:42.228: INFO: (19) /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster... (200; 57.437289ms)&#xA;May 18 14:51:42.229: INFO: (19) /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/: k8s�&#xA;�&#xA;�v1��Status���&#xA;�&#xA;�������Failure�cno endpoints available for service &#34;https:proxy-service-h... (503; 60.773842ms)&#xA;May 18 14:51:42.248: INFO: Pod proxy-service-hcvkb-p7lgg has the following error logs: &#xA;May 18 14:51:42.249: FAIL: 0 (503; 122.632008ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 122.772505ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 132.436866ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 133.141864ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 135.639443ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0 (503; 138.378166ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;0: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;0: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;0: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;0: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;1 (503; 44.802861ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;1: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;1 (503; 52.488828ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;1: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;1 (503; 55.111783ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1 (503; 61.93644ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1 (503; 64.495482ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;1 (503; 65.048243ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 44.360874ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 48.909571ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 59.725724ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 62.262363ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 64.526332ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2 (503; 65.502701ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;2: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;2: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;2: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;2: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3 (503; 41.276033ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 41.218023ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 55.030499ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;3 (503; 58.556651ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 59.716383ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3 (503; 64.3124ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;3: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;4 (503; 31.19244ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 50.531142ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 52.921724ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;4: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;4: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;4: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;4 (503; 66.968861ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 67.554017ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;4 (503; 68.780481ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 28.546474ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;5: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;5: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;5 (503; 64.459768ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;5 (503; 76.018262ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 76.312858ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 80.268481ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;5 (503; 80.578959ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 33.60103ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;6: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;6 (503; 50.617169ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 50.000594ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;6: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;6 (503; 56.375891ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 55.999408ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;6 (503; 57.879603ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 36.18631ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;7 (503; 38.958359ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 43.144835ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 44.126687ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 47.923834ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7 (503; 49.208871ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;7: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;7: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;7: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;8 (503; 26.402507ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8 (503; 37.518179ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8 (503; 39.200422ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;8: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;8 (503; 48.593496ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8 (503; 49.147133ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;8: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;8: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;8 (503; 52.519596ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 34.695832ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;9: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;9 (503; 39.70234ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 43.901314ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 48.493842ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;9: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;9 (503; 52.249322ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;9 (503; 53.575383ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 29.525922ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 34.406079ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;10 (503; 45.040869ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 46.10032ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 46.448136ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10 (503; 46.17586ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;10: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;10: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;10: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;11 (503; 39.420859ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 42.949061ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 46.755713ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;11 (503; 52.738467ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 54.637805ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;11 (503; 55.290781ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 36.85198ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 42.383759ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;12 (503; 45.065642ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;12: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;12: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;12 (503; 52.099283ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 52.813366ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;12 (503; 54.624628ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;13 (503; 33.63957ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13 (503; 37.927869ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13 (503; 42.286222ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;13 (503; 53.352479ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;13 (503; 56.694907ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13 (503; 57.992501ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;13: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14 (503; 35.587599ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14 (503; 47.411946ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14 (503; 50.881939ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14 (503; 52.080303ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;14: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;14 (503; 56.584105ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;14 (503; 57.640386ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 37.953952ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 40.797856ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 46.066855ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 50.260438ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 57.859097ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15 (503; 57.659702ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;15: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;15: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;15: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;15: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;16 (503; 37.457825ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 61.246233ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 61.517908ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 63.469583ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 73.505788ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16 (503; 72.846851ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;16: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;16: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;16: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;16: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 31.775997ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17 (503; 54.969429ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17 (503; 58.340161ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 63.300482ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 68.033331ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;17: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;17 (503; 69.520098ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;18 (503; 41.107331ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 47.711665ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;18: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;18 (503; 62.961912ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 65.723553ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 68.861685ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18 (503; 72.083912ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;18: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19: path /api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/http:proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/http:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19 (503; 43.353629ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19 (503; 45.426632ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19 (503; 50.434601ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19 (503; 56.434236ms): path /api/v1/namespaces/proxy-1529/services/proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19 (503; 56.222993ms): path /api/v1/namespaces/proxy-1529/services/http:proxy-service-hcvkb:portname2/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;19: path /api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/proxy-service-hcvkb-p7lgg:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:1080/proxy/rewriteme&#34;&gt;test&lt;/a&gt;&#xA;19: path /api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/: wanted &lt;a href=&#34;/api/v1/namespaces/proxy-1529/pods/https:proxy-service-hcvkb-p7lgg:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;, got &lt;a href=&#34;/api/v1/namespaces/vcluster/pods/https:proxy-service-hcvkb-p7lgg-x-proxy-1529-x-vcluster:443/proxy/tlsrewriteme&#34;&gt;test&lt;/a&gt;&#xA;19 (503; 60.773842ms): path /api/v1/namespaces/proxy-1529/services/https:proxy-service-hcvkb:tlsportname1/proxy/ gave status error: {TypeMeta:{Kind: APIVersion:} ListMeta:{SelfLink: ResourceVersion: Continue: RemainingItemCount:&lt;nil&gt;} Status:Failure Message:the server is currently unable to handle the request Reason:ServiceUnavailable Details:&amp;StatusDetails{Name:,Group:,Kind:,Causes:[]StatusCause{StatusCause{Type:UnexpectedServerResponse,Message:unknown,Field:,},},RetryAfterSeconds:0,UID:,} Code:503}&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;STEP: deleting ReplicationController proxy-service-hcvkb in namespace proxy-1529, will wait for the garbage collector to delete the pods&#xA;May 18 14:51:42.332: INFO: Deleting ReplicationController proxy-service-hcvkb took: 13.158279ms&#xA;May 18 14:51:42.932: INFO: Terminating ReplicationController proxy-service-hcvkb pods took: 600.444954ms&#xA;[AfterEach] version v1&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;proxy-1529&#34;.&#xA;STEP: Found 6 events.&#xA;May 18 14:51:50.758: INFO: At 2021-05-18 14:51:29 +0000 UTC - event for proxy-service-hcvkb: {replication-controller } SuccessfulCreate: Created pod: proxy-service-hcvkb-p7lgg&#xA;May 18 14:51:50.758: INFO: At 2021-05-18 14:51:29 +0000 UTC - event for proxy-service-hcvkb-p7lgg: {default-scheduler } Scheduled: Successfully assigned proxy-1529/proxy-service-hcvkb-p7lgg to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:51:50.758: INFO: At 2021-05-18 14:51:30 +0000 UTC - event for proxy-service-hcvkb-p7lgg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:51:50.758: INFO: At 2021-05-18 14:51:30 +0000 UTC - event for proxy-service-hcvkb-p7lgg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container proxy-service-hcvkb&#xA;May 18 14:51:50.758: INFO: At 2021-05-18 14:51:30 +0000 UTC - event for proxy-service-hcvkb-p7lgg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container proxy-service-hcvkb&#xA;May 18 14:51:50.758: INFO: At 2021-05-18 14:51:42 +0000 UTC - event for proxy-service-hcvkb-p7lgg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container proxy-service-hcvkb&#xA;May 18 14:51:50.765: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 14:51:50.765: INFO: &#xA;May 18 14:51:50.772: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:51:50.779: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 8200 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:51:50.780: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:51:50.789: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:51:50.810: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:51:50.810: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:51:50.817: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 8201 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:50:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:47:51 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:51:50.817: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:51:50.825: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:51:50.847: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:51:50.847: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:51:50.855: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 8204 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:50:05 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:50:22 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:51:50.855: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:51:50.864: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:51:50.886: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:51:50.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;proxy-1529&#34; for this suite.&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] SecurityContext should not be able to create pods with unknown usernames" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.572411143"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should update endpoints: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Ingress API should support creating Ingress API operations [Conformance]" classname="Kubernetes e2e suite" time="1.273387695"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide unchanging, static URL paths for kubernetes api services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]" classname="Kubernetes e2e suite" time="125.520259147"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume that cannot be mounted [Slow] should fail due to non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy based on PodSelector with MatchExpressions[Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should be passed when podInfoOnMount=true" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should allow ingress access from updated namespace [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for cronjob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy based on PodSelector and NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]" classname="Kubernetes e2e suite" time="9.961834239"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicationController Should scale from 1 pod to 3 pods and from 3 to 5 and verify decision stability" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]" classname="Kubernetes e2e suite" time="30.263591772"></testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and datastore specified in storage class when there are multiple datastores with the same name under different zones across datacenters" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="122.783920491">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 14:56:43.068: Did not get expected responses within the timeout period of 120.00 seconds.&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:65</failure>
          <system-out>[BeforeEach] [sig-apps] ReplicationController&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:54:40.508: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename replication-controller&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-apps] ReplicationController&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54&#xA;[It] should serve a basic image on each replica with a public image  [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: Creating replication controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c&#xA;May 18 14:54:40.982: INFO: Pod name my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Found 1 pods out of 1&#xA;May 18 14:54:40.982: INFO: Ensuring all pods for ReplicationController &#34;my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c&#34; are running&#xA;May 18 14:54:43.007: INFO: Pod &#34;my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d&#34; is running (conditions: [])&#xA;May 18 14:54:43.008: INFO: Trying to dial the pod&#xA;May 18 14:54:48.052: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:54:53.046: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:54:58.048: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:03.052: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:08.040: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:13.044: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:18.040: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:23.050: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:28.052: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:33.038: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:38.040: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:43.052: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:48.043: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:53.037: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:55:58.046: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:03.046: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:08.044: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:13.058: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:18.046: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:23.051: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:28.034: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:33.048: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:38.048: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:43.040: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:43.067: INFO: Controller my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: Failed to GET from replica 1 [my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d]: the server could not find the requested resource (get pods my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d)&#xA;pod status: v1.PodStatus{Phase:&#34;Pending&#34;, Conditions:[]v1.PodCondition(nil), Message:&#34;&#34;, Reason:&#34;&#34;, NominatedNodeName:&#34;&#34;, HostIP:&#34;&#34;, PodIP:&#34;&#34;, PodIPs:[]v1.PodIP(nil), StartTime:(*v1.Time)(nil), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus(nil), QOSClass:&#34;BestEffort&#34;, EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}&#xA;May 18 14:56:43.067: FAIL: Did not get expected responses within the timeout period of 120.00 seconds.&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/apps.glob..func8.2()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:65 +0x57&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] [sig-apps] ReplicationController&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;replication-controller-1599&#34;.&#xA;STEP: Found 5 events.&#xA;May 18 14:56:43.083: INFO: At 2021-05-18 14:54:40 +0000 UTC - event for my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c: {replication-controller } SuccessfulCreate: Created pod: my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d&#xA;May 18 14:56:43.083: INFO: At 2021-05-18 14:54:41 +0000 UTC - event for my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d: {default-scheduler } Scheduled: Successfully assigned replication-controller-1599/my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:56:43.083: INFO: At 2021-05-18 14:54:41 +0000 UTC - event for my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 14:56:43.083: INFO: At 2021-05-18 14:54:41 +0000 UTC - event for my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c&#xA;May 18 14:56:43.083: INFO: At 2021-05-18 14:54:41 +0000 UTC - event for my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c&#xA;May 18 14:56:43.093: INFO: POD                                                           NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 14:56:43.093: INFO: my-hostname-basic-70d85bed-6744-4bf2-bc4b-28fc7270a71c-dmf9d  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:54:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 14:54:41 +0000 UTC  }]&#xA;May 18 14:56:43.093: INFO: &#xA;May 18 14:56:43.103: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:56:43.111: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 9675 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:55:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:56:43.112: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:56:43.122: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 14:56:43.151: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:56:43.154: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:56:43.166: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 9677 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:55:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:52:53 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 14:56:43.166: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:56:43.174: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 14:56:43.198: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:56:43.198: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:56:43.205: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 9679 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 14:55:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:55:24 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:55:24 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:55:24 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:55:24 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 14:56:43.206: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:56:43.213: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 14:56:43.270: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 14:56:43.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;replication-controller-1599&#34; for this suite.&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] DNS should support configurable pod DNS servers" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for service endpoints using hostNetwork" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume storage capacity exhausted, immediate binding" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy based on NamespaceSelector with MatchExpressions[Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.572930865"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.5891213520000003"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should create NEGs for all ports with the Ingress annotation, and NEGs for the standalone annotation otherwise" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.6340201199999997"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="10.652668643"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for the cluster  [Conformance]" classname="Kubernetes e2e suite" time="2.861417373"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl apply should reuse port when apply to an existing SVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vsphere statefulset [Feature:vsphere] vsphere statefulset testing" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 2 pods to 1 pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to host port conflict [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce multiple ingress policies with ingress allow-all policy taking precedence [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.596182717"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]" classname="Kubernetes e2e suite" time="5.96377822"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for endpoint-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support rollover [Conformance]" classname="Kubernetes e2e suite" time="18.776265927"></testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should update nodePort: udp [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce updated policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should support multiple inline ephemeral volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]" classname="Kubernetes e2e suite" time="1.7834182950000002"></testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with compatible policy and datastore without any zones specified in the storage class fails (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.646603903"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]" classname="Kubernetes e2e suite" time="28.581578802"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should return command exit codes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should delete a job [Conformance]" classname="Kubernetes e2e suite" time="36.874444355"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.520940602"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]" classname="Kubernetes e2e suite" time="16.385918275"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Logging soak [Performance] [Slow] [Disruptive] should survive logging 1KB every 1s seconds, for a duration of 2m0s" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.6619887159999998"></testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - eagerzeroedthick is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI FSGroupPolicy [LinuxOnly] should modify fsGroup if fsGroupPolicy=default" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should support r/w [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl taint [Serial] should remove all the taints with the same key off a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Kubelet should not restart containers across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should fail on mounting directory &#39;adir&#39; when HostPathType is HostPathSocket" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pod with node different from PV&#39;s NodeAffinity should fail scheduling due to different NodeAffinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsNonRoot should not run without a specified user ID" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]" classname="Kubernetes e2e suite" time="2.393855278"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume storage capacity unlimited" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] regular resource usage tracking [Feature:RegularResourceUsageTracking] resource tracking for 100 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for LoadBalancer service with ESIPP on [Slow] [DisabledForLargeClusters] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce egress policy allowing traffic to a server in a different namespace based on PodSelector and NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should be able to mount block device &#39;ablkdev&#39; successfully when HostPathType is HostPathBlockDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsNonRoot should not run with an explicit root user ID [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.5858711960000003"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.59334611"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.607107627"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]" classname="Kubernetes e2e suite" time="9.647651503"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV with storage policy specified in storage class in waitForFirstConsumer binding mode with allowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]" classname="Kubernetes e2e suite" time="7.657996667"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a RuntimeClass with an unconfigured handler [NodeFeature:RuntimeHandler]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should provide secure master service  [Conformance]" classname="Kubernetes e2e suite" time="0.484672846"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type and ports of a service [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should support two pods which share the same volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should fail on mounting block device &#39;ablkdev&#39; when HostPathType is HostPathCharDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] deletion should be idempotent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation fails if no zones are specified in the storage class (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.560781268"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should fail on mounting directory &#39;adir&#39; when HostPathType is HostPathCharDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for endpoint-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="141.494863038">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:01:50.685: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002b80510&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.5.3:32453 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.5.3:32453 over TCP protocol&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3444</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 14:59:33.962: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename services&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745&#xA;[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating service in namespace services-5499&#xA;STEP: creating service affinity-nodeport in namespace services-5499&#xA;STEP: creating replication controller affinity-nodeport in namespace services-5499&#xA;May 18 14:59:37.782: INFO: Creating new exec pod&#xA;May 18 14:59:40.828: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80&#39;&#xA;May 18 14:59:42.408: INFO: rc: 1&#xA;May 18 14:59:42.408: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 affinity-nodeport 80&#xA;nc: connect to affinity-nodeport port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:59:43.408: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80&#39;&#xA;May 18 14:59:44.646: INFO: rc: 1&#xA;May 18 14:59:44.646: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 affinity-nodeport 80&#xA;nc: connect to affinity-nodeport port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:59:45.408: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80&#39;&#xA;May 18 14:59:45.675: INFO: stderr: &#34;+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 14:59:45.676: INFO: stdout: &#34;&#34;&#xA;May 18 14:59:45.676: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.9.95 80&#39;&#xA;May 18 14:59:45.933: INFO: stderr: &#34;+ nc -zv -t -w 2 10.108.9.95 80\nConnection to 10.108.9.95 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 14:59:45.933: INFO: stdout: &#34;&#34;&#xA;May 18 14:59:45.933: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 14:59:48.215: INFO: rc: 1&#xA;May 18 14:59:48.215: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:59:49.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 14:59:51.443: INFO: rc: 1&#xA;May 18 14:59:51.443: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:59:52.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 14:59:54.419: INFO: rc: 1&#xA;May 18 14:59:54.419: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:59:55.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 14:59:57.446: INFO: rc: 1&#xA;May 18 14:59:57.446: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 14:59:58.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:00.420: INFO: rc: 1&#xA;May 18 15:00:00.420: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:01.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:03.447: INFO: rc: 1&#xA;May 18 15:00:03.447: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:04.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:06.599: INFO: rc: 1&#xA;May 18 15:00:06.599: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:07.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:09.478: INFO: rc: 1&#xA;May 18 15:00:09.478: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:10.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:12.444: INFO: rc: 1&#xA;May 18 15:00:12.444: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:13.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:15.443: INFO: rc: 1&#xA;May 18 15:00:15.443: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:16.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:18.466: INFO: rc: 1&#xA;May 18 15:00:18.466: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:19.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:21.423: INFO: rc: 1&#xA;May 18 15:00:21.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:22.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:24.434: INFO: rc: 1&#xA;May 18 15:00:24.435: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:25.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:27.470: INFO: rc: 1&#xA;May 18 15:00:27.470: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:28.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:30.442: INFO: rc: 1&#xA;May 18 15:00:30.442: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:31.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:33.461: INFO: rc: 1&#xA;May 18 15:00:33.461: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:34.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:36.451: INFO: rc: 1&#xA;May 18 15:00:36.451: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:37.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:39.458: INFO: rc: 1&#xA;May 18 15:00:39.458: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:40.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:42.466: INFO: rc: 1&#xA;May 18 15:00:42.466: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:43.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:45.464: INFO: rc: 1&#xA;May 18 15:00:45.464: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:46.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:48.433: INFO: rc: 1&#xA;May 18 15:00:48.433: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:49.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:51.441: INFO: rc: 1&#xA;May 18 15:00:51.441: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:52.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:54.423: INFO: rc: 1&#xA;May 18 15:00:54.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:55.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:00:57.471: INFO: rc: 1&#xA;May 18 15:00:57.471: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:00:58.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:00.418: INFO: rc: 1&#xA;May 18 15:01:00.418: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:01.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:03.455: INFO: rc: 1&#xA;May 18 15:01:03.455: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:04.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:06.462: INFO: rc: 1&#xA;May 18 15:01:06.462: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:07.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:09.471: INFO: rc: 1&#xA;May 18 15:01:09.471: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:10.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:12.449: INFO: rc: 1&#xA;May 18 15:01:12.449: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:13.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:15.446: INFO: rc: 1&#xA;May 18 15:01:15.446: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:16.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:18.428: INFO: rc: 1&#xA;May 18 15:01:18.428: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:19.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:21.426: INFO: rc: 1&#xA;May 18 15:01:21.426: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:22.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:24.450: INFO: rc: 1&#xA;May 18 15:01:24.450: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:25.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:27.439: INFO: rc: 1&#xA;May 18 15:01:27.439: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:28.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:30.430: INFO: rc: 1&#xA;May 18 15:01:30.430: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:31.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:33.467: INFO: rc: 1&#xA;May 18 15:01:33.467: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:34.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:36.426: INFO: rc: 1&#xA;May 18 15:01:36.426: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:37.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:39.556: INFO: rc: 1&#xA;May 18 15:01:39.556: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:40.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:42.429: INFO: rc: 1&#xA;May 18 15:01:42.429: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:43.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:45.423: INFO: rc: 1&#xA;May 18 15:01:45.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:46.216: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:48.444: INFO: rc: 1&#xA;May 18 15:01:48.444: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:48.444: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453&#39;&#xA;May 18 15:01:50.684: INFO: rc: 1&#xA;May 18 15:01:50.684: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-5499 exec execpod-affinitydkrl9 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.3 32453:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.3 32453&#xA;nc: connect to 10.108.5.3 port 32453 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:01:50.685: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002b80510&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.5.3:32453 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.5.3:32453 over TCP protocol&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBServiceWithOptionalTransition(0xc000267a20, 0x5607080, 0xc0019fa6e0, 0xc000853b80, 0x0)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3444 +0x62e&#xA;k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBService(...)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3403&#xA;k8s.io/kubernetes/test/e2e/network.glob..func24.28()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2452 +0xa5&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;May 18 15:01:50.687: INFO: Cleaning up the exec pod&#xA;STEP: deleting ReplicationController affinity-nodeport in namespace services-5499, will wait for the garbage collector to delete the pods&#xA;May 18 15:01:50.773: INFO: Deleting ReplicationController affinity-nodeport took: 10.6624ms&#xA;May 18 15:01:51.573: INFO: Terminating ReplicationController affinity-nodeport pods took: 800.179771ms&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;services-5499&#34;.&#xA;STEP: Found 22 events.&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:34 +0000 UTC - event for affinity-nodeport: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-znltg&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:34 +0000 UTC - event for affinity-nodeport: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-pjp8q&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:34 +0000 UTC - event for affinity-nodeport: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-dlxlw&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:34 +0000 UTC - event for affinity-nodeport-dlxlw: {default-scheduler } Scheduled: Successfully assigned services-5499/affinity-nodeport-dlxlw to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:34 +0000 UTC - event for affinity-nodeport-pjp8q: {default-scheduler } Scheduled: Successfully assigned services-5499/affinity-nodeport-pjp8q to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:34 +0000 UTC - event for affinity-nodeport-znltg: {default-scheduler } Scheduled: Successfully assigned services-5499/affinity-nodeport-znltg to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:35 +0000 UTC - event for affinity-nodeport-pjp8q: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:35 +0000 UTC - event for affinity-nodeport-pjp8q: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-dlxlw: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-dlxlw: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-dlxlw: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-pjp8q: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport&#xA;May 18 15:01:55.221: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-znltg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-znltg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 14:59:36 +0000 UTC - event for affinity-nodeport-znltg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 14:59:37 +0000 UTC - event for execpod-affinitydkrl9: {default-scheduler } Scheduled: Successfully assigned services-5499/execpod-affinitydkrl9 to gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 14:59:38 +0000 UTC - event for execpod-affinitydkrl9: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 14:59:38 +0000 UTC - event for execpod-affinitydkrl9: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Created: Created container agnhost-container&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 14:59:38 +0000 UTC - event for execpod-affinitydkrl9: {kubelet gke-cluster-2-default-pool-426afc83-4s72} Started: Started container agnhost-container&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 15:01:51 +0000 UTC - event for affinity-nodeport-dlxlw: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 15:01:51 +0000 UTC - event for affinity-nodeport-pjp8q: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport&#xA;May 18 15:01:55.222: INFO: At 2021-05-18 15:01:51 +0000 UTC - event for affinity-nodeport-znltg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport&#xA;May 18 15:01:55.229: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 15:01:55.229: INFO: &#xA;May 18 15:01:55.236: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:01:55.243: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 10657 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:01:55.243: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:01:55.252: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:01:55.316: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:01:55.317: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:01:55.325: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 10659 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 14:57:54 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:01:55.326: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:01:55.334: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:01:55.379: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:01:55.379: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:01:55.386: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 10668 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:00:06 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:00:25 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:00:25 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:00:25 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:00:25 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:01:55.387: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:01:55.396: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:01:55.438: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:01:55.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;services-5499&#34; for this suite.&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets Should fail non-optional pod creation due to the key in the secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Memory Limits [Serial] [Slow] Allocatable node memory should be equal to a calculated allocatable memory value" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Service endpoints latency should not be very high  [Conformance]" classname="Kubernetes e2e suite" time="12.354425443"></testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policies to check ingress and egress policies can be controlled independently based on PodSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should have accelerator metrics [Feature:StackdriverAcceleratorMonitoring]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should allow egress access to server in CIDR block [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]" classname="Kubernetes e2e suite" time="0.719610582"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Stackdriver Metadata Agent [Feature:StackdriverMetadataAgent]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Conntrack should be able to preserve UDP traffic when server pod cycles for a ClusterIP service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere [Feature:vsphere] should test that deleting a PVC before the pod does not cause pod deletion to fail on vsphere volume detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicaSet Should scale from 5 pods to 3 pods and from 3 to 1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] server version should find the server version [Conformance]" classname="Kubernetes e2e suite" time="0.474821422"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]" classname="Kubernetes e2e suite" time="0.475187941"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should create endpoints for unready pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with invalid zone specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]" classname="Kubernetes e2e suite" time="23.156453517"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]" classname="Kubernetes e2e suite" time="183.713155773">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:05:35.867: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0028aab80&gt;: {&#xA;        s: &#34;replicaset \&#34;rs-pod1\&#34; never had desired number of .status.availableReplicas&#34;,&#xA;    }&#xA;    replicaset &#34;rs-pod1&#34; never had desired number of .status.availableReplicas&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:799</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:02:32.650: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename sched-preemption&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90&#xA;May 18 15:02:33.167: INFO: Waiting up to 1m0s for all nodes to be ready&#xA;May 18 15:03:33.226: INFO: Waiting for terminating namespaces to be deleted...&#xA;[BeforeEach] PreemptionExecutionPath&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:03:33.234: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename sched-preemption-path&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] PreemptionExecutionPath&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488&#xA;STEP: Finding an available node&#xA;STEP: Trying to launch a pod without a label to get a node which can launch it.&#xA;STEP: Explicitly delete pod here to free the resource it takes.&#xA;May 18 15:03:35.760: INFO: found a healthy node: gke-cluster-2-default-pool-426afc83-rll8&#xA;[It] runs ReplicaSets to verify preemption running path [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:05:35.867: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0028aab80&gt;: {&#xA;        s: &#34;replicaset \&#34;rs-pod1\&#34; never had desired number of .status.availableReplicas&#34;,&#xA;    }&#xA;    replicaset &#34;rs-pod1&#34; never had desired number of .status.availableReplicas&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/scheduling.runPauseRS(0xc000f44420, 0x1, 0x4dc05a6, 0x4, 0xc002d072a0, 0x1a, 0x0, 0x0, 0xc002c0bef0, 0xc002c0bd40, ...)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:799 +0xe5&#xA;k8s.io/kubernetes/test/e2e/scheduling.glob..func5.6.3()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:614 +0xc65&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] PreemptionExecutionPath&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;sched-preemption-path-1543&#34;.&#xA;STEP: Found 6 events.&#xA;May 18 15:05:35.876: INFO: At 2021-05-18 15:03:33 +0000 UTC - event for without-label: {default-scheduler } Scheduled: Successfully assigned sched-preemption-path-1543/without-label to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:35.876: INFO: At 2021-05-18 15:03:34 +0000 UTC - event for without-label: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/pause:3.2&#34; already present on machine&#xA;May 18 15:05:35.876: INFO: At 2021-05-18 15:03:34 +0000 UTC - event for without-label: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container without-label&#xA;May 18 15:05:35.876: INFO: At 2021-05-18 15:03:34 +0000 UTC - event for without-label: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container without-label&#xA;May 18 15:05:35.876: INFO: At 2021-05-18 15:03:35 +0000 UTC - event for rs-pod1: {replicaset-controller } SuccessfulCreate: Created pod: rs-pod1-pfsxq&#xA;May 18 15:05:35.876: INFO: At 2021-05-18 15:03:35 +0000 UTC - event for rs-pod1-pfsxq: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;rs-pod1-pfsxq-x-sched-preemption-path-1543-x-vcluster&#34; is forbidden: no PriorityClass with name p1 was found&#xA;May 18 15:05:35.884: INFO: POD            NODE  PHASE    GRACE  CONDITIONS&#xA;May 18 15:05:35.884: INFO: rs-pod1-pfsxq        Pending         []&#xA;May 18 15:05:35.884: INFO: &#xA;May 18 15:05:35.892: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:05:35.898: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 12719 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:05:35.899: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:05:35.907: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:05:35.941: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:05:35.941: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:05:35.949: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 12720 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:05:35.950: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:05:35.958: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:05:35.993: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:05:35.993: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:36.001: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 12722 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:05:36.001: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:36.008: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:36.038: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:05:36.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;sched-preemption-path-1543&#34; for this suite.&#xA;[AfterEach] PreemptionExecutionPath&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462&#xA;May 18 15:05:36.066: INFO: List existing PriorityClasses:&#xA;May 18 15:05:36.066: INFO: system-node-critical/2000001000 created at 2021-05-18 13:39:38 +0000 UTC&#xA;May 18 15:05:36.066: INFO: system-cluster-critical/2000000000 created at 2021-05-18 13:39:38 +0000 UTC&#xA;May 18 15:05:36.066: INFO: sched-preemption-low-priority/1 created at 2021-05-18 15:02:33 +0000 UTC&#xA;May 18 15:05:36.066: INFO: sched-preemption-medium-priority/100 created at 2021-05-18 15:02:33 +0000 UTC&#xA;May 18 15:05:36.066: INFO: sched-preemption-high-priority/1000 created at 2021-05-18 15:02:33 +0000 UTC&#xA;May 18 15:05:36.066: INFO: p1/1 created at 2021-05-18 15:03:35 +0000 UTC&#xA;May 18 15:05:36.066: INFO: p2/2 created at 2021-05-18 15:03:35 +0000 UTC&#xA;May 18 15:05:36.066: INFO: p3/3 created at 2021-05-18 15:03:35 +0000 UTC&#xA;May 18 15:05:36.066: INFO: p4/4 created at 2021-05-18 15:03:35 +0000 UTC&#xA;[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;sched-preemption-6338&#34;.&#xA;STEP: Found 0 events.&#xA;May 18 15:05:36.150: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 15:05:36.151: INFO: &#xA;May 18 15:05:36.159: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:05:36.167: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 12719 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:05:36.168: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:05:36.175: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:05:36.198: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:05:36.198: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:05:36.207: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 12720 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:05:36.208: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:05:36.216: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:05:36.236: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:05:36.236: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:36.245: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 12722 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:05:36.246: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:36.253: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:05:36.270: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:05:36.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;sched-preemption-6338&#34; for this suite.&#xA;[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted by liveness probe because startup probe delays it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] Deployment Should scale from 5 pods to 3 pods and from 3 to 1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create a functioning NodePort service [Conformance]" classname="Kubernetes e2e suite" time="136.424314645">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:07:52.554: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002822780&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31662 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31662 over TCP protocol&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:1179</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:05:36.366: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename services&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745&#xA;[It] should be able to create a functioning NodePort service [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating service nodeport-test with type=NodePort in namespace services-9782&#xA;STEP: creating replication controller nodeport-test in namespace services-9782&#xA;May 18 15:05:40.096: INFO: Creating new exec pod&#xA;May 18 15:05:43.162: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80&#39;&#xA;May 18 15:05:44.468: INFO: rc: 1&#xA;May 18 15:05:44.468: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 nodeport-test 80&#xA;nc: connect to nodeport-test port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:05:45.468: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80&#39;&#xA;May 18 15:05:46.707: INFO: rc: 1&#xA;May 18 15:05:46.707: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 nodeport-test 80&#xA;nc: connect to nodeport-test port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:05:47.468: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80&#39;&#xA;May 18 15:05:47.686: INFO: stderr: &#34;+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 15:05:47.686: INFO: stdout: &#34;&#34;&#xA;May 18 15:05:47.686: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.6.52 80&#39;&#xA;May 18 15:05:47.883: INFO: stderr: &#34;+ nc -zv -t -w 2 10.108.6.52 80\nConnection to 10.108.6.52 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 15:05:47.883: INFO: stdout: &#34;&#34;&#xA;May 18 15:05:47.883: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:05:50.150: INFO: rc: 1&#xA;May 18 15:05:50.150: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:05:51.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:05:53.356: INFO: rc: 1&#xA;May 18 15:05:53.356: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:05:54.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:05:56.377: INFO: rc: 1&#xA;May 18 15:05:56.377: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:05:57.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:05:59.369: INFO: rc: 1&#xA;May 18 15:05:59.369: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:00.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:02.378: INFO: rc: 1&#xA;May 18 15:06:02.378: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:03.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:05.360: INFO: rc: 1&#xA;May 18 15:06:05.360: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:06.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:08.360: INFO: rc: 1&#xA;May 18 15:06:08.360: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:09.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:11.368: INFO: rc: 1&#xA;May 18 15:06:11.368: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:12.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:14.371: INFO: rc: 1&#xA;May 18 15:06:14.371: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:15.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:17.379: INFO: rc: 1&#xA;May 18 15:06:17.379: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:18.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:20.355: INFO: rc: 1&#xA;May 18 15:06:20.355: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:21.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:23.365: INFO: rc: 1&#xA;May 18 15:06:23.365: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:24.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:26.377: INFO: rc: 1&#xA;May 18 15:06:26.377: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:27.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:29.395: INFO: rc: 1&#xA;May 18 15:06:29.395: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:30.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:32.462: INFO: rc: 1&#xA;May 18 15:06:32.462: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:33.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:35.359: INFO: rc: 1&#xA;May 18 15:06:35.359: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:36.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:38.362: INFO: rc: 1&#xA;May 18 15:06:38.362: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:39.151: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:41.341: INFO: rc: 1&#xA;May 18 15:06:41.342: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:42.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:44.383: INFO: rc: 1&#xA;May 18 15:06:44.383: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:45.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:47.354: INFO: rc: 1&#xA;May 18 15:06:47.354: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:48.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:50.361: INFO: rc: 1&#xA;May 18 15:06:50.361: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:51.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:53.364: INFO: rc: 1&#xA;May 18 15:06:53.364: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:54.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:56.354: INFO: rc: 1&#xA;May 18 15:06:56.354: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:06:57.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:06:59.408: INFO: rc: 1&#xA;May 18 15:06:59.408: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:00.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:02.379: INFO: rc: 1&#xA;May 18 15:07:02.379: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:03.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:05.459: INFO: rc: 1&#xA;May 18 15:07:05.459: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:06.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:08.374: INFO: rc: 1&#xA;May 18 15:07:08.374: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:09.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:11.383: INFO: rc: 1&#xA;May 18 15:07:11.383: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:12.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:14.357: INFO: rc: 1&#xA;May 18 15:07:14.357: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:15.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:17.363: INFO: rc: 1&#xA;May 18 15:07:17.363: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:18.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:20.368: INFO: rc: 1&#xA;May 18 15:07:20.368: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:21.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:23.366: INFO: rc: 1&#xA;May 18 15:07:23.366: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:24.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:26.362: INFO: rc: 1&#xA;May 18 15:07:26.362: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:27.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:29.387: INFO: rc: 1&#xA;May 18 15:07:29.387: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:30.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:32.555: INFO: rc: 1&#xA;May 18 15:07:32.555: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:33.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:35.691: INFO: rc: 1&#xA;May 18 15:07:35.691: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:36.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:38.354: INFO: rc: 1&#xA;May 18 15:07:38.354: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:39.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:41.354: INFO: rc: 1&#xA;May 18 15:07:41.354: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:42.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:44.379: INFO: rc: 1&#xA;May 18 15:07:44.379: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:45.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:47.377: INFO: rc: 1&#xA;May 18 15:07:47.377: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:48.150: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:50.361: INFO: rc: 1&#xA;May 18 15:07:50.361: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:50.361: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662&#39;&#xA;May 18 15:07:52.553: INFO: rc: 1&#xA;May 18 15:07:52.553: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-9782 exec execpodgx5xv -- /bin/sh -x -c nc -zv -t -w 2 10.108.1.212 31662:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.1.212 31662&#xA;nc: connect to 10.108.1.212 port 31662 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:07:52.554: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002822780&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31662 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.1.212:31662 over TCP protocol&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.glob..func24.11()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:1179 +0x265&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;services-9782&#34;.&#xA;STEP: Found 15 events.&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:37 +0000 UTC - event for nodeport-test: {replication-controller } SuccessfulCreate: Created pod: nodeport-test-vt2bt&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:37 +0000 UTC - event for nodeport-test: {replication-controller } SuccessfulCreate: Created pod: nodeport-test-b7487&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:37 +0000 UTC - event for nodeport-test-b7487: {default-scheduler } Scheduled: Successfully assigned services-9782/nodeport-test-b7487 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:37 +0000 UTC - event for nodeport-test-vt2bt: {default-scheduler } Scheduled: Successfully assigned services-9782/nodeport-test-vt2bt to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:37 +0000 UTC - event for nodeport-test-vt2bt: {pod-syncer } SyncError: Error syncing to physical cluster: Operation cannot be fulfilled on pods &#34;nodeport-test-vt2bt-x-services-9782-x-vcluster&#34;: the object has been modified; please apply your changes to the latest version and try again&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:38 +0000 UTC - event for nodeport-test-b7487: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container nodeport-test&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:38 +0000 UTC - event for nodeport-test-b7487: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container nodeport-test&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:38 +0000 UTC - event for nodeport-test-b7487: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:38 +0000 UTC - event for nodeport-test-vt2bt: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container nodeport-test&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:38 +0000 UTC - event for nodeport-test-vt2bt: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container nodeport-test&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:38 +0000 UTC - event for nodeport-test-vt2bt: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:40 +0000 UTC - event for execpodgx5xv: {default-scheduler } Scheduled: Successfully assigned services-9782/execpodgx5xv to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:40 +0000 UTC - event for execpodgx5xv: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:40 +0000 UTC - event for execpodgx5xv: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost-container&#xA;May 18 15:07:52.570: INFO: At 2021-05-18 15:05:41 +0000 UTC - event for execpodgx5xv: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost-container&#xA;May 18 15:07:52.577: INFO: POD                  NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 15:07:52.577: INFO: nodeport-test-b7487  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:37 +0000 UTC  }]&#xA;May 18 15:07:52.577: INFO: nodeport-test-vt2bt  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:37 +0000 UTC  }]&#xA;May 18 15:07:52.577: INFO: execpodgx5xv         gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:05:40 +0000 UTC  }]&#xA;May 18 15:07:52.577: INFO: &#xA;May 18 15:07:52.585: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:07:52.593: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 12719 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:07:52.593: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:07:52.601: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:07:52.652: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:07:52.652: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:07:52.661: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 12720 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:11 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:02:56 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:07:52.661: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:07:52.669: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:07:52.714: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:07:52.714: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:07:52.722: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 12722 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:05:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:05:26 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:07:52.723: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:07:52.730: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:07:52.768: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:07:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;services-9782&#34; for this suite.&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a secret for a workload the node has access to should succeed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should be able to handle large requests: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.564024975"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy to allow traffic from pods within server namespace based on PodSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Reboot [Disruptive] [Feature:Reboot] each node by dropping all inbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values and a VSAN datastore is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for node-Service: sctp [Feature:SCTPConnectivity][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] GKE node pools [Feature:GKENodePool] should create a cluster with multiple node pools [Feature:GKENodePool]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]" classname="Kubernetes e2e suite" time="36.399472051"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.59336985"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with an invalid VSAN capability along with a compatible zone combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]" classname="Kubernetes e2e suite" time="18.110059205"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes Default StorageClass pods that use multiple volumes should be reschedulable [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should support multiple inline ephemeral volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should preserve source pod IP for traffic thru service cluster IP [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s memory request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.6197588229999997"></testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.595407734"></testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] [Flaky] kubectl explain works for CR with the same resource name as built-in object." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale down when expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] [Disruptive]NodeLease NodeLease deletion node lease should be deleted when corresponding node is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] DNS horizontal autoscaling [Serial] [Slow] kube-dns-autoscaler should scale kube-dns pods when cluster size changed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] etcd Upgrade [Feature:EtcdUpgrade] etcd upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIServiceAccountToken [Feature:CSIServiceAccountToken] token should not be plumbed down when CSIDriver is not deployed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by removing the default annotation [Serial] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale up twice [Feature:ClusterAutoscalerScalability2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify fstype - ext3 formatted volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for LoadBalancer service with ESIPP off [Slow] [DisabledForLargeClusters] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]" classname="Kubernetes e2e suite" time="6.566550733"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pod with node different from PV&#39;s NodeAffinity should fail scheduling due to different NodeSelector" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV with storage policy specified in storage class in waitForFirstConsumer binding mode" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and there is another node pool that is not autoscaled [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="4.749581474"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy logs on node using proxy subresource " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI FSGroupPolicy [LinuxOnly] should not modify fsGroup if fsGroupPolicy=None" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI CSIDriver deployment after pod creation using non-attachable mock driver should bringup pod after deploying CSIDriver attach=false [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]" classname="Kubernetes e2e suite" time="43.365608536"></testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]" classname="Kubernetes e2e suite" time="1.660021309"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]" classname="Kubernetes e2e suite" time="3.758115926"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="23.219792778"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]" classname="Kubernetes e2e suite" time="6.142857726"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should fail on mounting non-existent block device &#39;does-not-exist-blk-dev&#39; when HostPathType is HostPathBlockDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should only allow access from service loadbalancer source ranges [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl client-side validation should create/apply a CR with unknown fields for CRD with no validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should update ConfigMap successfully" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should support cascading deletion of custom resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.602866921"></testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.566020648"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]" classname="Kubernetes e2e suite" time="6.5700288669999996"></testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.553729723"></testcase>
      <testcase name="[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]" classname="Kubernetes e2e suite" time="1.125678353"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Windows volume mounts  check volume mount permissions container should have readOnly permissions on emptyDir" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic Snapshot (retain policy)] snapshottable-stress[Feature:VolumeSnapshotDataSource] should support snapshotting of many volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0.67263519"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should support exec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicationController Should scale from 5 pods to 3 pods and from 3 to 1 and verify decision stability" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with terminating scopes through scope selectors." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="11.16901852"></testcase>
      <testcase name="[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]" classname="Kubernetes e2e suite" time="4.762756042"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should test that deleting a claim before the volume is provisioned deletes the volume." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vcp at scale [Feature:vsphere]  vsphere scale tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI online volume expansion should expand volume without restarting pod if attach=off, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] SCTP [Feature:SCTP] [LinuxOnly] should create a ClusterIP Service with SCTP ports" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should update endpoints: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should create read/write inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in the storage class. (No shared datastores exist among both zones)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl client-side validation should create/apply a valid CR with arbitrary-extra properties for CRD with partially-specified validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]" classname="Kubernetes e2e suite" time="135.873194907"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should mount projected service account token [Conformance]" classname="Kubernetes e2e suite" time="2.589012436"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should serve multiport endpoints from pods  [Conformance]" classname="Kubernetes e2e suite" time="4.804907049"></testcase>
      <testcase name="[sig-storage] HostPathType Directory [Slow] Should fail on mounting non-existent directory &#39;does-not-exist-dir&#39; when HostPathType is HostPathDirectory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic Snapshot (retain policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.522819025"></testcase>
      <testcase name="[sig-storage] CSI mock volume CSI NodeStage error cases [Slow] should not call NodeUnstage after NodeStage final error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PV Protection Verify &#34;immediate&#34; deletion of a PV that is not bound to a PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy to allow traffic only from a pod in a different namespace based on PodSelector and NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should support two pods which share the same volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging using Elasticsearch [Feature:Elasticsearch] should check that logs from containers are ingested into Elasticsearch" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] PodTemplates should delete a collection of pod templates [Conformance]" classname="Kubernetes e2e suite" time="0.610876506"></testcase>
      <testcase name="[sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with scheduling without taints " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target value from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should support OIDC discovery of service account issuer [Feature:ServiceAccountIssuerDiscovery]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.627456649"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.968272164"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale up correct target pool [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should fail on mounting socket &#39;asocket&#39; when HostPathType is HostPathBlockDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by changing the default annotation [Serial] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working zookeeper cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] GMSA Full [Serial] [Slow] GMSA support works end to end" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] PodTopologySpread Preemption validates proper pods are preempted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="22.519705528"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion should expand volume without restarting pod if nodeExpansion=off" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIStorageCapacity [Feature:CSIStorageCapacity] CSIStorageCapacity used, have capacity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Ports Security Check [Feature:KubeletSecurity] should not have port 4194 open on its all public IP addresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]" classname="Kubernetes e2e suite" time="17.583853911"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]" classname="Kubernetes e2e suite" time="16.852798527"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should fail on mounting file &#39;afile&#39; when HostPathType is HostPathDirectory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Downward API [Serial] [Disruptive] [NodeFeature:EphemeralStorage] Downward API tests for local ephemeral storage should provide container&#39;s limits.ephemeral-storage and requests.ephemeral-storage as env vars" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume Snapshots [Feature:VolumeSnapshotDataSource] volumesnapshotcontent and pvc in Bound state with deletion timestamp set should not get deleted while snapshot finalizer exists" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.629960196"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Windows volume mounts  check volume mount permissions container should have readOnly permissions on hostMapPath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for the cluster [Provider:GCE]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s cpu request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.639561616"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining system pods with pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should remove from active list jobs that have been deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should create a single stack service with cluster ip from primary service range [Feature:IPv6DualStackAlphaFeature:Phase2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with incompatible storage policy along with compatible zone and datastore combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.5502432600000002"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.611625043"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI NodeStage error cases [Slow] should retry NodeStage after NodeStage ephemeral error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned Snapshot (retain policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement [Feature:vsphere] should create and delete pod with the same volume source on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] client-go should negotiate watch and report errors with accept &#34;application/json,application/vnd.kubernetes.protobuf&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]" classname="Kubernetes e2e suite" time="13.739471026"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when node&#39;s API object is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]" classname="Kubernetes e2e suite" time="7.870159326"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap Should fail non-optional pod creation due to the key in the configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should create service with ipv4,v6 cluster ip [Feature:IPv6DualStackAlphaFeature:Phase2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid capability name objectSpaceReserve is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]" classname="Kubernetes e2e suite" time="304.748289389"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce multiple egress policies with egress allow-all policy taking precedence [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="6.630838791"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should sign the new added bootstrap tokens" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two External metrics from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to switch between IG and NEG modes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default arguments (docker cmd) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.55915014"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="144.863318968">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:22:51.803: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002a16f80&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.0.52:32511 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.0.52:32511 over TCP protocol&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3444</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:20:36.181: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename services&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745&#xA;[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating service in namespace services-2558&#xA;STEP: creating service affinity-nodeport-transition in namespace services-2558&#xA;STEP: creating replication controller affinity-nodeport-transition in namespace services-2558&#xA;May 18 15:20:39.811: INFO: Creating new exec pod&#xA;May 18 15:20:42.860: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80&#39;&#xA;May 18 15:20:44.691: INFO: rc: 1&#xA;May 18 15:20:44.691: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 affinity-nodeport-transition 80&#xA;nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:20:45.691: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80&#39;&#xA;May 18 15:20:46.931: INFO: stderr: &#34;+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 15:20:46.931: INFO: stdout: &#34;&#34;&#xA;May 18 15:20:46.932: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.8.52 80&#39;&#xA;May 18 15:20:47.167: INFO: stderr: &#34;+ nc -zv -t -w 2 10.108.8.52 80\nConnection to 10.108.8.52 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 15:20:47.167: INFO: stdout: &#34;&#34;&#xA;May 18 15:20:47.167: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:20:49.371: INFO: rc: 1&#xA;May 18 15:20:49.371: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:20:50.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:20:52.589: INFO: rc: 1&#xA;May 18 15:20:52.589: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:20:53.372: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:20:55.787: INFO: rc: 1&#xA;May 18 15:20:55.787: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:20:56.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:20:58.571: INFO: rc: 1&#xA;May 18 15:20:58.571: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:20:59.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:01.579: INFO: rc: 1&#xA;May 18 15:21:01.579: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:02.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:04.582: INFO: rc: 1&#xA;May 18 15:21:04.582: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:05.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:07.561: INFO: rc: 1&#xA;May 18 15:21:07.561: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:08.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:10.591: INFO: rc: 1&#xA;May 18 15:21:10.591: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:11.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:13.577: INFO: rc: 1&#xA;May 18 15:21:13.577: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:14.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:16.575: INFO: rc: 1&#xA;May 18 15:21:16.575: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:17.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:19.596: INFO: rc: 1&#xA;May 18 15:21:19.596: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:20.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:22.618: INFO: rc: 1&#xA;May 18 15:21:22.618: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:23.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:25.586: INFO: rc: 1&#xA;May 18 15:21:25.586: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:26.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:28.591: INFO: rc: 1&#xA;May 18 15:21:28.591: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:29.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:31.561: INFO: rc: 1&#xA;May 18 15:21:31.561: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:32.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:34.600: INFO: rc: 1&#xA;May 18 15:21:34.600: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:35.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:37.583: INFO: rc: 1&#xA;May 18 15:21:37.583: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:38.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:40.586: INFO: rc: 1&#xA;May 18 15:21:40.586: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:41.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:43.595: INFO: rc: 1&#xA;May 18 15:21:43.595: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:44.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:46.603: INFO: rc: 1&#xA;May 18 15:21:46.603: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:47.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:49.585: INFO: rc: 1&#xA;May 18 15:21:49.585: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:50.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:52.577: INFO: rc: 1&#xA;May 18 15:21:52.577: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:53.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:55.585: INFO: rc: 1&#xA;May 18 15:21:55.585: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:56.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:21:58.574: INFO: rc: 1&#xA;May 18 15:21:58.574: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:21:59.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:01.581: INFO: rc: 1&#xA;May 18 15:22:01.581: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:02.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:04.606: INFO: rc: 1&#xA;May 18 15:22:04.606: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:05.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:07.733: INFO: rc: 1&#xA;May 18 15:22:07.733: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:08.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:10.636: INFO: rc: 1&#xA;May 18 15:22:10.636: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:11.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:13.604: INFO: rc: 1&#xA;May 18 15:22:13.604: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:14.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:16.580: INFO: rc: 1&#xA;May 18 15:22:16.580: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:17.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:19.589: INFO: rc: 1&#xA;May 18 15:22:19.589: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:20.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:22.606: INFO: rc: 1&#xA;May 18 15:22:22.606: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:23.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:25.582: INFO: rc: 1&#xA;May 18 15:22:25.582: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:26.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:28.625: INFO: rc: 1&#xA;May 18 15:22:28.625: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:29.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:31.573: INFO: rc: 1&#xA;May 18 15:22:31.573: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:32.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:34.588: INFO: rc: 1&#xA;May 18 15:22:34.588: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:35.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:37.603: INFO: rc: 1&#xA;May 18 15:22:37.603: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:38.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:40.575: INFO: rc: 1&#xA;May 18 15:22:40.575: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:41.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:43.591: INFO: rc: 1&#xA;May 18 15:22:43.591: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:44.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:46.597: INFO: rc: 1&#xA;May 18 15:22:46.597: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:47.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:49.596: INFO: rc: 1&#xA;May 18 15:22:49.596: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:49.596: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511&#39;&#xA;May 18 15:22:51.802: INFO: rc: 1&#xA;May 18 15:22:51.802: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2558 exec execpod-affinityc66mq -- /bin/sh -x -c nc -zv -t -w 2 10.108.0.52 32511:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.0.52 32511&#xA;nc: connect to 10.108.0.52 port 32511 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:22:51.803: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002a16f80&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.0.52:32511 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.0.52:32511 over TCP protocol&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBServiceWithOptionalTransition(0xc000267a20, 0x5607080, 0xc00571f4a0, 0xc0004a8c80, 0x1)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3444 +0x62e&#xA;k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBServiceWithTransition(...)&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3399&#xA;k8s.io/kubernetes/test/e2e/network.glob..func24.30()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2485 +0xa5&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;May 18 15:22:51.804: INFO: Cleaning up the exec pod&#xA;STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2558, will wait for the garbage collector to delete the pods&#xA;May 18 15:22:51.894: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.778049ms&#xA;May 18 15:22:52.694: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 800.201966ms&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;services-2558&#34;.&#xA;STEP: Found 23 events.&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:36 +0000 UTC - event for affinity-nodeport-transition: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-transition-2jhpd&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:36 +0000 UTC - event for affinity-nodeport-transition: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-transition-mmxrg&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:36 +0000 UTC - event for affinity-nodeport-transition: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-transition-8g82j&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:36 +0000 UTC - event for affinity-nodeport-transition-2jhpd: {default-scheduler } Scheduled: Successfully assigned services-2558/affinity-nodeport-transition-2jhpd to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:36 +0000 UTC - event for affinity-nodeport-transition-8g82j: {default-scheduler } Scheduled: Successfully assigned services-2558/affinity-nodeport-transition-8g82j to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:36 +0000 UTC - event for affinity-nodeport-transition-mmxrg: {default-scheduler } Scheduled: Successfully assigned services-2558/affinity-nodeport-transition-mmxrg to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:37 +0000 UTC - event for affinity-nodeport-transition-mmxrg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:37 +0000 UTC - event for affinity-nodeport-transition-mmxrg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-2jhpd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-2jhpd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-2jhpd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-8g82j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-8g82j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-8g82j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:38 +0000 UTC - event for affinity-nodeport-transition-mmxrg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:39 +0000 UTC - event for execpod-affinityc66mq: {default-scheduler } Scheduled: Successfully assigned services-2558/execpod-affinityc66mq to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:40 +0000 UTC - event for execpod-affinityc66mq: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:40 +0000 UTC - event for execpod-affinityc66mq: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost-container&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:20:40 +0000 UTC - event for execpod-affinityc66mq: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost-container&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:22:52 +0000 UTC - event for affinity-nodeport-transition: {endpoint-controller } FailedToUpdateEndpoint: Failed to update endpoint services-2558/affinity-nodeport-transition: Operation cannot be fulfilled on endpoints &#34;affinity-nodeport-transition&#34;: the object has been modified; please apply your changes to the latest version and try again&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:22:52 +0000 UTC - event for affinity-nodeport-transition-2jhpd: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:22:52 +0000 UTC - event for affinity-nodeport-transition-8g82j: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport-transition&#xA;May 18 15:23:00.845: INFO: At 2021-05-18 15:22:52 +0000 UTC - event for affinity-nodeport-transition-mmxrg: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Killing: Stopping container affinity-nodeport-transition&#xA;May 18 15:23:00.852: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 15:23:00.852: INFO: &#xA;May 18 15:23:00.861: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:23:00.868: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 14608 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:20:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:23:00.868: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:23:00.878: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:23:00.922: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:23:00.922: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:23:00.930: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 14609 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:20:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:18:00 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:23:00.931: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:23:00.941: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:23:00.977: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:23:00.978: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:00.986: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 14631 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:20:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:20:31 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:20:31 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:20:31 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:20:31 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:23:00.987: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:00.994: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:23:01.023: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:23:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;services-2558&#34; for this suite.&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with non-vsan datastore is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Scheduler should continue assigning pods to nodes across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Liveness liveness pods should be automatically restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement [Feature:vsphere] should create and delete pod with the same volume source attach/detach to different worker nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the token secret when the secret expired" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]" classname="Kubernetes e2e suite" time="61.233862639"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]" classname="Kubernetes e2e suite" time="7.510383045"></testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Pod Container Status should never report success for a pending container" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default command (docker entrypoint) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.602776356"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid diskStripes value is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]" classname="Kubernetes e2e suite" time="3.834222854"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsNonRoot should run with an image specified user ID" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should allow ingress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] gpu Upgrade [Feature:GPUUpgrade] master upgrade should NOT disrupt gpu pod [Feature:GPUMasterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for pod-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] A node shouldn&#39;t be able to create another node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl copy should copy a file from a running Pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] should only target nodes with endpoints" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation fails if only datastore is specified in the storage class (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services [Slow] should be able to create and tear down a standard-tier load balancer [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]" classname="Kubernetes e2e suite" time="6.153245521"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned Snapshot (retain policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.SupplementalGroups [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement [Feature:vsphere] test back to back pod creation and deletion with different volume sources on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]" classname="Kubernetes e2e suite" time="1.225713193"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should update nodePort: http [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp runtime/default [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Certificates API [Privileged:ClusterAdmin] should support building a client with a CSR" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks succeed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: sctp [LinuxOnly][Feature:SCTPConnectivity][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpExists)." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="52.940842129"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.576180582"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]" classname="Kubernetes e2e suite" time="29.424719093"></testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should ensure an IP overlapping both IPBlock.CIDR and IPBlock.Except is allowed [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic Snapshot (delete policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned Snapshot (delete policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] [Feature:GPUDevicePlugin] run Nvidia GPU Device Plugin tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]" classname="Kubernetes e2e suite" time="0.480187423"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]" classname="Kubernetes e2e suite" time="7.739223175"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates pod overhead is considered along with resource limits of pods that are allowed to run verify pod overhead is accounted for" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should support multiple inline ephemeral volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed and one node is broken [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="26.909467928"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce except clause while egress access to server in CIDR block [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]" classname="Kubernetes e2e suite" time="5.324018947"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should support multiple inline ephemeral volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.586184308"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on the allowed zones, datastore and storage policy specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Reboot [Disruptive] [Feature:Reboot] each node by ordering unclean reboot and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]" classname="Kubernetes e2e suite" time="1.180921463"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Kibana Logging Instances Is Alive [Feature:Elasticsearch] should check that the Kibana logging instance is alive" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Node Unregister [Feature:vsphere] [Slow] [Disruptive] node unregister" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]" classname="Kubernetes e2e suite" time="28.668815009"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 1 containers and 2 PDs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on localhost that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should remove clusters as expected" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should support https-only annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]" classname="Kubernetes e2e suite" time="140.00093232"></testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]" classname="Kubernetes e2e suite" time="109.977041544"></testcase>
      <testcase name="[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]" classname="Kubernetes e2e suite" time="2.65579066"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for pod-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] KubeProxy should set TCP CLOSE_WAIT timeout [Privileged]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up when expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volumes ConfigMap should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Reboot [Disruptive] [Feature:Reboot] each node by ordering clean reboot and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthenticator] The kubelet can delegate ServiceAccount tokens to the API server" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere [Feature:vsphere] should test that deleting the Namespace of a PVC and Pod causes the successful detach of vsphere volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should fail on mounting block device &#39;ablkdev&#39; when HostPathType is HostPathDirectory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl apply should apply a new configuration to an existing RC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]" classname="Kubernetes e2e suite" time="16.743915892"></testcase>
      <testcase name="[sig-storage] Flexvolumes should be mountable when attachable [Feature:Flexvolumes]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] NodeLease when the NodeLease feature is enabled should have OwnerReferences set" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should cap back-off at MaxContainerBackOff [Slow][NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for endpoint-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Upgrade kube-proxy from static pods to a DaemonSet should maintain a functioning cluster [Feature:KubeProxyDaemonSetUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks [Serial] attach on previously attached volumes should work" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIServiceAccountToken [Feature:CSIServiceAccountToken] token should be plumbed down when csiServiceAccountTokenEnabled=true" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.712370449"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to cadvisor port 4194 using proxy subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl taint [Serial] should update the taint on a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning on Datastore [Feature:vsphere] verify dynamically provisioned pv using storageclass fails on an invalid datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Hybrid cluster network for all supported CNIs should have stable networking for Linux and Windows pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.525583478"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down when rescheduling a pod is required and pdb allows for it[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a container&#39;s args [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.579782032"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes spread across nodes when pod has anti-affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: CSI Ephemeral-volume (default fs)] ephemeral should create read-only inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]" classname="Kubernetes e2e suite" time="0.583377294"></testcase>
      <testcase name="[sig-windows] [Feature:Windows] SecurityContext should be able create pods and run containers with a given username" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 [Slow] Nginx should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify static provisioning on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]" classname="Kubernetes e2e suite" time="8.086155742"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]" classname="Kubernetes e2e suite" time="2.608390307"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthenticator] The kubelet&#39;s main port 10250 should reject requests with no credentials" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] HA-master [Feature:HAMaster] survive addition/removal replicas different zones [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]" classname="Kubernetes e2e suite" time="1.960655032"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.602188235"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Conntrack should be able to preserve UDP traffic when server pod cycles for a NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should update/patch PodDisruptionBudget status" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy to allow traffic only from a different namespace, based on NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Block Device [Slow] Should fail on mounting block device &#39;ablkdev&#39; when HostPathType is HostPathFile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should not allow access by TCP when a policy specifies only SCTP [Feature:NetworkPolicy] [Feature:SCTP]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.814867778"></testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl create quota should reject quota with invalid scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes on one node when pod management is parallel and pod has affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted flexvolume volume expand [Slow] [Feature:ExpandInUsePersistentVolumes] should be resizable when mounted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should fail on mounting file &#39;afile&#39; when HostPathType is HostPathBlockDev" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="6.649719563"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Downgrade kube-proxy from a DaemonSet to static pods should maintain a functioning cluster [Feature:KubeProxyDaemonSetDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with best effort scope using scope-selectors." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] HA-master [Feature:HAMaster] survive addition/removal replicas multizone workers [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Attach Verify [Feature:vsphere][Serial][Disruptive] verify volume remains attached after master kubelet restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Size [Feature:vsphere] verify dynamically provisioned pv has size rounded up correctly" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should be able to mount file &#39;afile&#39; successfully when HostPathType is HostPathUnset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]" classname="Kubernetes e2e suite" time="131.965383756">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:34:15.617: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc004212380&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.5.57:80 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.5.57:80 over TCP protocol&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:1701</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:32:04.002: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename services&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745&#xA;[It] should be able to change the type from ExternalName to NodePort [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating a service externalname-service with the type=ExternalName in namespace services-7433&#xA;STEP: changing the ExternalName service to type=NodePort&#xA;STEP: creating replication controller externalname-service in namespace services-7433&#xA;May 18 15:32:07.711: INFO: Creating new exec pod&#xA;May 18 15:32:10.769: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80&#39;&#xA;May 18 15:32:10.993: INFO: stderr: &#34;+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 15:32:10.993: INFO: stdout: &#34;&#34;&#xA;May 18 15:32:10.994: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:13.195: INFO: rc: 1&#xA;May 18 15:32:13.195: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:14.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:16.431: INFO: rc: 1&#xA;May 18 15:32:16.431: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:17.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:19.402: INFO: rc: 1&#xA;May 18 15:32:19.402: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:20.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:22.412: INFO: rc: 1&#xA;May 18 15:32:22.412: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:23.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:25.401: INFO: rc: 1&#xA;May 18 15:32:25.401: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:26.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:28.456: INFO: rc: 1&#xA;May 18 15:32:28.456: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:29.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:31.417: INFO: rc: 1&#xA;May 18 15:32:31.417: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:32.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:34.400: INFO: rc: 1&#xA;May 18 15:32:34.400: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:35.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:37.436: INFO: rc: 1&#xA;May 18 15:32:37.436: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:38.197: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:40.412: INFO: rc: 1&#xA;May 18 15:32:40.412: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:41.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:43.420: INFO: rc: 1&#xA;May 18 15:32:43.420: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:44.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:46.417: INFO: rc: 1&#xA;May 18 15:32:46.417: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:47.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:49.407: INFO: rc: 1&#xA;May 18 15:32:49.407: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:50.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:52.411: INFO: rc: 1&#xA;May 18 15:32:52.411: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:53.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:55.395: INFO: rc: 1&#xA;May 18 15:32:55.395: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:56.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:32:58.487: INFO: rc: 1&#xA;May 18 15:32:58.487: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:32:59.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:01.411: INFO: rc: 1&#xA;May 18 15:33:01.411: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:02.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:04.399: INFO: rc: 1&#xA;May 18 15:33:04.399: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:05.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:07.424: INFO: rc: 1&#xA;May 18 15:33:07.424: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:08.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:10.504: INFO: rc: 1&#xA;May 18 15:33:10.504: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:11.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:13.444: INFO: rc: 1&#xA;May 18 15:33:13.444: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:14.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:16.392: INFO: rc: 1&#xA;May 18 15:33:16.392: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:17.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:19.414: INFO: rc: 1&#xA;May 18 15:33:19.414: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:20.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:22.423: INFO: rc: 1&#xA;May 18 15:33:22.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:23.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:25.405: INFO: rc: 1&#xA;May 18 15:33:25.405: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:26.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:28.411: INFO: rc: 1&#xA;May 18 15:33:28.411: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:29.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:31.420: INFO: rc: 1&#xA;May 18 15:33:31.420: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:32.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:34.410: INFO: rc: 1&#xA;May 18 15:33:34.410: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:35.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:37.443: INFO: rc: 1&#xA;May 18 15:33:37.443: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:38.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:40.393: INFO: rc: 1&#xA;May 18 15:33:40.393: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:41.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:43.405: INFO: rc: 1&#xA;May 18 15:33:43.405: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:44.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:46.407: INFO: rc: 1&#xA;May 18 15:33:46.408: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:47.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:49.423: INFO: rc: 1&#xA;May 18 15:33:49.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:50.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:52.429: INFO: rc: 1&#xA;May 18 15:33:52.429: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:53.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:55.410: INFO: rc: 1&#xA;May 18 15:33:55.410: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:56.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:33:58.410: INFO: rc: 1&#xA;May 18 15:33:58.410: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:33:59.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:34:01.424: INFO: rc: 1&#xA;May 18 15:34:01.424: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:34:02.196: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:34:04.405: INFO: rc: 1&#xA;May 18 15:34:04.405: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:34:05.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:34:07.421: INFO: rc: 1&#xA;May 18 15:34:07.421: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:34:08.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:34:10.477: INFO: rc: 1&#xA;May 18 15:34:10.477: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:34:11.195: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:34:13.403: INFO: rc: 1&#xA;May 18 15:34:13.403: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:34:13.403: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80&#39;&#xA;May 18 15:34:15.616: INFO: rc: 1&#xA;May 18 15:34:15.616: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-7433 exec execpod6hsn5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.5.57 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.5.57 80&#xA;nc: connect to 10.108.5.57 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:34:15.617: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc004212380&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.5.57:80 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.5.57:80 over TCP protocol&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.glob..func24.15()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:1701 +0x358&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;May 18 15:34:15.618: INFO: Cleaning up the ExternalName to NodePort test service&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;services-7433&#34;.&#xA;STEP: Found 14 events.&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:04 +0000 UTC - event for externalname-service: {replication-controller } SuccessfulCreate: Created pod: externalname-service-8vljk&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:04 +0000 UTC - event for externalname-service: {replication-controller } SuccessfulCreate: Created pod: externalname-service-t698s&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:04 +0000 UTC - event for externalname-service-8vljk: {default-scheduler } Scheduled: Successfully assigned services-7433/externalname-service-8vljk to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:04 +0000 UTC - event for externalname-service-t698s: {default-scheduler } Scheduled: Successfully assigned services-7433/externalname-service-t698s to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:05 +0000 UTC - event for externalname-service-8vljk: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:05 +0000 UTC - event for externalname-service-8vljk: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container externalname-service&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:05 +0000 UTC - event for externalname-service-8vljk: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container externalname-service&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:05 +0000 UTC - event for externalname-service-t698s: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container externalname-service&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:05 +0000 UTC - event for externalname-service-t698s: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:05 +0000 UTC - event for externalname-service-t698s: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container externalname-service&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:07 +0000 UTC - event for execpod6hsn5: {default-scheduler } Scheduled: Successfully assigned services-7433/execpod6hsn5 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:08 +0000 UTC - event for execpod6hsn5: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:08 +0000 UTC - event for execpod6hsn5: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost-container&#xA;May 18 15:34:15.704: INFO: At 2021-05-18 15:32:08 +0000 UTC - event for execpod6hsn5: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost-container&#xA;May 18 15:34:15.716: INFO: POD                         NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 15:34:15.717: INFO: externalname-service-8vljk  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:04 +0000 UTC  }]&#xA;May 18 15:34:15.717: INFO: externalname-service-t698s  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:04 +0000 UTC  }]&#xA;May 18 15:34:15.717: INFO: execpod6hsn5                gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:32:07 +0000 UTC  }]&#xA;May 18 15:34:15.717: INFO: &#xA;May 18 15:34:15.749: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:34:15.762: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 16084 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:30:10 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:30:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:30:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:30:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:30:34 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:34:15.762: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:34:15.771: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:34:15.801: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:34:15.802: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:34:15.810: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 16604 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:30:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:34:15.810: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:34:15.821: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:34:15.885: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:34:15.885: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:34:15.892: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 16605 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:30:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:33:04 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:34:15.893: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:34:15.901: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:34:15.949: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:34:15.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;services-7433&#34; for this suite.&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere [Feature:vsphere] should test that deleting the PV before the pod does not cause pod deletion to fail on vsphere volume detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]" classname="Kubernetes e2e suite" time="9.86245287"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="5.116773222"></testcase>
      <testcase name="[k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.594812099"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.602353968"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="5.570169402"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should prevent NodePort collisions" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on localhost that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] should work for type=LoadBalancer" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should support two pods which share the same volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes spread across nodes when pod management is parallel and pod has anti-affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should create a PodDisruptionBudget" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should work with Ingress,Egress specified together [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]" classname="Kubernetes e2e suite" time="19.295517062"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.559840324"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="25.013299652"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy [Feature:PodSecurityPolicy] should enforce the restricted policy.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver contain ephemeral=true when using inline volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for pod-Service(hostNetwork): udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid hostFailuresToTolerate value is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.672473589"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should create read/write inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="244.606809869"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes on one node when pod has affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl get componentstatuses should get componentstatuses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="20.990066288"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere [Feature:vsphere] should test that a file written to the vsphere volume mount before kubelet restart can be read after restart [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should support exec through an HTTP proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver Change stubDomain should be able to change stubDomain configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for API chunking should support continue listing from the last key if the original version has been compacted away, though the list is inconsistent [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with incompatible storagePolicy and zone combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="22.766849301"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Etcd failure [Disruptive] should recover from network partition with master" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should failover to a different zone when all nodes in one zone become unreachable [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]" classname="Kubernetes e2e suite" time="63.430125166"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for LoadBalancer service with ESIPP off [Slow] [DisabledForLargeClusters] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] [sig-auth] ServiceAccount admission controller migration [Feature:BoundServiceAccountTokenVolume] master upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]" classname="Kubernetes e2e suite" time="1.339829269"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 0 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NodeProblemDetector [DisabledForLargeClusters] should run without error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on a VSAN capability, datastore and compatible zone specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on localhost should support forwarding over websockets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support volume SELinux relabeling [Flaky] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should be able to mount socket &#39;asocket&#39; successfully when HostPathType is HostPathUnset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [Feature:SCTPConnectivity][LinuxOnly][Disruptive] NetworkPolicy between server and client using SCTP should support a &#39;default-deny&#39; policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.579512725"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: too few pods, absolute =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.562006417"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should support two pods which share the same volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.594268594"></testcase>
      <testcase name="[sig-storage] Volume Placement [Feature:vsphere] should create and delete pod with multiple volumes from same datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=nil" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap Should fail non-optional pod creation due to configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on the allowed zones specified in storage class when the datastore under the zone is present in another datacenter" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints to NEG" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with backend HTTPS" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should fail on mounting socket &#39;asocket&#39; when HostPathType is HostPathFile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.60276"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer =&gt; should not allow an eviction [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should support port-forward" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for client IP based session affinity: http [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for LoadBalancer service with ESIPP on [Slow] [DisabledForLargeClusters] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] GMSA Kubelet [Slow] kubelet GMSA support when creating a pod with correct GMSA credential specs passes the credential specs down to the Pod&#39;s containers" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify PVC creation with incompatible datastore and zone combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should test the lifecycle of an Endpoint [Conformance]" classname="Kubernetes e2e suite" time="1.443062352"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to up and down services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv3 should be mountable for NFSv3" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should support subPath [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]" classname="Kubernetes e2e suite" time="11.65387272"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale down underutilized nodes [Feature:ClusterAutoscalerScalability4]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]" classname="Kubernetes e2e suite" time="23.605460632"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should create read/write inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.612395158"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="7.110689707"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a RuntimeClass with conflicting node selector" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Object from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] should work from pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.559800952"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Events should delete a collection of events [Conformance]" classname="Kubernetes e2e suite" time="0.581563171"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Events API should delete a collection of events [Conformance]" classname="Kubernetes e2e suite" time="0.5539351"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]" classname="Kubernetes e2e suite" time="131.863737251">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:44:36.453: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002e9b190&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.7.85:80 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.7.85:80 over TCP protocol&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:1662</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:42:24.897: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename services&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745&#xA;[It] should be able to change the type from ExternalName to ClusterIP [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: creating a service externalname-service with the type=ExternalName in namespace services-2508&#xA;STEP: changing the ExternalName service to type=ClusterIP&#xA;STEP: creating replication controller externalname-service in namespace services-2508&#xA;May 18 15:42:28.562: INFO: Creating new exec pod&#xA;May 18 15:42:31.610: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80&#39;&#xA;May 18 15:42:31.816: INFO: stderr: &#34;+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n&#34;&#xA;May 18 15:42:31.816: INFO: stdout: &#34;&#34;&#xA;May 18 15:42:31.816: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:34.029: INFO: rc: 1&#xA;May 18 15:42:34.029: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:35.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:37.241: INFO: rc: 1&#xA;May 18 15:42:37.241: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:38.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:40.245: INFO: rc: 1&#xA;May 18 15:42:40.245: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:41.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:43.270: INFO: rc: 1&#xA;May 18 15:42:43.270: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:44.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:46.237: INFO: rc: 1&#xA;May 18 15:42:46.237: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:47.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:49.261: INFO: rc: 1&#xA;May 18 15:42:49.261: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:50.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:52.246: INFO: rc: 1&#xA;May 18 15:42:52.246: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:53.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:55.306: INFO: rc: 1&#xA;May 18 15:42:55.306: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:56.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:42:58.248: INFO: rc: 1&#xA;May 18 15:42:58.248: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:42:59.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:01.244: INFO: rc: 1&#xA;May 18 15:43:01.244: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:02.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:04.224: INFO: rc: 1&#xA;May 18 15:43:04.224: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:05.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:07.257: INFO: rc: 1&#xA;May 18 15:43:07.257: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:08.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:10.247: INFO: rc: 1&#xA;May 18 15:43:10.247: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:11.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:13.241: INFO: rc: 1&#xA;May 18 15:43:13.241: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:14.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:16.304: INFO: rc: 1&#xA;May 18 15:43:16.304: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:17.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:19.254: INFO: rc: 1&#xA;May 18 15:43:19.254: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:20.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:22.232: INFO: rc: 1&#xA;May 18 15:43:22.232: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:23.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:25.251: INFO: rc: 1&#xA;May 18 15:43:25.251: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:26.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:28.429: INFO: rc: 1&#xA;May 18 15:43:28.429: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:29.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:31.277: INFO: rc: 1&#xA;May 18 15:43:31.277: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:32.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:34.247: INFO: rc: 1&#xA;May 18 15:43:34.247: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:35.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:37.265: INFO: rc: 1&#xA;May 18 15:43:37.265: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:38.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:40.262: INFO: rc: 1&#xA;May 18 15:43:40.262: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:41.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:43.265: INFO: rc: 1&#xA;May 18 15:43:43.265: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:44.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:46.240: INFO: rc: 1&#xA;May 18 15:43:46.240: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:47.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:49.245: INFO: rc: 1&#xA;May 18 15:43:49.245: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:50.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:52.231: INFO: rc: 1&#xA;May 18 15:43:52.232: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:53.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:55.253: INFO: rc: 1&#xA;May 18 15:43:55.253: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:56.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:43:58.237: INFO: rc: 1&#xA;May 18 15:43:58.237: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:43:59.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:01.279: INFO: rc: 1&#xA;May 18 15:44:01.279: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:02.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:04.238: INFO: rc: 1&#xA;May 18 15:44:04.238: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:05.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:07.258: INFO: rc: 1&#xA;May 18 15:44:07.258: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:08.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:10.252: INFO: rc: 1&#xA;May 18 15:44:10.252: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:11.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:13.252: INFO: rc: 1&#xA;May 18 15:44:13.252: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:14.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:16.257: INFO: rc: 1&#xA;May 18 15:44:16.257: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:17.030: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:19.319: INFO: rc: 1&#xA;May 18 15:44:19.319: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:20.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:22.234: INFO: rc: 1&#xA;May 18 15:44:22.234: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:23.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:25.543: INFO: rc: 1&#xA;May 18 15:44:25.544: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:26.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:28.296: INFO: rc: 1&#xA;May 18 15:44:28.296: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:29.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:31.250: INFO: rc: 1&#xA;May 18 15:44:31.250: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:32.029: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:34.248: INFO: rc: 1&#xA;May 18 15:44:34.248: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:34.248: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80&#39;&#xA;May 18 15:44:36.453: INFO: rc: 1&#xA;May 18 15:44:36.453: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=services-2508 exec execpod6n5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.7.85 80:&#xA;Command stdout:&#xA;&#xA;stderr:&#xA;+ nc -zv -t -w 2 10.108.7.85 80&#xA;nc: connect to 10.108.7.85 port 80 (tcp) timed out: Operation in progress&#xA;command terminated with exit code 1&#xA;&#xA;error:&#xA;exit status 1&#xA;Retrying...&#xA;May 18 15:44:36.453: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc002e9b190&gt;: {&#xA;        s: &#34;service is not reachable within 2m0s timeout on endpoint 10.108.7.85:80 over TCP protocol&#34;,&#xA;    }&#xA;    service is not reachable within 2m0s timeout on endpoint 10.108.7.85:80 over TCP protocol&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/network.glob..func24.14()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:1662 +0x358&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;May 18 15:44:36.454: INFO: Cleaning up the ExternalName to ClusterIP test service&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;services-2508&#34;.&#xA;STEP: Found 14 events.&#xA;May 18 15:44:36.533: INFO: At 2021-05-18 15:42:25 +0000 UTC - event for externalname-service: {replication-controller } SuccessfulCreate: Created pod: externalname-service-47wwr&#xA;May 18 15:44:36.534: INFO: At 2021-05-18 15:42:25 +0000 UTC - event for externalname-service: {replication-controller } SuccessfulCreate: Created pod: externalname-service-9kc8d&#xA;May 18 15:44:36.534: INFO: At 2021-05-18 15:42:25 +0000 UTC - event for externalname-service-47wwr: {default-scheduler } Scheduled: Successfully assigned services-2508/externalname-service-47wwr to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:25 +0000 UTC - event for externalname-service-9kc8d: {default-scheduler } Scheduled: Successfully assigned services-2508/externalname-service-9kc8d to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:26 +0000 UTC - event for externalname-service-47wwr: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:26 +0000 UTC - event for externalname-service-47wwr: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container externalname-service&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:26 +0000 UTC - event for externalname-service-47wwr: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container externalname-service&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:26 +0000 UTC - event for externalname-service-9kc8d: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:26 +0000 UTC - event for externalname-service-9kc8d: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container externalname-service&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:26 +0000 UTC - event for externalname-service-9kc8d: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container externalname-service&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:28 +0000 UTC - event for execpod6n5w7: {default-scheduler } Scheduled: Successfully assigned services-2508/execpod6n5w7 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:29 +0000 UTC - event for execpod6n5w7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:29 +0000 UTC - event for execpod6n5w7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost-container&#xA;May 18 15:44:36.538: INFO: At 2021-05-18 15:42:29 +0000 UTC - event for execpod6n5w7: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost-container&#xA;May 18 15:44:36.547: INFO: POD                         NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 15:44:36.547: INFO: externalname-service-9kc8d  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:25 +0000 UTC  }]&#xA;May 18 15:44:36.547: INFO: externalname-service-47wwr  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:25 +0000 UTC  }]&#xA;May 18 15:44:36.547: INFO: execpod6n5w7                gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:42:28 +0000 UTC  }]&#xA;May 18 15:44:36.547: INFO: &#xA;May 18 15:44:36.558: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:44:36.568: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 17318 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:40:12 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:40:37 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:40:37 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:40:37 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:40:37 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:44:36.569: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:44:36.582: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:44:36.616: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:44:36.616: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:44:36.639: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 17922 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:40:16 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:44:36.640: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:44:36.649: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:44:36.677: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:44:36.677: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:44:36.685: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 17923 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:40:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:43:07 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:44:36.685: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:44:36.696: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:44:36.742: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:44:36.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;services-2508&#34; for this suite.&#xA;[AfterEach] [sig-network] Services&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]" classname="Kubernetes e2e suite" time="14.46474469"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.567296205"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] AppArmor load AppArmor profiles should enforce an AppArmor profile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale up when non expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should create read-only inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted startup probe fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume that cannot be mounted [Slow] should fail due to wrong node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Flexvolumes should be mountable when non-attachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Addon update should propagate add-on file changes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume storage capacity exhausted, late binding, no topology" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vcp-performance [Feature:vsphere] vcp performance tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale down GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should release no longer matching pods [Conformance]" classname="Kubernetes e2e suite" time="0.559820046"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale down when non expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="244.409039893"></testcase>
      <testcase name="[sig-apps] DisruptionController evictions: no PDB =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should eagerly create replacement pod during network partition when termination grace is non-zero" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]" classname="Kubernetes e2e suite" time="11.144870079"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.581278126"></testcase>
      <testcase name="[sig-windows] [Feature:Windows] Memory Limits [Serial] [Slow] attempt to deploy past allocatable memory limits should fail deployments of pods once there isn&#39;t enough memory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should patch a secret [Conformance]" classname="Kubernetes e2e suite" time="0.55379101"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (cpu, memory quota set) against a pod with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]" classname="Kubernetes e2e suite" time="7.841240502"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI FSGroupPolicy [LinuxOnly] should modify fsGroup if fsGroupPolicy=File" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for node-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: enough pods, absolute =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI NodeStage error cases [Slow] should call NodeUnstage after NodeStage success" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI online volume expansion should expand volume without restarting pod if attach=on, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIStorageCapacity [Feature:CSIStorageCapacity] CSIStorageCapacity used, insufficient capacity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for endpoint-Service: sctp [Feature:SCTPConnectivity][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] IngressClass [Feature:Ingress] should set default value on new IngressClass [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="4.678850203"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for node-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]" classname="Kubernetes e2e suite" time="14.525942438"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should be able to mount character device &#39;achardev&#39; successfully when HostPathType is HostPathUnset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere [Feature:vsphere] should test that a vsphere volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should fail on mounting socket &#39;asocket&#39; when HostPathType is HostPathDirectory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a non-local redirect http liveness probe" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl cluster-info dump should check if cluster-info dump succeeds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] should work for type=NodePort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI NodeStage error cases [Slow] should call NodeUnstage after NodeStage ephemeral error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and datastore specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] rolling update backend pods should not cause service disruption" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s cpu request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.584673361"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] host cleanup with volume mounts [sig-storage][HostCleanup][Flaky] Host cleanup after disrupting NFS volume [NFS] after stopping the nfs-server and deleting the (active) client pod, the NFS mount and the pod&#39;s UID directory should be removed." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should support inline execution and attach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]" classname="Kubernetes e2e suite" time="63.661304172"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]" classname="Kubernetes e2e suite" time="0.908471258"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.740832111"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]" classname="Kubernetes e2e suite" time="62.468793147"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Simple pod should contain last line of the log" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should not scale GPU pool up if pod does not require GPUs [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]" classname="Kubernetes e2e suite" time="2.978824472"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]" classname="Kubernetes e2e suite" time="11.562696938">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:52:06.805: Failed to connect to exposed host ports&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/internal/leafnodes/runner.go:113</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:51:55.463: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename sched-pred&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92&#xA;May 18 15:51:55.962: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready&#xA;May 18 15:51:55.980: INFO: Waiting for terminating namespaces to be deleted...&#xA;May 18 15:51:55.988: INFO: &#xA;Logging pods the apiserver thinks is on node gke-cluster-2-default-pool-426afc83-4s72 before test&#xA;May 18 15:51:55.997: INFO: sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-46lgd from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 15:51:55.997: INFO: &#x9;Container sonobuoy-worker ready: false, restart count 18&#xA;May 18 15:51:55.997: INFO: &#x9;Container systemd-logs ready: true, restart count 0&#xA;May 18 15:51:55.997: INFO: sonobuoy from sonobuoy started at 2021-05-18 13:41:17 +0000 UTC (1 container statuses recorded)&#xA;May 18 15:51:55.997: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;May 18 15:51:55.997: INFO: &#xA;Logging pods the apiserver thinks is on node gke-cluster-2-default-pool-426afc83-0shb before test&#xA;May 18 15:51:56.006: INFO: coredns-854c77959c-62dp5 from kube-system started at 2021-05-18 13:39:44 +0000 UTC (1 container statuses recorded)&#xA;May 18 15:51:56.006: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 18 15:51:56.006: INFO: sonobuoy-e2e-job-495605ea5c7c42c0 from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 15:51:56.006: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;May 18 15:51:56.006: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;May 18 15:51:56.006: INFO: sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-rxm4c from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 15:51:56.006: INFO: &#x9;Container sonobuoy-worker ready: false, restart count 18&#xA;May 18 15:51:56.006: INFO: &#x9;Container systemd-logs ready: true, restart count 0&#xA;May 18 15:51:56.006: INFO: &#xA;Logging pods the apiserver thinks is on node gke-cluster-2-default-pool-426afc83-rll8 before test&#xA;May 18 15:51:56.014: INFO: sonobuoy-systemd-logs-daemon-set-b3ff707fd0a74adb-gcqgt from sonobuoy started at 2021-05-18 13:41:25 +0000 UTC (2 container statuses recorded)&#xA;May 18 15:51:56.014: INFO: &#x9;Container sonobuoy-worker ready: false, restart count 18&#xA;May 18 15:51:56.014: INFO: &#x9;Container systemd-logs ready: true, restart count 0&#xA;May 18 15:51:56.014: INFO: agnhost-primary-krn7v from kubectl-4144 started at 2021-05-18 15:51:53 +0000 UTC (1 container statuses recorded)&#xA;May 18 15:51:56.014: INFO: &#x9;Container agnhost-primary ready: true, restart count 0&#xA;[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: Trying to launch a pod without a label to get a node which can launch it.&#xA;STEP: Explicitly delete pod here to free the resource it takes.&#xA;STEP: Trying to apply a random label on the found node.&#xA;STEP: verifying the node has the label kubernetes.io/e2e-c3d7902a-6fb8-4f95-bead-23ef2c189074 90&#xA;STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled&#xA;STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.108.1.212 on the node which pod1 resides and expect scheduled&#xA;STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.108.1.212 but use UDP protocol on the node which pod2 resides&#xA;STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321&#xA;May 18 15:52:06.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.108.1.212 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4978 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}&#xA;May 18 15:52:06.317: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;May 18 15:52:06.432: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54321&#xA;STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321&#xA;May 18 15:52:06.432: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.108.1.212 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4978 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}&#xA;May 18 15:52:06.432: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;May 18 15:52:06.523: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54321&#xA;STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321&#xA;May 18 15:52:06.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.108.1.212 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4978 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}&#xA;May 18 15:52:06.523: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;May 18 15:52:06.616: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54321&#xA;STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321&#xA;May 18 15:52:06.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.108.1.212 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4978 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}&#xA;May 18 15:52:06.617: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;May 18 15:52:06.711: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54321&#xA;STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321&#xA;May 18 15:52:06.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.108.1.212 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4978 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}&#xA;May 18 15:52:06.711: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;May 18 15:52:06.804: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54321&#xA;May 18 15:52:06.805: FAIL: Failed to connect to exposed host ports&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;STEP: removing the label kubernetes.io/e2e-c3d7902a-6fb8-4f95-bead-23ef2c189074 off the node gke-cluster-2-default-pool-426afc83-rll8&#xA;STEP: verifying the node doesn&#39;t have the label kubernetes.io/e2e-c3d7902a-6fb8-4f95-bead-23ef2c189074&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;sched-pred-4978&#34;.&#xA;STEP: Found 20 events.&#xA;May 18 15:52:06.846: INFO: At 2021-05-18 15:51:56 +0000 UTC - event for without-label: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/pause:3.2&#34; already present on machine&#xA;May 18 15:52:06.846: INFO: At 2021-05-18 15:51:56 +0000 UTC - event for without-label: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container without-label&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:51:56 +0000 UTC - event for without-label: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container without-label&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:51:56 +0000 UTC - event for without-label: {default-scheduler } Scheduled: Successfully assigned sched-pred-4978/without-label to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:51:58 +0000 UTC - event for pod1: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:51:58 +0000 UTC - event for pod1: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:51:58 +0000 UTC - event for pod1: {default-scheduler } Scheduled: Successfully assigned sched-pred-4978/pod1 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:51:59 +0000 UTC - event for pod1: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost&#xA;May 18 15:52:06.847: INFO: At 2021-05-18 15:52:00 +0000 UTC - event for pod2: {default-scheduler } Scheduled: Successfully assigned sched-pred-4978/pod2 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:01 +0000 UTC - event for pod2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:01 +0000 UTC - event for pod2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:01 +0000 UTC - event for pod2: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:02 +0000 UTC - event for pod3: {default-scheduler } Scheduled: Successfully assigned sched-pred-4978/pod3 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:03 +0000 UTC - event for pod3: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:03 +0000 UTC - event for pod3: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container agnhost&#xA;May 18 15:52:06.848: INFO: At 2021-05-18 15:52:03 +0000 UTC - event for pod3: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container agnhost&#xA;May 18 15:52:06.849: INFO: At 2021-05-18 15:52:04 +0000 UTC - event for e2e-host-exec: {default-scheduler } Scheduled: Successfully assigned sched-pred-4978/e2e-host-exec to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.849: INFO: At 2021-05-18 15:52:05 +0000 UTC - event for e2e-host-exec: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 15:52:06.849: INFO: At 2021-05-18 15:52:05 +0000 UTC - event for e2e-host-exec: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container e2e-host-exec&#xA;May 18 15:52:06.849: INFO: At 2021-05-18 15:52:05 +0000 UTC - event for e2e-host-exec: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container e2e-host-exec&#xA;May 18 15:52:06.857: INFO: POD            NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 15:52:06.857: INFO: pod2           gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:00 +0000 UTC ContainersNotReady containers with unready status: [agnhost]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:00 +0000 UTC ContainersNotReady containers with unready status: [agnhost]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:00 +0000 UTC  }]&#xA;May 18 15:52:06.857: INFO: pod1           gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:51:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:51:58 +0000 UTC  }]&#xA;May 18 15:52:06.857: INFO: pod3           gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:02 +0000 UTC ContainersNotReady containers with unready status: [agnhost]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:02 +0000 UTC ContainersNotReady containers with unready status: [agnhost]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:02 +0000 UTC  }]&#xA;May 18 15:52:06.857: INFO: e2e-host-exec  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 15:52:04 +0000 UTC  }]&#xA;May 18 15:52:06.857: INFO: &#xA;May 18 15:52:06.865: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:52:06.872: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 18478 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:52:06.873: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:52:06.882: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:52:06.923: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:52:06.923: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:52:06.930: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 18479 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:17 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:48:09 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:52:06.931: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:52:06.938: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:52:06.965: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:52:06.966: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.973: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 18661 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:50:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:50:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:50:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:50:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:50:40 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:52:06.973: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:06.983: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:52:07.007: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:52:07.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;sched-pred-4978&#34; for this suite.&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Node Poweroff [Feature:vsphere] [Slow] [Disruptive] verify volume status after node power off" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.644560256"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes GlusterFS should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] volumeLimits should support volume limits [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing configmap should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid objectSpaceReservation and iopsLimit values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should support a &#39;default-deny-all&#39; policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.649498655"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Nodes [Disruptive] Resize [Slow] should be able to add nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support [Feature:vsphere] Verify a pod is created and attached to a dynamically created PV with storage policy specified in storage class in waitForFirstConsumer binding mode with multiple allowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.583088091"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should have cluster metrics [Feature:StackdriverMonitoring]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should update endpoints: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]" classname="Kubernetes e2e suite" time="10.589248961"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType File [Slow] Should be able to mount file &#39;afile&#39; successfully when HostPathType is HostPathFile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]" classname="Kubernetes e2e suite" time="4.599875769"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="22.759559409"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]" classname="Kubernetes e2e suite" time="0.591192053"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should support allow-all policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]" classname="Kubernetes e2e suite" time="40.7813164"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="5.11066761"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPathType Socket [Slow] Should fail on mounting non-existent socket &#39;does-not-exist-socket&#39; when HostPathType is HostPathSocket" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]" classname="Kubernetes e2e suite" time="361.046639853">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 15:59:40.072: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:173</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 15:53:39.366: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename sched-preemption&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90&#xA;May 18 15:53:39.874: INFO: Waiting up to 1m0s for all nodes to be ready&#xA;May 18 15:54:39.929: INFO: Waiting for terminating namespaces to be deleted...&#xA;[It] validates basic preemption works [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: Create pods that use 2/3 of node resources.&#xA;May 18 15:54:39.974: INFO: Created pod: pod0-sched-preemption-low-priority&#xA;May 18 15:54:39.997: INFO: Created pod: pod1-sched-preemption-medium-priority&#xA;May 18 15:54:40.032: INFO: Created pod: pod2-sched-preemption-medium-priority&#xA;STEP: Wait for pods to be scheduled.&#xA;May 18 15:59:40.072: FAIL: Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0001f41f0&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/scheduling.glob..func5.3()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:173 +0x951&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;sched-preemption-7034&#34;.&#xA;STEP: Found 3 events.&#xA;May 18 15:59:40.082: INFO: At 2021-05-18 15:54:40 +0000 UTC - event for pod0-sched-preemption-low-priority: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;pod0-sched-preemption-low-priority-x-sched-preemptio-275c95f950&#34; is forbidden: no PriorityClass with name sched-preemption-low-priority was found&#xA;May 18 15:59:40.082: INFO: At 2021-05-18 15:54:40 +0000 UTC - event for pod1-sched-preemption-medium-priority: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;pod1-sched-preemption-medium-priority-x-sched-preemp-af3b303ae9&#34; is forbidden: no PriorityClass with name sched-preemption-medium-priority was found&#xA;May 18 15:59:40.082: INFO: At 2021-05-18 15:54:40 +0000 UTC - event for pod2-sched-preemption-medium-priority: {pod-syncer } SyncError: Error syncing to physical cluster: pods &#34;pod2-sched-preemption-medium-priority-x-sched-preemp-8facaa53bc&#34; is forbidden: no PriorityClass with name sched-preemption-medium-priority was found&#xA;May 18 15:59:40.090: INFO: POD                                    NODE  PHASE    GRACE  CONDITIONS&#xA;May 18 15:59:40.090: INFO: pod0-sched-preemption-low-priority           Pending         []&#xA;May 18 15:59:40.090: INFO: pod1-sched-preemption-medium-priority        Pending         []&#xA;May 18 15:59:40.090: INFO: pod2-sched-preemption-medium-priority        Pending         []&#xA;May 18 15:59:40.090: INFO: &#xA;May 18 15:59:40.098: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:59:40.107: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 19320 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:55:14 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:55:42 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:55:42 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:55:42 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:55:42 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:59:40.107: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:59:40.117: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 15:59:40.185: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:59:40.185: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:59:40.193: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 19351 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 15:59:40.194: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:59:40.203: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 15:59:40.243: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:59:40.243: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:59:40.251: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 19352 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 15:55:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 15:59:40.251: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:59:40.259: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 15:59:40.323: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 15:59:40.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;sched-preemption-7034&#34; for this suite.&#xA;[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="10.758349698"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]" classname="Kubernetes e2e suite" time="0.913426462"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be rejected when no endpoints exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] CA ignores unschedulable pods while scheduling schedulable pods [Feature:ClusterAutoscalerScalability6]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should resolve DNS of partial qualified names for the cluster [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 1 pod to 2 pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [Job] should create new pods when node is partitioned" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="11.121356138"></testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - zeroedthick is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should ensure a single API token exists" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working mysql cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed when there is non autoscaled pool[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] SecurityContext should override SecurityContext username if set" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when pod is evicted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]" classname="Kubernetes e2e suite" time="7.251380985"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return pod details" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Firewall rule should have correct firewall rules for e2e cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI attach test using mock driver should preserve attachment policy when no CSIDriver present" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic Snapshot (retain policy)] snapshottable[Feature:VolumeSnapshotDataSource] volume snapshot controller  should check snapshot fields, check restore correctly works after modifying source data, check deletion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is force deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup skips ownership changes to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target average value from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy based on Ports [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]" classname="Kubernetes e2e suite" time="0.802068666"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider-gcp] Upgrade [Feature:Upgrade] master upgrade should maintain a functioning cluster [Feature:MasterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should be able to handle large requests: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - thin is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services should function for multiple endpoint-Services with same selector" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.561208952"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking IPerf [Experimental] [Slow] [Feature:Networking-Performance] should transfer ~ 1GB onto the service endpoint 1 servers (maximum of 1 clients)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:vsphere][Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere [Feature:vsphere] should delete persistent volume when reclaimPolicy set to delete and associated claim is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]" classname="Kubernetes e2e suite" time="5.481874371"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should create service with ipv4 cluster ip [Feature:IPv6DualStackAlphaFeature:Phase2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]" classname="Kubernetes e2e suite" time="15.990682171"></testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should have ipv4 and ipv6 internal node ip" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]" classname="Kubernetes e2e suite" time="19.840831843"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] should create pod, add ipv6 and ipv4 ip to pod ips" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with scheduling with taints [Serial] " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (immediate-binding)] ephemeral should create read/write inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Operations Storm [Feature:vsphere] should create pod with many volumes and verify no attach call fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSIStorageCapacity [Feature:CSIStorageCapacity] CSIStorageCapacity unused" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]" classname="Kubernetes e2e suite" time="8.113769927"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]" classname="Kubernetes e2e suite" time="11.557937859"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]" classname="Kubernetes e2e suite" time="48.639247182"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ClusterDns [Feature:Example] should create pod that uses dns" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv used in a pod that is deleted while the kubelet is down cleans up when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should support orphan deletion of custom resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should function for client IP based session affinity: http [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should be able to handle large requests: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should observe PodDisruptionBudget status updated" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]" classname="Kubernetes e2e suite" time="38.827095801">
          <failure type="Failure">/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;May 18 16:02:41.754: unexpected &#39;kubectl attach&#39; error message. expected to contain &#34;attaching to pod &#39;to-be-attached-pod&#39; is not allowed&#34;, got &#34;timed out waiting for command /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=webhook-3012 attach --namespace=webhook-3012 to-be-attached-pod -i -c=container1:\nCommand stdout:\n\nstderr:\nIf you don&#39;t see a command prompt, try pressing enter.\n&#34;&#xA;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:208</failure>
          <system-out>[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174&#xA;STEP: Creating a kubernetes client&#xA;May 18 16:02:03.467: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-449519394&#xA;STEP: Building a namespace api object, basename webhook&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86&#xA;STEP: Setting up server cert&#xA;STEP: Create role binding to let webhook read extension-apiserver-authentication&#xA;STEP: Deploying the webhook pod&#xA;STEP: Wait for the deployment to be ready&#xA;May 18 16:02:04.518: INFO: deployment &#34;sample-webhook-deployment&#34; doesn&#39;t have the required revision set&#xA;May 18 16:02:06.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756950524, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756950524, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756950524, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756950524, loc:(*time.Location)(0x797fe80)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-webhook-deployment-6bd9446d55\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;STEP: Deploying the webhook service&#xA;STEP: Verifying the service has paired with the endpoint&#xA;May 18 16:02:09.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1&#xA;[It] should be able to deny attaching pod [Conformance]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629&#xA;STEP: Registering the webhook via the AdmissionRegistration API&#xA;STEP: create a pod&#xA;STEP: &#39;kubectl attach&#39; the pod, should be denied by the webhook&#xA;May 18 16:02:11.754: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=webhook-3012 attach --namespace=webhook-3012 to-be-attached-pod -i -c=container1&#39;&#xA;May 18 16:02:41.754: FAIL: unexpected &#39;kubectl attach&#39; error message. expected to contain &#34;attaching to pod &#39;to-be-attached-pod&#39; is not allowed&#34;, got &#34;timed out waiting for command /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-449519394 --namespace=webhook-3012 attach --namespace=webhook-3012 to-be-attached-pod -i -c=container1:\nCommand stdout:\n\nstderr:\nIf you don&#39;t see a command prompt, try pressing enter.\n&#34;&#xA;&#xA;Full Stack Trace&#xA;k8s.io/kubernetes/test/e2e/apimachinery.glob..func23.5()&#xA;&#x9;/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:208 +0x86&#xA;k8s.io/kubernetes/test/e2e.RunE2ETests(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c&#xA;k8s.io/kubernetes/test/e2e.TestE2E(0xc000ee6900)&#xA;&#x9;_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b&#xA;testing.tRunner(0xc000ee6900, 0x4fc2a88)&#xA;&#x9;/usr/local/go/src/testing/testing.go:1123 +0xef&#xA;created by testing.(*T).Run&#xA;&#x9;/usr/local/go/src/testing/testing.go:1168 +0x2b3&#xA;[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175&#xA;STEP: Collecting events from namespace &#34;webhook-3012&#34;.&#xA;STEP: Found 10 events.&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:04 +0000 UTC - event for sample-webhook-deployment: {deployment-controller } ScalingReplicaSet: Scaled up replica set sample-webhook-deployment-6bd9446d55 to 1&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:04 +0000 UTC - event for sample-webhook-deployment-6bd9446d55: {replicaset-controller } SuccessfulCreate: Created pod: sample-webhook-deployment-6bd9446d55-bw6q4&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:04 +0000 UTC - event for sample-webhook-deployment-6bd9446d55-bw6q4: {default-scheduler } Scheduled: Successfully assigned webhook-3012/sample-webhook-deployment-6bd9446d55-bw6q4 to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:05 +0000 UTC - event for sample-webhook-deployment-6bd9446d55-bw6q4: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/e2e-test-images/agnhost:2.21&#34; already present on machine&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:05 +0000 UTC - event for sample-webhook-deployment-6bd9446d55-bw6q4: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container sample-webhook&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:05 +0000 UTC - event for sample-webhook-deployment-6bd9446d55-bw6q4: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container sample-webhook&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:09 +0000 UTC - event for to-be-attached-pod: {default-scheduler } Scheduled: Successfully assigned webhook-3012/to-be-attached-pod to gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:10 +0000 UTC - event for to-be-attached-pod: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Pulled: Container image &#34;k8s.gcr.io/pause:3.2&#34; already present on machine&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:10 +0000 UTC - event for to-be-attached-pod: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Created: Created container container1&#xA;May 18 16:02:41.800: INFO: At 2021-05-18 16:02:10 +0000 UTC - event for to-be-attached-pod: {kubelet gke-cluster-2-default-pool-426afc83-rll8} Started: Started container container1&#xA;May 18 16:02:41.807: INFO: POD                                         NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 18 16:02:41.807: INFO: sample-webhook-deployment-6bd9446d55-bw6q4  gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:04 +0000 UTC  }]&#xA;May 18 16:02:41.807: INFO: to-be-attached-pod                          gke-cluster-2-default-pool-426afc83-rll8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 16:02:09 +0000 UTC  }]&#xA;May 18 16:02:41.807: INFO: &#xA;May 18 16:02:41.814: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 16:02:41.821: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 19671 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 16:02:41.822: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 16:02:41.831: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 16:02:41.875: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 16:02:41.875: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 16:02:41.888: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 19694 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 16:02:41.888: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 16:02:41.897: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 16:02:41.951: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 16:02:41.951: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:41.960: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 19824 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 16:02:41.961: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:41.970: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:42.003: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;STEP: Collecting events from namespace &#34;webhook-3012-markers&#34;.&#xA;STEP: Found 0 events.&#xA;May 18 16:02:42.020: INFO: POD  NODE  PHASE  GRACE  CONDITIONS&#xA;May 18 16:02:42.020: INFO: &#xA;May 18 16:02:42.028: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 16:02:42.036: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-4s72    2003772b-7e35-4584-9419-98e9a6e4561e 19671 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-4s72 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:7198626613059026157 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-4s72&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.0.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.0.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-4s72,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 16:00:18 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.5.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:e31e412bb8907fbfc394756c1da4026b,SystemUUID:e31e412b-b890-7fbf-c394-756c1da4026b,BootID:a865e143-8392-4a45-b5d0-40d5310e4bad,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2 gke.gcr.io/prometheus-to-sd:v0.4.2],SizeBytes:9515926,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:9515805,},ContainerImage{Names:[gke.gcr.io/proxy-agent-amd64@sha256:5e03a718a3d8c8e772ad00d1be23f558e66be60c9a96281df880cbf132036427 gke.gcr.io/proxy-agent-amd64:v0.0.16-gke.0],SizeBytes:8583548,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 16:02:42.036: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 16:02:42.046: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-4s72&#xA;May 18 16:02:42.067: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-4s72: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 16:02:42.067: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 16:02:42.075: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-0shb    c093ecb8-e148-43a7-9c62-dff65afc9686 19694 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-0shb kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:5669010715379137773 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-0shb&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 13:39:44 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.2.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}},&#34;f:volumesAttached&#34;:{},&#34;f:volumesInUse&#34;:{}}}}]},Spec:NodeSpec{PodCIDR:10.104.2.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-0shb,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:19 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:59 +0000 UTC,LastTransitionTime:2021-05-18 13:34:59 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:34:45 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 15:58:13 +0000 UTC,LastTransitionTime:2021-05-18 13:35:17 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.0.52,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:8d757347c00a6f6282b53699b16e5581,SystemUUID:8d757347-c00a-6f62-82b5-3699b16e5581,BootID:be0bca57-e09f-46d5-ba36-68c6e93a97b5,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:eaadb6b00875477864b5da6e34bd9bf43e8437e0fecdd96e877538f95370be33 k8s.gcr.io/conformance:v1.20.5],SizeBytes:70244688,},ContainerImage{Names:[docker.io/rancher/k3s@sha256:01f2f181db87002f1665b2c218c7d0ed2aba1026246657c7dd6f7527702987c1 docker.io/rancher/k3s:v1.20.5-k3s1],SizeBytes:52394275,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[ghcr.io/loft-sh/loft-enterprise/dev-vcluster@sha256:f0566aaf4d71405233d4a965dd10b520f4d4dd560514ac54aa9192e6b1639bf7 ghcr.io/loft-sh/loft-enterprise/dev-vcluster:NoHaZRQ],SizeBytes:28570295,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[docker.io/rancher/coredns-coredns@sha256:8b675d12eb9faf3121475b12db478ac2cf5129046d92137aa9067dd04f3b4e10 docker.io/rancher/coredns-coredns:1.8.0],SizeBytes:12944537,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0 asia.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},ContainerImage{Names:[gke.gcr.io/addon-resizer@sha256:9caaedb49f2d8c81b5b8ef5859d162866de415d45673c1b311cf94bcb67fa3fe gke.gcr.io/addon-resizer:1.8.12-gke.0],SizeBytes:9637636,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd@sha256:1d49fb3b108e6b42542e4a9b056dee308f06f88824326cde1636eea0472b799d k8s.gcr.io/prometheus-to-sd:v0.7.2],SizeBytes:9588954,},},VolumesInUse:[kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/gce-pd/gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,DevicePath:/dev/disk/by-id/google-gke-cluster-2-d1ba7ad5-pvc-217612ae-ef38-4720-9f34-d77a825e0493,},},Config:nil,},}&#xA;May 18 16:02:42.077: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 16:02:42.085: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-0shb&#xA;May 18 16:02:42.112: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-0shb: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 16:02:42.112: INFO: &#xA;Logging node info for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:42.120: INFO: Node Info: &amp;Node{ObjectMeta:{gke-cluster-2-default-pool-426afc83-rll8    544fba07-95f6-4b1a-9b69-0b192d5c1d1d 19824 0 2021-05-18 13:39:43 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:e2-medium beta.kubernetes.io/os:linux cloud.google.com/gke-boot-disk:pd-standard cloud.google.com/gke-container-runtime:containerd cloud.google.com/gke-nodepool:default-pool cloud.google.com/gke-os-distribution:cos cloud.google.com/machine-family:e2 failure-domain.beta.kubernetes.io/region:europe-west3 failure-domain.beta.kubernetes.io/zone:europe-west3-a kubernetes.io/arch:amd64 kubernetes.io/hostname:gke-cluster-2-default-pool-426afc83-rll8 kubernetes.io/os:linux node.kubernetes.io/instance-type:e2-medium topology.gke.io/zone:europe-west3-a topology.kubernetes.io/region:europe-west3 topology.kubernetes.io/zone:europe-west3-a] map[container.googleapis.com/instance_id:6275254909806738669 csi.volume.kubernetes.io/nodeid:{&#34;pd.csi.storage.gke.io&#34;:&#34;projects/fabian-test-311613/zones/europe-west3-a/instances/gke-cluster-2-default-pool-426afc83-rll8&#34;} node.alpha.kubernetes.io/ttl:0 node.gke.io/last-applied-node-labels:cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cloud.google.com/gke-os-distribution=cos,cloud.google.com/machine-family=e2 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [wrangler.cattle.io/node]  [{k3s Update v1 2021-05-18 13:39:43 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:finalizers&#34;:{&#34;.&#34;:{},&#34;v:\&#34;wrangler.cattle.io/node\&#34;&#34;:{}}}}} {vcluster Update v1 2021-05-18 15:12:00 +0000 UTC FieldsV1 {&#34;f:metadata&#34;:{&#34;f:annotations&#34;:{&#34;.&#34;:{},&#34;f:container.googleapis.com/instance_id&#34;:{},&#34;f:csi.volume.kubernetes.io/nodeid&#34;:{},&#34;f:node.alpha.kubernetes.io/ttl&#34;:{},&#34;f:node.gke.io/last-applied-node-labels&#34;:{},&#34;f:volumes.kubernetes.io/controller-managed-attach-detach&#34;:{}},&#34;f:labels&#34;:{&#34;.&#34;:{},&#34;f:beta.kubernetes.io/arch&#34;:{},&#34;f:beta.kubernetes.io/instance-type&#34;:{},&#34;f:beta.kubernetes.io/os&#34;:{},&#34;f:cloud.google.com/gke-boot-disk&#34;:{},&#34;f:cloud.google.com/gke-container-runtime&#34;:{},&#34;f:cloud.google.com/gke-nodepool&#34;:{},&#34;f:cloud.google.com/gke-os-distribution&#34;:{},&#34;f:cloud.google.com/machine-family&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/region&#34;:{},&#34;f:failure-domain.beta.kubernetes.io/zone&#34;:{},&#34;f:kubernetes.io/arch&#34;:{},&#34;f:kubernetes.io/hostname&#34;:{},&#34;f:kubernetes.io/os&#34;:{},&#34;f:node.kubernetes.io/instance-type&#34;:{},&#34;f:topology.gke.io/zone&#34;:{},&#34;f:topology.kubernetes.io/region&#34;:{},&#34;f:topology.kubernetes.io/zone&#34;:{}}},&#34;f:spec&#34;:{&#34;f:podCIDR&#34;:{},&#34;f:podCIDRs&#34;:{&#34;.&#34;:{},&#34;v:\&#34;10.104.1.0/24\&#34;&#34;:{}},&#34;f:providerID&#34;:{}},&#34;f:status&#34;:{&#34;f:addresses&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;InternalIP\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:address&#34;:{},&#34;f:type&#34;:{}}},&#34;f:allocatable&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:capacity&#34;:{&#34;.&#34;:{},&#34;f:attachable-volumes-gce-pd&#34;:{},&#34;f:cpu&#34;:{},&#34;f:ephemeral-storage&#34;:{},&#34;f:hugepages-1Gi&#34;:{},&#34;f:hugepages-2Mi&#34;:{},&#34;f:memory&#34;:{},&#34;f:pods&#34;:{}},&#34;f:conditions&#34;:{&#34;.&#34;:{},&#34;k:{\&#34;type\&#34;:\&#34;CorruptDockerOverlay2\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;DiskPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentContainerdRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentDockerRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentKubeletRestart\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;FrequentUnregisterNetDevice\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;KernelDeadlock\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;MemoryPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;NetworkUnavailable\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;PIDPressure\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;ReadonlyFilesystem\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}},&#34;k:{\&#34;type\&#34;:\&#34;Ready\&#34;}&#34;:{&#34;.&#34;:{},&#34;f:lastHeartbeatTime&#34;:{},&#34;f:lastTransitionTime&#34;:{},&#34;f:message&#34;:{},&#34;f:reason&#34;:{},&#34;f:status&#34;:{},&#34;f:type&#34;:{}}},&#34;f:daemonEndpoints&#34;:{&#34;f:kubeletEndpoint&#34;:{&#34;f:Port&#34;:{}}},&#34;f:images&#34;:{},&#34;f:nodeInfo&#34;:{&#34;f:architecture&#34;:{},&#34;f:bootID&#34;:{},&#34;f:containerRuntimeVersion&#34;:{},&#34;f:kernelVersion&#34;:{},&#34;f:kubeProxyVersion&#34;:{},&#34;f:kubeletVersion&#34;:{},&#34;f:machineID&#34;:{},&#34;f:operatingSystem&#34;:{},&#34;f:osImage&#34;:{},&#34;f:systemUUID&#34;:{}}}}}]},Spec:NodeSpec{PodCIDR:10.104.1.0/24,DoNotUseExternalID:,ProviderID:gce://fabian-test-311613/europe-west3-a/gke-cluster-2-default-pool-426afc83-rll8,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.104.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4130885632 0} {&lt;nil&gt;} 4034068Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{15 0} {&lt;nil&gt;} 15 DecimalSI},cpu: {{940 -3} {&lt;nil&gt;} 940m DecimalSI},ephemeral-storage: {{47093746742 0} {&lt;nil&gt;} 47093746742 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{2957529088 0} {&lt;nil&gt;} 2888212Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:FrequentUnregisterNetDevice,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentUnregisterNetDevice,Message:node is functioning properly,},NodeCondition{Type:FrequentKubeletRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentKubeletRestart,Message:kubelet is functioning properly,},NodeCondition{Type:FrequentDockerRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentDockerRestart,Message:docker is functioning properly,},NodeCondition{Type:FrequentContainerdRestart,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoFrequentContainerdRestart,Message:containerd is functioning properly,},NodeCondition{Type:KernelDeadlock,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:KernelHasNoDeadlock,Message:kernel has no deadlock,},NodeCondition{Type:ReadonlyFilesystem,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:FilesystemIsNotReadOnly,Message:Filesystem is not read-only,},NodeCondition{Type:CorruptDockerOverlay2,Status:False,LastHeartbeatTime:2021-05-18 16:00:15 +0000 UTC,LastTransitionTime:2021-05-18 13:34:57 +0000 UTC,Reason:NoCorruptDockerOverlay2,Message:docker overlay2 is functioning properly,},NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2021-05-18 13:34:58 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:RouteCreated,Message:NodeController create implicit route,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:46 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-05-18 16:00:43 +0000 UTC,LastTransitionTime:2021-05-18 13:34:58 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.108.1.212,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:8443,},},NodeInfo:NodeSystemInfo{MachineID:c9cac64897d2ea789512497eaaf0ee14,SystemUUID:c9cac648-97d2-ea78-9512-497eaaf0ee14,BootID:aae94a2e-12cf-4376-83c0-3c142c3be889,KernelVersion:5.4.104+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.6-gke.1000,KubeProxyVersion:v1.20.6-gke.1000,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gke.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000 k8s.gcr.io/kube-proxy-amd64:v1.20.6-gke.1000],SizeBytes:130986092,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[gke.gcr.io/gcp-compute-persistent-disk-csi-driver@sha256:e9e3a3af496e330d473b7c5c42958de4ebc1c17fbbb311360b7ecdf7b28e1c93 gke.gcr.io/gcp-compute-persistent-disk-csi-driver:v1.2.0-gke.8],SizeBytes:74280698,},ContainerImage{Names:[gke.gcr.io/event-exporter@sha256:34b034af960047020f4674151bbb2b55878f82a0b76dabca81b518e9fe82180d gke.gcr.io/event-exporter:v0.3.4-gke.0],SizeBytes:48883337,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[gke.gcr.io/prometheus-to-sd@sha256:c5e12680a431990d5e39ed249dcb43a7672e99f7ef94a9928be40cf2f418f62f gke.gcr.io/prometheus-to-sd:v0.10.0-gke.0],SizeBytes:44144056,},ContainerImage{Names:[gke.gcr.io/k8s-dns-kube-dns-amd64@sha256:f5210cf47c3d04c72835499fdade27f176ebedb7316172c560251d3dbd5180fb gke.gcr.io/k8s-dns-kube-dns-amd64:1.17.3-gke.0],SizeBytes:43982589,},ContainerImage{Names:[gke.gcr.io/k8s-dns-sidecar-amd64@sha256:4a0754f097979fd6fc957fd3e6371be82e278a3dc1643eec69a1b48b2d34c874 gke.gcr.io/k8s-dns-sidecar-amd64:1.17.3-gke.0],SizeBytes:41990458,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:6f59f610ddc1e0279b83705d545b668f77733a9bb74db1afdf5b575fe862f5ab gke.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.17.3-gke.0],SizeBytes:41023689,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[gke.gcr.io/gke-metrics-agent@sha256:6d49d58d07a9dfded57b22aa900952428a192c152e8a05759a7bf4b4babfe1d2 gke.gcr.io/gke-metrics-agent:1.0.3-gke.0],SizeBytes:38421070,},ContainerImage{Names:[gke.gcr.io/fluent-bit@sha256:7ee887d550fb472cda882212112dcf3cca313fffa318fc762032415e190a10f1 gke.gcr.io/fluent-bit:v1.5.7-gke.1],SizeBytes:37851912,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:ee14fd553315d41a72f705ad4e792809fa3c99f0fa73fec4c3af3d3da8d13ee5 docker.io/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608830,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[gcr.io/stackdriver-agents/metadata-agent-go@sha256:b7d97f6543ebd8d133da944f3631cc2501c16dbc2abc835e741f4dcc8daba3d7 gcr.io/stackdriver-agents/metadata-agent-go:1.2.0],SizeBytes:23949960,},ContainerImage{Names:[gke.gcr.io/fluent-bit-gke-exporter@sha256:31efae697729afb1f4c2fbf4de07961232c9b54668b4977b960a27f1ef9aafb1 gke.gcr.io/fluent-bit-gke-exporter:v0.16.2-gke.0],SizeBytes:19829156,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:3b600d2d5eafa6d5fcedbd783b28694af3f7dd470f3580e83200244fab32b35e gke.gcr.io/cluster-proportional-autoscaler-amd64:1.8.1-gke.0 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.8.1-gke.0],SizeBytes:16833220,},ContainerImage{Names:[gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 eu.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989 gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0 gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0],SizeBytes:16572417,},ContainerImage{Names:[gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 eu.gcr.io/gke-release-staging/cpvpa-amd64@sha256:b802d79f207ea9cbc996cc208781ea34715fe796be68e5250da37b2ae73d42a6 gcr.io/gke-release-staging/cpvpa-amd64:v0.8.3-gke.0 gke.gcr.io/cpvpa-amd64:v0.8.3-gke.0],SizeBytes:16376177,},ContainerImage{Names:[k8s.gcr.io/metrics-server-amd64@sha256:c9c4e95068b51d6b33a9dccc61875df07dc650abbf4ac1a19d58b4628f89288b k8s.gcr.io/metrics-server-amd64:v0.3.6],SizeBytes:10542830,},ContainerImage{Names:[gke.gcr.io/csi-node-driver-registrar@sha256:877ecfbb4119d63e83a45659044d128326f814ae1091b5630e236930a50b741d gke.gcr.io/csi-node-driver-registrar:v2.1.0-gke.0],SizeBytes:10028812,},ContainerImage{Names:[eu.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d asia.gcr.io/gke-release-staging/addon-resizer@sha256:fe1d103a6ccdbee3a8d32fb2b20fe400f16283767040cb295a68d61b1096e91d gke.gcr.io/addon-resizer:1.8.11-gke.0 eu.gcr.io/gke-release-staging/addon-resizer:1.8.11-gke.0],SizeBytes:9653144,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}&#xA;May 18 16:02:42.121: INFO: &#xA;Logging kubelet events for node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:42.129: INFO: &#xA;Logging pods the kubelet thinks is on node gke-cluster-2-default-pool-426afc83-rll8&#xA;May 18 16:02:42.146: INFO: Unable to retrieve kubelet pods for node gke-cluster-2-default-pool-426afc83-rll8: serializer for text/plain; version=0.0.4; charset=utf-8 doesn&#39;t exist&#xA;May 18 16:02:42.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;webhook-3012&#34; for this suite.&#xA;STEP: Destroying namespace &#34;webhook-3012-markers&#34; for this suite.&#xA;[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]&#xA;  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (block volmode)] disruptive[Disruptive][LinuxOnly] Should test that pv written before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted volume expand Should verify mounted devices can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Generic Ephemeral-volume (default fs) [Feature:GenericEphemeralVolume] (late-binding)] ephemeral should create read/write inline ephemeral volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify fstype - default value should be ext4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]" classname="Kubernetes e2e suite" time="3.828913699"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] volume-stress multiple pods should access different volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with different fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should fail if subpath file is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] SCTP [Feature:SCTP] [LinuxOnly] should create a Pod with SCTP HostPort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.551671242"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy [LinuxOnly] NetworkPolicy between server and client should enforce policy based on PodSelector or NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:IPv6DualStackAlphaFeature] [LinuxOnly] Granular Checks: Services Secondary IP Family should be able to handle large requests: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp unconfined on the container [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with pvc data source in parallel [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand Verify if offline PVC expansion works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (default fs)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap Should fail non-optional pod creation due to configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="Recreate [Feature:Recreate] recreate nodes and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single read-only volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]" classname="Kubernetes e2e suite" time="0.588233392"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic Snapshot (delete policy)] snapshottable-stress[Feature:VolumeSnapshotDataSource] should support snapshotting of many volumes repeatedly [Slow] [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vsphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (Always)[LinuxOnly], pod created with an initial fsgroup, volume contents ownership changed in first pod, new pod with same fsgroup applied to the volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] fsgroupchangepolicy (OnRootMismatch)[LinuxOnly], pod created with an initial fsgroup, new pod fsgroup applied to volume contents" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="22.758385062"></testcase>
      <testcase name="[sig-storage] HostPathType Character Device [Slow] Should fail on mounting character device &#39;achardev&#39; when HostPathType is HostPathDirectory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="2.64580345"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)(allowExpansion)][sig-windows] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: windows-gcepd] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Stress with local volumes [Serial] should be able to process many pods and reuse local volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp default which is unconfined [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two metrics of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client Kubectl create quota should create a quota with scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] SecurityContext should ignore Linux Specific SecurityContext if set" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to use a volume in a pod with mismatched mode [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow][LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure-disk] [Testpattern: Pre-provisioned PV (xfs)][Slow] volumes should store data" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
  </testsuite>